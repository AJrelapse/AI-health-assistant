{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.5,
  "eval_steps": 500,
  "global_step": 19000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005,
      "grad_norm": 7.000163555145264,
      "learning_rate": 4.9980000000000006e-05,
      "loss": 3.689,
      "step": 10
    },
    {
      "epoch": 0.01,
      "grad_norm": 7.9376912117004395,
      "learning_rate": 4.9955e-05,
      "loss": 3.5895,
      "step": 20
    },
    {
      "epoch": 0.015,
      "grad_norm": 9.940776824951172,
      "learning_rate": 4.9930000000000005e-05,
      "loss": 3.3515,
      "step": 30
    },
    {
      "epoch": 0.02,
      "grad_norm": 6.399010181427002,
      "learning_rate": 4.9905000000000004e-05,
      "loss": 3.06,
      "step": 40
    },
    {
      "epoch": 0.025,
      "grad_norm": 4.341258525848389,
      "learning_rate": 4.9880000000000004e-05,
      "loss": 2.785,
      "step": 50
    },
    {
      "epoch": 0.03,
      "grad_norm": 7.607991695404053,
      "learning_rate": 4.9855e-05,
      "loss": 2.5816,
      "step": 60
    },
    {
      "epoch": 0.035,
      "grad_norm": 4.320233345031738,
      "learning_rate": 4.983e-05,
      "loss": 2.4622,
      "step": 70
    },
    {
      "epoch": 0.04,
      "grad_norm": 5.38251256942749,
      "learning_rate": 4.9805e-05,
      "loss": 2.4374,
      "step": 80
    },
    {
      "epoch": 0.045,
      "grad_norm": 4.741649150848389,
      "learning_rate": 4.978e-05,
      "loss": 2.3228,
      "step": 90
    },
    {
      "epoch": 0.05,
      "grad_norm": 4.676573753356934,
      "learning_rate": 4.9755e-05,
      "loss": 2.1775,
      "step": 100
    },
    {
      "epoch": 0.055,
      "grad_norm": 4.850433826446533,
      "learning_rate": 4.973000000000001e-05,
      "loss": 2.1255,
      "step": 110
    },
    {
      "epoch": 0.06,
      "grad_norm": 4.61882209777832,
      "learning_rate": 4.9705e-05,
      "loss": 2.3208,
      "step": 120
    },
    {
      "epoch": 0.065,
      "grad_norm": 3.6585042476654053,
      "learning_rate": 4.9680000000000005e-05,
      "loss": 2.0106,
      "step": 130
    },
    {
      "epoch": 0.07,
      "grad_norm": 7.767890930175781,
      "learning_rate": 4.9655000000000005e-05,
      "loss": 1.9631,
      "step": 140
    },
    {
      "epoch": 0.075,
      "grad_norm": 7.112246036529541,
      "learning_rate": 4.9630000000000004e-05,
      "loss": 2.166,
      "step": 150
    },
    {
      "epoch": 0.08,
      "grad_norm": 17.55562400817871,
      "learning_rate": 4.9605000000000004e-05,
      "loss": 2.2865,
      "step": 160
    },
    {
      "epoch": 0.085,
      "grad_norm": 8.72468376159668,
      "learning_rate": 4.958e-05,
      "loss": 2.1147,
      "step": 170
    },
    {
      "epoch": 0.09,
      "grad_norm": 4.8587870597839355,
      "learning_rate": 4.9555e-05,
      "loss": 1.9859,
      "step": 180
    },
    {
      "epoch": 0.095,
      "grad_norm": 5.859605312347412,
      "learning_rate": 4.953e-05,
      "loss": 1.9856,
      "step": 190
    },
    {
      "epoch": 0.1,
      "grad_norm": 6.526833534240723,
      "learning_rate": 4.9505e-05,
      "loss": 1.9134,
      "step": 200
    },
    {
      "epoch": 0.105,
      "grad_norm": 5.73093318939209,
      "learning_rate": 4.948000000000001e-05,
      "loss": 2.005,
      "step": 210
    },
    {
      "epoch": 0.11,
      "grad_norm": 5.113691329956055,
      "learning_rate": 4.9455e-05,
      "loss": 1.9738,
      "step": 220
    },
    {
      "epoch": 0.115,
      "grad_norm": 5.470981121063232,
      "learning_rate": 4.9430000000000006e-05,
      "loss": 2.0841,
      "step": 230
    },
    {
      "epoch": 0.12,
      "grad_norm": 6.18114709854126,
      "learning_rate": 4.9405e-05,
      "loss": 2.1069,
      "step": 240
    },
    {
      "epoch": 0.125,
      "grad_norm": 7.96823263168335,
      "learning_rate": 4.9380000000000005e-05,
      "loss": 1.9162,
      "step": 250
    },
    {
      "epoch": 0.13,
      "grad_norm": 6.609747409820557,
      "learning_rate": 4.9355000000000004e-05,
      "loss": 2.042,
      "step": 260
    },
    {
      "epoch": 0.135,
      "grad_norm": 5.873908519744873,
      "learning_rate": 4.9330000000000004e-05,
      "loss": 1.8613,
      "step": 270
    },
    {
      "epoch": 0.14,
      "grad_norm": 5.851371765136719,
      "learning_rate": 4.9305e-05,
      "loss": 1.9235,
      "step": 280
    },
    {
      "epoch": 0.145,
      "grad_norm": 7.067171096801758,
      "learning_rate": 4.928e-05,
      "loss": 1.9434,
      "step": 290
    },
    {
      "epoch": 0.15,
      "grad_norm": 5.983580589294434,
      "learning_rate": 4.9255e-05,
      "loss": 1.9683,
      "step": 300
    },
    {
      "epoch": 0.155,
      "grad_norm": 6.324028968811035,
      "learning_rate": 4.923e-05,
      "loss": 2.2537,
      "step": 310
    },
    {
      "epoch": 0.16,
      "grad_norm": 6.529838562011719,
      "learning_rate": 4.9205e-05,
      "loss": 1.9548,
      "step": 320
    },
    {
      "epoch": 0.165,
      "grad_norm": 11.898024559020996,
      "learning_rate": 4.918000000000001e-05,
      "loss": 1.9238,
      "step": 330
    },
    {
      "epoch": 0.17,
      "grad_norm": 12.799983024597168,
      "learning_rate": 4.9155e-05,
      "loss": 2.2375,
      "step": 340
    },
    {
      "epoch": 0.175,
      "grad_norm": 6.4609880447387695,
      "learning_rate": 4.9130000000000006e-05,
      "loss": 1.8915,
      "step": 350
    },
    {
      "epoch": 0.18,
      "grad_norm": 7.284613132476807,
      "learning_rate": 4.9105e-05,
      "loss": 2.0117,
      "step": 360
    },
    {
      "epoch": 0.185,
      "grad_norm": 10.386825561523438,
      "learning_rate": 4.9080000000000004e-05,
      "loss": 1.9509,
      "step": 370
    },
    {
      "epoch": 0.19,
      "grad_norm": 7.916159629821777,
      "learning_rate": 4.9055000000000004e-05,
      "loss": 1.9403,
      "step": 380
    },
    {
      "epoch": 0.195,
      "grad_norm": 8.956114768981934,
      "learning_rate": 4.903e-05,
      "loss": 1.9932,
      "step": 390
    },
    {
      "epoch": 0.2,
      "grad_norm": 6.734665393829346,
      "learning_rate": 4.9005e-05,
      "loss": 2.152,
      "step": 400
    },
    {
      "epoch": 0.205,
      "grad_norm": 6.826788902282715,
      "learning_rate": 4.898e-05,
      "loss": 1.9981,
      "step": 410
    },
    {
      "epoch": 0.21,
      "grad_norm": 4.9666428565979,
      "learning_rate": 4.8955e-05,
      "loss": 1.7793,
      "step": 420
    },
    {
      "epoch": 0.215,
      "grad_norm": 6.2226457595825195,
      "learning_rate": 4.8932500000000006e-05,
      "loss": 1.9299,
      "step": 430
    },
    {
      "epoch": 0.22,
      "grad_norm": 9.545181274414062,
      "learning_rate": 4.89075e-05,
      "loss": 1.9424,
      "step": 440
    },
    {
      "epoch": 0.225,
      "grad_norm": 7.405815601348877,
      "learning_rate": 4.8882500000000005e-05,
      "loss": 1.7936,
      "step": 450
    },
    {
      "epoch": 0.23,
      "grad_norm": 7.403995513916016,
      "learning_rate": 4.88575e-05,
      "loss": 1.9123,
      "step": 460
    },
    {
      "epoch": 0.235,
      "grad_norm": 6.941356182098389,
      "learning_rate": 4.8832500000000004e-05,
      "loss": 1.9036,
      "step": 470
    },
    {
      "epoch": 0.24,
      "grad_norm": 5.838131427764893,
      "learning_rate": 4.88075e-05,
      "loss": 2.0568,
      "step": 480
    },
    {
      "epoch": 0.245,
      "grad_norm": 20.09534454345703,
      "learning_rate": 4.87825e-05,
      "loss": 2.0545,
      "step": 490
    },
    {
      "epoch": 0.25,
      "grad_norm": 8.514161109924316,
      "learning_rate": 4.87575e-05,
      "loss": 2.0759,
      "step": 500
    },
    {
      "epoch": 0.255,
      "grad_norm": 8.394305229187012,
      "learning_rate": 4.87325e-05,
      "loss": 1.875,
      "step": 510
    },
    {
      "epoch": 0.26,
      "grad_norm": 6.456119537353516,
      "learning_rate": 4.87075e-05,
      "loss": 1.6356,
      "step": 520
    },
    {
      "epoch": 0.265,
      "grad_norm": 7.362597942352295,
      "learning_rate": 4.86825e-05,
      "loss": 1.843,
      "step": 530
    },
    {
      "epoch": 0.27,
      "grad_norm": 6.753175258636475,
      "learning_rate": 4.86575e-05,
      "loss": 2.012,
      "step": 540
    },
    {
      "epoch": 0.275,
      "grad_norm": 7.381443977355957,
      "learning_rate": 4.8632500000000006e-05,
      "loss": 2.2626,
      "step": 550
    },
    {
      "epoch": 0.28,
      "grad_norm": 6.317389488220215,
      "learning_rate": 4.86075e-05,
      "loss": 1.7854,
      "step": 560
    },
    {
      "epoch": 0.285,
      "grad_norm": 6.474985122680664,
      "learning_rate": 4.8582500000000005e-05,
      "loss": 1.8686,
      "step": 570
    },
    {
      "epoch": 0.29,
      "grad_norm": 8.774740219116211,
      "learning_rate": 4.85575e-05,
      "loss": 1.772,
      "step": 580
    },
    {
      "epoch": 0.295,
      "grad_norm": 18.22159194946289,
      "learning_rate": 4.85325e-05,
      "loss": 1.7339,
      "step": 590
    },
    {
      "epoch": 0.3,
      "grad_norm": 6.688895225524902,
      "learning_rate": 4.85075e-05,
      "loss": 1.9149,
      "step": 600
    },
    {
      "epoch": 0.305,
      "grad_norm": 15.383301734924316,
      "learning_rate": 4.84825e-05,
      "loss": 2.1468,
      "step": 610
    },
    {
      "epoch": 0.31,
      "grad_norm": 8.110637664794922,
      "learning_rate": 4.84575e-05,
      "loss": 1.8359,
      "step": 620
    },
    {
      "epoch": 0.315,
      "grad_norm": 7.2800445556640625,
      "learning_rate": 4.84325e-05,
      "loss": 1.8157,
      "step": 630
    },
    {
      "epoch": 0.32,
      "grad_norm": 6.386188507080078,
      "learning_rate": 4.84075e-05,
      "loss": 1.8048,
      "step": 640
    },
    {
      "epoch": 0.325,
      "grad_norm": 7.32275915145874,
      "learning_rate": 4.83825e-05,
      "loss": 1.9307,
      "step": 650
    },
    {
      "epoch": 0.33,
      "grad_norm": 14.9103422164917,
      "learning_rate": 4.83575e-05,
      "loss": 1.8495,
      "step": 660
    },
    {
      "epoch": 0.335,
      "grad_norm": 12.806757926940918,
      "learning_rate": 4.8332500000000005e-05,
      "loss": 1.9368,
      "step": 670
    },
    {
      "epoch": 0.34,
      "grad_norm": 10.670047760009766,
      "learning_rate": 4.83075e-05,
      "loss": 1.9606,
      "step": 680
    },
    {
      "epoch": 0.345,
      "grad_norm": 8.036849975585938,
      "learning_rate": 4.8282500000000004e-05,
      "loss": 1.9372,
      "step": 690
    },
    {
      "epoch": 0.35,
      "grad_norm": 9.146262168884277,
      "learning_rate": 4.82575e-05,
      "loss": 1.7765,
      "step": 700
    },
    {
      "epoch": 0.355,
      "grad_norm": 11.004975318908691,
      "learning_rate": 4.82325e-05,
      "loss": 1.6851,
      "step": 710
    },
    {
      "epoch": 0.36,
      "grad_norm": 13.084080696105957,
      "learning_rate": 4.82075e-05,
      "loss": 1.7558,
      "step": 720
    },
    {
      "epoch": 0.365,
      "grad_norm": 7.463730335235596,
      "learning_rate": 4.81825e-05,
      "loss": 1.9029,
      "step": 730
    },
    {
      "epoch": 0.37,
      "grad_norm": 8.099459648132324,
      "learning_rate": 4.81575e-05,
      "loss": 1.7912,
      "step": 740
    },
    {
      "epoch": 0.375,
      "grad_norm": 7.398362636566162,
      "learning_rate": 4.81325e-05,
      "loss": 1.7521,
      "step": 750
    },
    {
      "epoch": 0.38,
      "grad_norm": 7.8694233894348145,
      "learning_rate": 4.81075e-05,
      "loss": 1.8647,
      "step": 760
    },
    {
      "epoch": 0.385,
      "grad_norm": 11.119146347045898,
      "learning_rate": 4.8082500000000006e-05,
      "loss": 1.7446,
      "step": 770
    },
    {
      "epoch": 0.39,
      "grad_norm": 10.085947036743164,
      "learning_rate": 4.80575e-05,
      "loss": 1.6834,
      "step": 780
    },
    {
      "epoch": 0.395,
      "grad_norm": 20.016338348388672,
      "learning_rate": 4.8032500000000005e-05,
      "loss": 1.9204,
      "step": 790
    },
    {
      "epoch": 0.4,
      "grad_norm": 8.2753324508667,
      "learning_rate": 4.80075e-05,
      "loss": 1.7441,
      "step": 800
    },
    {
      "epoch": 0.405,
      "grad_norm": 10.44144344329834,
      "learning_rate": 4.7982500000000004e-05,
      "loss": 1.963,
      "step": 810
    },
    {
      "epoch": 0.41,
      "grad_norm": 7.932163715362549,
      "learning_rate": 4.79575e-05,
      "loss": 1.8241,
      "step": 820
    },
    {
      "epoch": 0.415,
      "grad_norm": 9.882116317749023,
      "learning_rate": 4.79325e-05,
      "loss": 1.8173,
      "step": 830
    },
    {
      "epoch": 0.42,
      "grad_norm": 6.230210781097412,
      "learning_rate": 4.79075e-05,
      "loss": 2.0376,
      "step": 840
    },
    {
      "epoch": 0.425,
      "grad_norm": 7.284298419952393,
      "learning_rate": 4.78825e-05,
      "loss": 1.8575,
      "step": 850
    },
    {
      "epoch": 0.43,
      "grad_norm": 11.362064361572266,
      "learning_rate": 4.78575e-05,
      "loss": 1.954,
      "step": 860
    },
    {
      "epoch": 0.435,
      "grad_norm": 8.871236801147461,
      "learning_rate": 4.78325e-05,
      "loss": 1.9,
      "step": 870
    },
    {
      "epoch": 0.44,
      "grad_norm": 9.292006492614746,
      "learning_rate": 4.7807500000000006e-05,
      "loss": 1.9106,
      "step": 880
    },
    {
      "epoch": 0.445,
      "grad_norm": 9.61070442199707,
      "learning_rate": 4.7782500000000005e-05,
      "loss": 1.8434,
      "step": 890
    },
    {
      "epoch": 0.45,
      "grad_norm": 8.70384407043457,
      "learning_rate": 4.7757500000000005e-05,
      "loss": 1.5893,
      "step": 900
    },
    {
      "epoch": 0.455,
      "grad_norm": 7.535331726074219,
      "learning_rate": 4.7732500000000004e-05,
      "loss": 1.8103,
      "step": 910
    },
    {
      "epoch": 0.46,
      "grad_norm": 8.939045906066895,
      "learning_rate": 4.7707500000000004e-05,
      "loss": 1.596,
      "step": 920
    },
    {
      "epoch": 0.465,
      "grad_norm": 8.642134666442871,
      "learning_rate": 4.76825e-05,
      "loss": 1.8374,
      "step": 930
    },
    {
      "epoch": 0.47,
      "grad_norm": 6.09809684753418,
      "learning_rate": 4.76575e-05,
      "loss": 1.9628,
      "step": 940
    },
    {
      "epoch": 0.475,
      "grad_norm": 16.344297409057617,
      "learning_rate": 4.76325e-05,
      "loss": 1.8826,
      "step": 950
    },
    {
      "epoch": 0.48,
      "grad_norm": 9.791821479797363,
      "learning_rate": 4.760750000000001e-05,
      "loss": 1.6674,
      "step": 960
    },
    {
      "epoch": 0.485,
      "grad_norm": 9.082563400268555,
      "learning_rate": 4.75825e-05,
      "loss": 1.7522,
      "step": 970
    },
    {
      "epoch": 0.49,
      "grad_norm": 9.508834838867188,
      "learning_rate": 4.755750000000001e-05,
      "loss": 1.815,
      "step": 980
    },
    {
      "epoch": 0.495,
      "grad_norm": 6.843518257141113,
      "learning_rate": 4.75325e-05,
      "loss": 1.8386,
      "step": 990
    },
    {
      "epoch": 0.5,
      "grad_norm": 7.2275190353393555,
      "learning_rate": 4.7507500000000006e-05,
      "loss": 1.7332,
      "step": 1000
    },
    {
      "epoch": 0.505,
      "grad_norm": 7.059435844421387,
      "learning_rate": 4.7482500000000005e-05,
      "loss": 1.7759,
      "step": 1010
    },
    {
      "epoch": 0.51,
      "grad_norm": 7.744873046875,
      "learning_rate": 4.7457500000000004e-05,
      "loss": 1.7706,
      "step": 1020
    },
    {
      "epoch": 0.515,
      "grad_norm": 7.692896366119385,
      "learning_rate": 4.7432500000000004e-05,
      "loss": 1.7869,
      "step": 1030
    },
    {
      "epoch": 0.52,
      "grad_norm": 9.15715503692627,
      "learning_rate": 4.74075e-05,
      "loss": 1.9314,
      "step": 1040
    },
    {
      "epoch": 0.525,
      "grad_norm": 10.992947578430176,
      "learning_rate": 4.73825e-05,
      "loss": 1.8422,
      "step": 1050
    },
    {
      "epoch": 0.53,
      "grad_norm": 7.079051494598389,
      "learning_rate": 4.73575e-05,
      "loss": 1.755,
      "step": 1060
    },
    {
      "epoch": 0.535,
      "grad_norm": 11.847060203552246,
      "learning_rate": 4.73325e-05,
      "loss": 1.659,
      "step": 1070
    },
    {
      "epoch": 0.54,
      "grad_norm": 10.035859107971191,
      "learning_rate": 4.730750000000001e-05,
      "loss": 1.6173,
      "step": 1080
    },
    {
      "epoch": 0.545,
      "grad_norm": 11.035881042480469,
      "learning_rate": 4.72825e-05,
      "loss": 1.6853,
      "step": 1090
    },
    {
      "epoch": 0.55,
      "grad_norm": 9.486039161682129,
      "learning_rate": 4.7257500000000006e-05,
      "loss": 1.9782,
      "step": 1100
    },
    {
      "epoch": 0.555,
      "grad_norm": 7.544719219207764,
      "learning_rate": 4.72325e-05,
      "loss": 1.5776,
      "step": 1110
    },
    {
      "epoch": 0.56,
      "grad_norm": 8.788000106811523,
      "learning_rate": 4.7207500000000005e-05,
      "loss": 1.6054,
      "step": 1120
    },
    {
      "epoch": 0.565,
      "grad_norm": 20.44849967956543,
      "learning_rate": 4.7182500000000004e-05,
      "loss": 1.6063,
      "step": 1130
    },
    {
      "epoch": 0.57,
      "grad_norm": 8.57906723022461,
      "learning_rate": 4.7157500000000004e-05,
      "loss": 1.7783,
      "step": 1140
    },
    {
      "epoch": 0.575,
      "grad_norm": 13.289475440979004,
      "learning_rate": 4.71325e-05,
      "loss": 1.7245,
      "step": 1150
    },
    {
      "epoch": 0.58,
      "grad_norm": 7.981741428375244,
      "learning_rate": 4.71075e-05,
      "loss": 1.7797,
      "step": 1160
    },
    {
      "epoch": 0.585,
      "grad_norm": 11.26968765258789,
      "learning_rate": 4.70825e-05,
      "loss": 2.0183,
      "step": 1170
    },
    {
      "epoch": 0.59,
      "grad_norm": 8.492910385131836,
      "learning_rate": 4.70575e-05,
      "loss": 1.8031,
      "step": 1180
    },
    {
      "epoch": 0.595,
      "grad_norm": 13.511219024658203,
      "learning_rate": 4.70325e-05,
      "loss": 1.932,
      "step": 1190
    },
    {
      "epoch": 0.6,
      "grad_norm": 7.601946830749512,
      "learning_rate": 4.700750000000001e-05,
      "loss": 1.583,
      "step": 1200
    },
    {
      "epoch": 0.605,
      "grad_norm": 9.328937530517578,
      "learning_rate": 4.69825e-05,
      "loss": 1.9339,
      "step": 1210
    },
    {
      "epoch": 0.61,
      "grad_norm": 8.582978248596191,
      "learning_rate": 4.6957500000000006e-05,
      "loss": 1.6722,
      "step": 1220
    },
    {
      "epoch": 0.615,
      "grad_norm": 8.951892852783203,
      "learning_rate": 4.69325e-05,
      "loss": 1.6199,
      "step": 1230
    },
    {
      "epoch": 0.62,
      "grad_norm": 16.487163543701172,
      "learning_rate": 4.6907500000000004e-05,
      "loss": 1.9262,
      "step": 1240
    },
    {
      "epoch": 0.625,
      "grad_norm": 9.115212440490723,
      "learning_rate": 4.6882500000000004e-05,
      "loss": 1.6551,
      "step": 1250
    },
    {
      "epoch": 0.63,
      "grad_norm": 8.68182373046875,
      "learning_rate": 4.68575e-05,
      "loss": 1.6808,
      "step": 1260
    },
    {
      "epoch": 0.635,
      "grad_norm": 11.25213623046875,
      "learning_rate": 4.68325e-05,
      "loss": 1.6599,
      "step": 1270
    },
    {
      "epoch": 0.64,
      "grad_norm": 7.615752220153809,
      "learning_rate": 4.68075e-05,
      "loss": 1.8814,
      "step": 1280
    },
    {
      "epoch": 0.645,
      "grad_norm": 11.364786148071289,
      "learning_rate": 4.67825e-05,
      "loss": 1.8224,
      "step": 1290
    },
    {
      "epoch": 0.65,
      "grad_norm": 9.952449798583984,
      "learning_rate": 4.67575e-05,
      "loss": 1.7838,
      "step": 1300
    },
    {
      "epoch": 0.655,
      "grad_norm": 10.846966743469238,
      "learning_rate": 4.67325e-05,
      "loss": 1.9132,
      "step": 1310
    },
    {
      "epoch": 0.66,
      "grad_norm": 8.497391700744629,
      "learning_rate": 4.6707500000000006e-05,
      "loss": 1.9086,
      "step": 1320
    },
    {
      "epoch": 0.665,
      "grad_norm": 7.2135443687438965,
      "learning_rate": 4.66825e-05,
      "loss": 2.061,
      "step": 1330
    },
    {
      "epoch": 0.67,
      "grad_norm": 8.554430961608887,
      "learning_rate": 4.6657500000000005e-05,
      "loss": 1.508,
      "step": 1340
    },
    {
      "epoch": 0.675,
      "grad_norm": 7.0656867027282715,
      "learning_rate": 4.6632500000000005e-05,
      "loss": 1.8923,
      "step": 1350
    },
    {
      "epoch": 0.68,
      "grad_norm": 10.076354026794434,
      "learning_rate": 4.6607500000000004e-05,
      "loss": 1.896,
      "step": 1360
    },
    {
      "epoch": 0.685,
      "grad_norm": 6.828597545623779,
      "learning_rate": 4.65825e-05,
      "loss": 1.6438,
      "step": 1370
    },
    {
      "epoch": 0.69,
      "grad_norm": 6.9675798416137695,
      "learning_rate": 4.65575e-05,
      "loss": 1.9162,
      "step": 1380
    },
    {
      "epoch": 0.695,
      "grad_norm": 7.454276084899902,
      "learning_rate": 4.65325e-05,
      "loss": 1.8503,
      "step": 1390
    },
    {
      "epoch": 0.7,
      "grad_norm": 7.555865287780762,
      "learning_rate": 4.65075e-05,
      "loss": 1.536,
      "step": 1400
    },
    {
      "epoch": 0.705,
      "grad_norm": 7.102753639221191,
      "learning_rate": 4.64825e-05,
      "loss": 1.5754,
      "step": 1410
    },
    {
      "epoch": 0.71,
      "grad_norm": 9.040836334228516,
      "learning_rate": 4.645750000000001e-05,
      "loss": 1.9605,
      "step": 1420
    },
    {
      "epoch": 0.715,
      "grad_norm": 18.122102737426758,
      "learning_rate": 4.64325e-05,
      "loss": 2.0484,
      "step": 1430
    },
    {
      "epoch": 0.72,
      "grad_norm": 7.865592956542969,
      "learning_rate": 4.6407500000000006e-05,
      "loss": 1.7553,
      "step": 1440
    },
    {
      "epoch": 0.725,
      "grad_norm": 12.86226749420166,
      "learning_rate": 4.63825e-05,
      "loss": 1.8404,
      "step": 1450
    },
    {
      "epoch": 0.73,
      "grad_norm": 10.457435607910156,
      "learning_rate": 4.6357500000000005e-05,
      "loss": 1.6929,
      "step": 1460
    },
    {
      "epoch": 0.735,
      "grad_norm": 14.574907302856445,
      "learning_rate": 4.6332500000000004e-05,
      "loss": 1.8736,
      "step": 1470
    },
    {
      "epoch": 0.74,
      "grad_norm": 8.641559600830078,
      "learning_rate": 4.6307500000000003e-05,
      "loss": 1.5915,
      "step": 1480
    },
    {
      "epoch": 0.745,
      "grad_norm": 8.586819648742676,
      "learning_rate": 4.62825e-05,
      "loss": 1.7859,
      "step": 1490
    },
    {
      "epoch": 0.75,
      "grad_norm": 16.00773811340332,
      "learning_rate": 4.62575e-05,
      "loss": 1.7371,
      "step": 1500
    },
    {
      "epoch": 0.755,
      "grad_norm": 17.446474075317383,
      "learning_rate": 4.62325e-05,
      "loss": 1.9951,
      "step": 1510
    },
    {
      "epoch": 0.76,
      "grad_norm": 10.0380277633667,
      "learning_rate": 4.62075e-05,
      "loss": 1.7621,
      "step": 1520
    },
    {
      "epoch": 0.765,
      "grad_norm": 8.806221961975098,
      "learning_rate": 4.61825e-05,
      "loss": 1.6775,
      "step": 1530
    },
    {
      "epoch": 0.77,
      "grad_norm": 8.26603889465332,
      "learning_rate": 4.6157500000000007e-05,
      "loss": 1.8408,
      "step": 1540
    },
    {
      "epoch": 0.775,
      "grad_norm": 9.650077819824219,
      "learning_rate": 4.61325e-05,
      "loss": 1.8668,
      "step": 1550
    },
    {
      "epoch": 0.78,
      "grad_norm": 8.390660285949707,
      "learning_rate": 4.6107500000000005e-05,
      "loss": 1.8881,
      "step": 1560
    },
    {
      "epoch": 0.785,
      "grad_norm": 13.236443519592285,
      "learning_rate": 4.60825e-05,
      "loss": 1.9474,
      "step": 1570
    },
    {
      "epoch": 0.79,
      "grad_norm": 8.266517639160156,
      "learning_rate": 4.6057500000000004e-05,
      "loss": 1.6304,
      "step": 1580
    },
    {
      "epoch": 0.795,
      "grad_norm": 19.07062530517578,
      "learning_rate": 4.6032500000000004e-05,
      "loss": 1.715,
      "step": 1590
    },
    {
      "epoch": 0.8,
      "grad_norm": 16.738161087036133,
      "learning_rate": 4.60075e-05,
      "loss": 1.5576,
      "step": 1600
    },
    {
      "epoch": 0.805,
      "grad_norm": 17.484554290771484,
      "learning_rate": 4.59825e-05,
      "loss": 1.727,
      "step": 1610
    },
    {
      "epoch": 0.81,
      "grad_norm": 8.56137752532959,
      "learning_rate": 4.59575e-05,
      "loss": 2.0226,
      "step": 1620
    },
    {
      "epoch": 0.815,
      "grad_norm": 6.96348762512207,
      "learning_rate": 4.59325e-05,
      "loss": 1.5528,
      "step": 1630
    },
    {
      "epoch": 0.82,
      "grad_norm": 9.605183601379395,
      "learning_rate": 4.59075e-05,
      "loss": 1.5219,
      "step": 1640
    },
    {
      "epoch": 0.825,
      "grad_norm": 6.240694522857666,
      "learning_rate": 4.58825e-05,
      "loss": 1.9237,
      "step": 1650
    },
    {
      "epoch": 0.83,
      "grad_norm": 10.023343086242676,
      "learning_rate": 4.5857500000000006e-05,
      "loss": 1.7945,
      "step": 1660
    },
    {
      "epoch": 0.835,
      "grad_norm": 9.926480293273926,
      "learning_rate": 4.58325e-05,
      "loss": 1.5631,
      "step": 1670
    },
    {
      "epoch": 0.84,
      "grad_norm": 8.40878677368164,
      "learning_rate": 4.5807500000000005e-05,
      "loss": 1.7713,
      "step": 1680
    },
    {
      "epoch": 0.845,
      "grad_norm": 12.70329761505127,
      "learning_rate": 4.57825e-05,
      "loss": 1.7554,
      "step": 1690
    },
    {
      "epoch": 0.85,
      "grad_norm": 9.805335998535156,
      "learning_rate": 4.5757500000000004e-05,
      "loss": 1.6825,
      "step": 1700
    },
    {
      "epoch": 0.855,
      "grad_norm": 8.174409866333008,
      "learning_rate": 4.57325e-05,
      "loss": 1.6659,
      "step": 1710
    },
    {
      "epoch": 0.86,
      "grad_norm": 7.441574573516846,
      "learning_rate": 4.57075e-05,
      "loss": 1.4174,
      "step": 1720
    },
    {
      "epoch": 0.865,
      "grad_norm": 12.449482917785645,
      "learning_rate": 4.56825e-05,
      "loss": 1.8446,
      "step": 1730
    },
    {
      "epoch": 0.87,
      "grad_norm": 6.522368431091309,
      "learning_rate": 4.56575e-05,
      "loss": 1.6449,
      "step": 1740
    },
    {
      "epoch": 0.875,
      "grad_norm": 6.77031946182251,
      "learning_rate": 4.56325e-05,
      "loss": 1.7762,
      "step": 1750
    },
    {
      "epoch": 0.88,
      "grad_norm": 10.130439758300781,
      "learning_rate": 4.56075e-05,
      "loss": 1.4664,
      "step": 1760
    },
    {
      "epoch": 0.885,
      "grad_norm": 9.126436233520508,
      "learning_rate": 4.55825e-05,
      "loss": 1.6276,
      "step": 1770
    },
    {
      "epoch": 0.89,
      "grad_norm": 7.016491889953613,
      "learning_rate": 4.5557500000000006e-05,
      "loss": 1.5689,
      "step": 1780
    },
    {
      "epoch": 0.895,
      "grad_norm": 10.745831489562988,
      "learning_rate": 4.55325e-05,
      "loss": 1.6834,
      "step": 1790
    },
    {
      "epoch": 0.9,
      "grad_norm": 12.328550338745117,
      "learning_rate": 4.5507500000000004e-05,
      "loss": 1.461,
      "step": 1800
    },
    {
      "epoch": 0.905,
      "grad_norm": 17.385677337646484,
      "learning_rate": 4.54825e-05,
      "loss": 1.8161,
      "step": 1810
    },
    {
      "epoch": 0.91,
      "grad_norm": 9.195854187011719,
      "learning_rate": 4.54575e-05,
      "loss": 1.8424,
      "step": 1820
    },
    {
      "epoch": 0.915,
      "grad_norm": 12.325849533081055,
      "learning_rate": 4.54325e-05,
      "loss": 1.7779,
      "step": 1830
    },
    {
      "epoch": 0.92,
      "grad_norm": 10.906588554382324,
      "learning_rate": 4.54075e-05,
      "loss": 1.8063,
      "step": 1840
    },
    {
      "epoch": 0.925,
      "grad_norm": 6.672791957855225,
      "learning_rate": 4.53825e-05,
      "loss": 1.7427,
      "step": 1850
    },
    {
      "epoch": 0.93,
      "grad_norm": 9.062288284301758,
      "learning_rate": 4.53575e-05,
      "loss": 1.7806,
      "step": 1860
    },
    {
      "epoch": 0.935,
      "grad_norm": 6.502415180206299,
      "learning_rate": 4.53325e-05,
      "loss": 1.7118,
      "step": 1870
    },
    {
      "epoch": 0.94,
      "grad_norm": 8.354060173034668,
      "learning_rate": 4.53075e-05,
      "loss": 1.7763,
      "step": 1880
    },
    {
      "epoch": 0.945,
      "grad_norm": 9.005587577819824,
      "learning_rate": 4.52825e-05,
      "loss": 1.6036,
      "step": 1890
    },
    {
      "epoch": 0.95,
      "grad_norm": 8.437121391296387,
      "learning_rate": 4.5257500000000005e-05,
      "loss": 1.4709,
      "step": 1900
    },
    {
      "epoch": 0.955,
      "grad_norm": 11.923377990722656,
      "learning_rate": 4.52325e-05,
      "loss": 1.8773,
      "step": 1910
    },
    {
      "epoch": 0.96,
      "grad_norm": 7.748147487640381,
      "learning_rate": 4.5207500000000004e-05,
      "loss": 1.6129,
      "step": 1920
    },
    {
      "epoch": 0.965,
      "grad_norm": 11.268333435058594,
      "learning_rate": 4.51825e-05,
      "loss": 1.6151,
      "step": 1930
    },
    {
      "epoch": 0.97,
      "grad_norm": 10.95048713684082,
      "learning_rate": 4.51575e-05,
      "loss": 1.8259,
      "step": 1940
    },
    {
      "epoch": 0.975,
      "grad_norm": 13.455390930175781,
      "learning_rate": 4.51325e-05,
      "loss": 1.7025,
      "step": 1950
    },
    {
      "epoch": 0.98,
      "grad_norm": 11.105806350708008,
      "learning_rate": 4.51075e-05,
      "loss": 1.8368,
      "step": 1960
    },
    {
      "epoch": 0.985,
      "grad_norm": 10.195911407470703,
      "learning_rate": 4.50825e-05,
      "loss": 1.6675,
      "step": 1970
    },
    {
      "epoch": 0.99,
      "grad_norm": 8.871874809265137,
      "learning_rate": 4.50575e-05,
      "loss": 1.649,
      "step": 1980
    },
    {
      "epoch": 0.995,
      "grad_norm": 8.616354942321777,
      "learning_rate": 4.50325e-05,
      "loss": 1.6742,
      "step": 1990
    },
    {
      "epoch": 1.0,
      "grad_norm": 8.279183387756348,
      "learning_rate": 4.5007500000000006e-05,
      "loss": 2.048,
      "step": 2000
    },
    {
      "epoch": 1.005,
      "grad_norm": 13.610426902770996,
      "learning_rate": 4.49825e-05,
      "loss": 1.6111,
      "step": 2010
    },
    {
      "epoch": 1.01,
      "grad_norm": 9.965251922607422,
      "learning_rate": 4.4957500000000005e-05,
      "loss": 1.5539,
      "step": 2020
    },
    {
      "epoch": 1.015,
      "grad_norm": 7.345335960388184,
      "learning_rate": 4.49325e-05,
      "loss": 1.5707,
      "step": 2030
    },
    {
      "epoch": 1.02,
      "grad_norm": 14.1624755859375,
      "learning_rate": 4.49075e-05,
      "loss": 1.7827,
      "step": 2040
    },
    {
      "epoch": 1.025,
      "grad_norm": 12.484307289123535,
      "learning_rate": 4.48825e-05,
      "loss": 1.6029,
      "step": 2050
    },
    {
      "epoch": 1.03,
      "grad_norm": 11.782843589782715,
      "learning_rate": 4.48575e-05,
      "loss": 1.7398,
      "step": 2060
    },
    {
      "epoch": 1.035,
      "grad_norm": 8.586226463317871,
      "learning_rate": 4.48325e-05,
      "loss": 1.556,
      "step": 2070
    },
    {
      "epoch": 1.04,
      "grad_norm": 8.697772026062012,
      "learning_rate": 4.48075e-05,
      "loss": 1.6085,
      "step": 2080
    },
    {
      "epoch": 1.045,
      "grad_norm": 19.019556045532227,
      "learning_rate": 4.47825e-05,
      "loss": 1.9722,
      "step": 2090
    },
    {
      "epoch": 1.05,
      "grad_norm": 18.059200286865234,
      "learning_rate": 4.47575e-05,
      "loss": 2.0452,
      "step": 2100
    },
    {
      "epoch": 1.055,
      "grad_norm": 8.215279579162598,
      "learning_rate": 4.47325e-05,
      "loss": 1.7431,
      "step": 2110
    },
    {
      "epoch": 1.06,
      "grad_norm": 9.377579689025879,
      "learning_rate": 4.4707500000000005e-05,
      "loss": 1.5391,
      "step": 2120
    },
    {
      "epoch": 1.065,
      "grad_norm": 9.536906242370605,
      "learning_rate": 4.4682500000000005e-05,
      "loss": 1.7725,
      "step": 2130
    },
    {
      "epoch": 1.07,
      "grad_norm": 6.752513885498047,
      "learning_rate": 4.4657500000000004e-05,
      "loss": 1.8511,
      "step": 2140
    },
    {
      "epoch": 1.075,
      "grad_norm": 21.16021156311035,
      "learning_rate": 4.46325e-05,
      "loss": 1.6753,
      "step": 2150
    },
    {
      "epoch": 1.08,
      "grad_norm": 8.363773345947266,
      "learning_rate": 4.46075e-05,
      "loss": 1.6013,
      "step": 2160
    },
    {
      "epoch": 1.085,
      "grad_norm": 11.378761291503906,
      "learning_rate": 4.45825e-05,
      "loss": 1.5698,
      "step": 2170
    },
    {
      "epoch": 1.09,
      "grad_norm": 9.7935152053833,
      "learning_rate": 4.45575e-05,
      "loss": 1.6979,
      "step": 2180
    },
    {
      "epoch": 1.095,
      "grad_norm": 13.745264053344727,
      "learning_rate": 4.453250000000001e-05,
      "loss": 1.3621,
      "step": 2190
    },
    {
      "epoch": 1.1,
      "grad_norm": 10.542648315429688,
      "learning_rate": 4.45075e-05,
      "loss": 1.3472,
      "step": 2200
    },
    {
      "epoch": 1.105,
      "grad_norm": 10.954105377197266,
      "learning_rate": 4.4482500000000007e-05,
      "loss": 1.8683,
      "step": 2210
    },
    {
      "epoch": 1.11,
      "grad_norm": 8.714597702026367,
      "learning_rate": 4.44575e-05,
      "loss": 1.5488,
      "step": 2220
    },
    {
      "epoch": 1.115,
      "grad_norm": 14.25689697265625,
      "learning_rate": 4.4432500000000005e-05,
      "loss": 1.402,
      "step": 2230
    },
    {
      "epoch": 1.12,
      "grad_norm": 13.367942810058594,
      "learning_rate": 4.4407500000000005e-05,
      "loss": 1.361,
      "step": 2240
    },
    {
      "epoch": 1.125,
      "grad_norm": 6.645061492919922,
      "learning_rate": 4.4382500000000004e-05,
      "loss": 1.6397,
      "step": 2250
    },
    {
      "epoch": 1.13,
      "grad_norm": 11.000247955322266,
      "learning_rate": 4.4357500000000003e-05,
      "loss": 1.8274,
      "step": 2260
    },
    {
      "epoch": 1.135,
      "grad_norm": 6.8458356857299805,
      "learning_rate": 4.43325e-05,
      "loss": 1.2771,
      "step": 2270
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 16.693605422973633,
      "learning_rate": 4.43075e-05,
      "loss": 1.3482,
      "step": 2280
    },
    {
      "epoch": 1.145,
      "grad_norm": 13.360411643981934,
      "learning_rate": 4.42825e-05,
      "loss": 1.7121,
      "step": 2290
    },
    {
      "epoch": 1.15,
      "grad_norm": 9.808645248413086,
      "learning_rate": 4.42575e-05,
      "loss": 1.7906,
      "step": 2300
    },
    {
      "epoch": 1.155,
      "grad_norm": 8.460457801818848,
      "learning_rate": 4.423250000000001e-05,
      "loss": 1.8718,
      "step": 2310
    },
    {
      "epoch": 1.16,
      "grad_norm": 10.006552696228027,
      "learning_rate": 4.42075e-05,
      "loss": 1.4451,
      "step": 2320
    },
    {
      "epoch": 1.165,
      "grad_norm": 21.22226905822754,
      "learning_rate": 4.4182500000000006e-05,
      "loss": 1.6047,
      "step": 2330
    },
    {
      "epoch": 1.17,
      "grad_norm": 8.469339370727539,
      "learning_rate": 4.41575e-05,
      "loss": 1.7533,
      "step": 2340
    },
    {
      "epoch": 1.175,
      "grad_norm": 15.820470809936523,
      "learning_rate": 4.4132500000000005e-05,
      "loss": 1.4195,
      "step": 2350
    },
    {
      "epoch": 1.18,
      "grad_norm": 15.950112342834473,
      "learning_rate": 4.4107500000000004e-05,
      "loss": 1.8664,
      "step": 2360
    },
    {
      "epoch": 1.185,
      "grad_norm": 10.726712226867676,
      "learning_rate": 4.4082500000000004e-05,
      "loss": 1.3426,
      "step": 2370
    },
    {
      "epoch": 1.19,
      "grad_norm": 7.995706081390381,
      "learning_rate": 4.40575e-05,
      "loss": 1.8082,
      "step": 2380
    },
    {
      "epoch": 1.195,
      "grad_norm": 8.187686920166016,
      "learning_rate": 4.40325e-05,
      "loss": 1.553,
      "step": 2390
    },
    {
      "epoch": 1.2,
      "grad_norm": 9.877100944519043,
      "learning_rate": 4.40075e-05,
      "loss": 1.6614,
      "step": 2400
    },
    {
      "epoch": 1.205,
      "grad_norm": 6.15311336517334,
      "learning_rate": 4.39825e-05,
      "loss": 1.7371,
      "step": 2410
    },
    {
      "epoch": 1.21,
      "grad_norm": 9.006941795349121,
      "learning_rate": 4.39575e-05,
      "loss": 1.6392,
      "step": 2420
    },
    {
      "epoch": 1.215,
      "grad_norm": 13.633414268493652,
      "learning_rate": 4.393250000000001e-05,
      "loss": 1.7126,
      "step": 2430
    },
    {
      "epoch": 1.22,
      "grad_norm": 11.263716697692871,
      "learning_rate": 4.39075e-05,
      "loss": 1.5649,
      "step": 2440
    },
    {
      "epoch": 1.225,
      "grad_norm": 13.219616889953613,
      "learning_rate": 4.3882500000000005e-05,
      "loss": 1.5044,
      "step": 2450
    },
    {
      "epoch": 1.23,
      "grad_norm": 10.006059646606445,
      "learning_rate": 4.38575e-05,
      "loss": 1.5438,
      "step": 2460
    },
    {
      "epoch": 1.2349999999999999,
      "grad_norm": 9.430448532104492,
      "learning_rate": 4.3832500000000004e-05,
      "loss": 1.7387,
      "step": 2470
    },
    {
      "epoch": 1.24,
      "grad_norm": 30.063873291015625,
      "learning_rate": 4.3807500000000004e-05,
      "loss": 1.7281,
      "step": 2480
    },
    {
      "epoch": 1.245,
      "grad_norm": 8.707937240600586,
      "learning_rate": 4.37825e-05,
      "loss": 1.5169,
      "step": 2490
    },
    {
      "epoch": 1.25,
      "grad_norm": 6.456330299377441,
      "learning_rate": 4.37575e-05,
      "loss": 1.8587,
      "step": 2500
    },
    {
      "epoch": 1.255,
      "grad_norm": 7.938021183013916,
      "learning_rate": 4.37325e-05,
      "loss": 1.6845,
      "step": 2510
    },
    {
      "epoch": 1.26,
      "grad_norm": 14.097137451171875,
      "learning_rate": 4.37075e-05,
      "loss": 1.5349,
      "step": 2520
    },
    {
      "epoch": 1.2650000000000001,
      "grad_norm": 11.001574516296387,
      "learning_rate": 4.36825e-05,
      "loss": 1.5121,
      "step": 2530
    },
    {
      "epoch": 1.27,
      "grad_norm": 10.211174011230469,
      "learning_rate": 4.36575e-05,
      "loss": 1.4172,
      "step": 2540
    },
    {
      "epoch": 1.275,
      "grad_norm": 17.205759048461914,
      "learning_rate": 4.3632500000000006e-05,
      "loss": 1.8684,
      "step": 2550
    },
    {
      "epoch": 1.28,
      "grad_norm": 13.798624992370605,
      "learning_rate": 4.36075e-05,
      "loss": 1.371,
      "step": 2560
    },
    {
      "epoch": 1.285,
      "grad_norm": 21.88161849975586,
      "learning_rate": 4.3582500000000005e-05,
      "loss": 1.6181,
      "step": 2570
    },
    {
      "epoch": 1.29,
      "grad_norm": 17.200973510742188,
      "learning_rate": 4.3557500000000004e-05,
      "loss": 1.8066,
      "step": 2580
    },
    {
      "epoch": 1.295,
      "grad_norm": 12.466277122497559,
      "learning_rate": 4.3532500000000004e-05,
      "loss": 1.6273,
      "step": 2590
    },
    {
      "epoch": 1.3,
      "grad_norm": 9.834688186645508,
      "learning_rate": 4.35075e-05,
      "loss": 1.2774,
      "step": 2600
    },
    {
      "epoch": 1.305,
      "grad_norm": 15.626541137695312,
      "learning_rate": 4.34825e-05,
      "loss": 1.5922,
      "step": 2610
    },
    {
      "epoch": 1.31,
      "grad_norm": 8.544983863830566,
      "learning_rate": 4.34575e-05,
      "loss": 1.5632,
      "step": 2620
    },
    {
      "epoch": 1.315,
      "grad_norm": 8.630270957946777,
      "learning_rate": 4.34325e-05,
      "loss": 1.7748,
      "step": 2630
    },
    {
      "epoch": 1.32,
      "grad_norm": 9.239152908325195,
      "learning_rate": 4.34075e-05,
      "loss": 1.3948,
      "step": 2640
    },
    {
      "epoch": 1.325,
      "grad_norm": 8.138467788696289,
      "learning_rate": 4.338250000000001e-05,
      "loss": 1.72,
      "step": 2650
    },
    {
      "epoch": 1.33,
      "grad_norm": 8.73186206817627,
      "learning_rate": 4.336e-05,
      "loss": 1.541,
      "step": 2660
    },
    {
      "epoch": 1.335,
      "grad_norm": 6.828509330749512,
      "learning_rate": 4.3335000000000004e-05,
      "loss": 1.6536,
      "step": 2670
    },
    {
      "epoch": 1.34,
      "grad_norm": 10.742618560791016,
      "learning_rate": 4.3310000000000004e-05,
      "loss": 1.4691,
      "step": 2680
    },
    {
      "epoch": 1.345,
      "grad_norm": 15.23116397857666,
      "learning_rate": 4.3285e-05,
      "loss": 1.7938,
      "step": 2690
    },
    {
      "epoch": 1.35,
      "grad_norm": 16.359376907348633,
      "learning_rate": 4.326e-05,
      "loss": 1.644,
      "step": 2700
    },
    {
      "epoch": 1.355,
      "grad_norm": 8.773707389831543,
      "learning_rate": 4.3235e-05,
      "loss": 1.5289,
      "step": 2710
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 6.605494976043701,
      "learning_rate": 4.321e-05,
      "loss": 1.9998,
      "step": 2720
    },
    {
      "epoch": 1.365,
      "grad_norm": 6.830323696136475,
      "learning_rate": 4.3185e-05,
      "loss": 1.5523,
      "step": 2730
    },
    {
      "epoch": 1.37,
      "grad_norm": 7.9728474617004395,
      "learning_rate": 4.316e-05,
      "loss": 1.6923,
      "step": 2740
    },
    {
      "epoch": 1.375,
      "grad_norm": 13.774538040161133,
      "learning_rate": 4.3135000000000006e-05,
      "loss": 1.7257,
      "step": 2750
    },
    {
      "epoch": 1.38,
      "grad_norm": 15.659137725830078,
      "learning_rate": 4.311e-05,
      "loss": 1.7239,
      "step": 2760
    },
    {
      "epoch": 1.385,
      "grad_norm": 9.87157154083252,
      "learning_rate": 4.3085000000000005e-05,
      "loss": 1.4491,
      "step": 2770
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 7.852679252624512,
      "learning_rate": 4.306e-05,
      "loss": 1.3316,
      "step": 2780
    },
    {
      "epoch": 1.395,
      "grad_norm": 8.736557960510254,
      "learning_rate": 4.3035000000000004e-05,
      "loss": 1.3373,
      "step": 2790
    },
    {
      "epoch": 1.4,
      "grad_norm": 10.98764419555664,
      "learning_rate": 4.301e-05,
      "loss": 1.8098,
      "step": 2800
    },
    {
      "epoch": 1.405,
      "grad_norm": 16.459938049316406,
      "learning_rate": 4.2985e-05,
      "loss": 1.6665,
      "step": 2810
    },
    {
      "epoch": 1.41,
      "grad_norm": 15.2486572265625,
      "learning_rate": 4.296e-05,
      "loss": 1.6191,
      "step": 2820
    },
    {
      "epoch": 1.415,
      "grad_norm": 6.376078128814697,
      "learning_rate": 4.2935e-05,
      "loss": 1.6829,
      "step": 2830
    },
    {
      "epoch": 1.42,
      "grad_norm": 9.634902954101562,
      "learning_rate": 4.291e-05,
      "loss": 1.5955,
      "step": 2840
    },
    {
      "epoch": 1.425,
      "grad_norm": 8.113348960876465,
      "learning_rate": 4.2885e-05,
      "loss": 1.4737,
      "step": 2850
    },
    {
      "epoch": 1.43,
      "grad_norm": 9.21866226196289,
      "learning_rate": 4.286e-05,
      "loss": 1.5887,
      "step": 2860
    },
    {
      "epoch": 1.435,
      "grad_norm": 8.00015640258789,
      "learning_rate": 4.2835000000000006e-05,
      "loss": 1.5887,
      "step": 2870
    },
    {
      "epoch": 1.44,
      "grad_norm": 13.65694808959961,
      "learning_rate": 4.281e-05,
      "loss": 1.6555,
      "step": 2880
    },
    {
      "epoch": 1.445,
      "grad_norm": 11.558887481689453,
      "learning_rate": 4.2785000000000005e-05,
      "loss": 1.8227,
      "step": 2890
    },
    {
      "epoch": 1.45,
      "grad_norm": 8.409043312072754,
      "learning_rate": 4.276e-05,
      "loss": 1.595,
      "step": 2900
    },
    {
      "epoch": 1.455,
      "grad_norm": 7.290674686431885,
      "learning_rate": 4.2735e-05,
      "loss": 1.6077,
      "step": 2910
    },
    {
      "epoch": 1.46,
      "grad_norm": 9.408723831176758,
      "learning_rate": 4.271e-05,
      "loss": 1.5739,
      "step": 2920
    },
    {
      "epoch": 1.465,
      "grad_norm": 17.53661346435547,
      "learning_rate": 4.2685e-05,
      "loss": 1.9508,
      "step": 2930
    },
    {
      "epoch": 1.47,
      "grad_norm": 13.770756721496582,
      "learning_rate": 4.266e-05,
      "loss": 2.0087,
      "step": 2940
    },
    {
      "epoch": 1.475,
      "grad_norm": 8.982649803161621,
      "learning_rate": 4.2635e-05,
      "loss": 1.5754,
      "step": 2950
    },
    {
      "epoch": 1.48,
      "grad_norm": 8.622909545898438,
      "learning_rate": 4.261e-05,
      "loss": 1.5574,
      "step": 2960
    },
    {
      "epoch": 1.4849999999999999,
      "grad_norm": 8.245400428771973,
      "learning_rate": 4.2585e-05,
      "loss": 1.7592,
      "step": 2970
    },
    {
      "epoch": 1.49,
      "grad_norm": 7.653885364532471,
      "learning_rate": 4.256e-05,
      "loss": 1.6004,
      "step": 2980
    },
    {
      "epoch": 1.495,
      "grad_norm": 8.525081634521484,
      "learning_rate": 4.2535000000000005e-05,
      "loss": 1.4043,
      "step": 2990
    },
    {
      "epoch": 1.5,
      "grad_norm": 10.02475357055664,
      "learning_rate": 4.251e-05,
      "loss": 1.6093,
      "step": 3000
    },
    {
      "epoch": 1.505,
      "grad_norm": 11.091157913208008,
      "learning_rate": 4.2485000000000004e-05,
      "loss": 1.729,
      "step": 3010
    },
    {
      "epoch": 1.51,
      "grad_norm": 7.009831428527832,
      "learning_rate": 4.246e-05,
      "loss": 1.6469,
      "step": 3020
    },
    {
      "epoch": 1.5150000000000001,
      "grad_norm": 9.165077209472656,
      "learning_rate": 4.2435e-05,
      "loss": 1.8559,
      "step": 3030
    },
    {
      "epoch": 1.52,
      "grad_norm": 10.161787033081055,
      "learning_rate": 4.241e-05,
      "loss": 1.5175,
      "step": 3040
    },
    {
      "epoch": 1.525,
      "grad_norm": 6.144357204437256,
      "learning_rate": 4.2385e-05,
      "loss": 1.5172,
      "step": 3050
    },
    {
      "epoch": 1.53,
      "grad_norm": 8.491695404052734,
      "learning_rate": 4.236e-05,
      "loss": 1.0939,
      "step": 3060
    },
    {
      "epoch": 1.5350000000000001,
      "grad_norm": 8.874382972717285,
      "learning_rate": 4.2335e-05,
      "loss": 1.5323,
      "step": 3070
    },
    {
      "epoch": 1.54,
      "grad_norm": 10.60819149017334,
      "learning_rate": 4.231e-05,
      "loss": 1.4983,
      "step": 3080
    },
    {
      "epoch": 1.545,
      "grad_norm": 8.662959098815918,
      "learning_rate": 4.2285e-05,
      "loss": 1.6533,
      "step": 3090
    },
    {
      "epoch": 1.55,
      "grad_norm": 4.143826961517334,
      "learning_rate": 4.226e-05,
      "loss": 1.3752,
      "step": 3100
    },
    {
      "epoch": 1.5550000000000002,
      "grad_norm": 19.009815216064453,
      "learning_rate": 4.2235000000000005e-05,
      "loss": 1.2651,
      "step": 3110
    },
    {
      "epoch": 1.56,
      "grad_norm": 11.06748104095459,
      "learning_rate": 4.221e-05,
      "loss": 1.4693,
      "step": 3120
    },
    {
      "epoch": 1.565,
      "grad_norm": 14.686559677124023,
      "learning_rate": 4.2185000000000004e-05,
      "loss": 1.7243,
      "step": 3130
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 15.334303855895996,
      "learning_rate": 4.2159999999999996e-05,
      "loss": 1.5415,
      "step": 3140
    },
    {
      "epoch": 1.575,
      "grad_norm": 7.968599796295166,
      "learning_rate": 4.2135e-05,
      "loss": 1.6206,
      "step": 3150
    },
    {
      "epoch": 1.58,
      "grad_norm": 8.973098754882812,
      "learning_rate": 4.211e-05,
      "loss": 1.8009,
      "step": 3160
    },
    {
      "epoch": 1.585,
      "grad_norm": 9.974983215332031,
      "learning_rate": 4.2085e-05,
      "loss": 1.4558,
      "step": 3170
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 6.952636241912842,
      "learning_rate": 4.206e-05,
      "loss": 1.5713,
      "step": 3180
    },
    {
      "epoch": 1.595,
      "grad_norm": 13.083369255065918,
      "learning_rate": 4.2035e-05,
      "loss": 1.8338,
      "step": 3190
    },
    {
      "epoch": 1.6,
      "grad_norm": 8.150562286376953,
      "learning_rate": 4.201e-05,
      "loss": 1.4565,
      "step": 3200
    },
    {
      "epoch": 1.605,
      "grad_norm": 9.37905502319336,
      "learning_rate": 4.1985000000000005e-05,
      "loss": 1.3833,
      "step": 3210
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 3.4618921279907227,
      "learning_rate": 4.196e-05,
      "loss": 1.5943,
      "step": 3220
    },
    {
      "epoch": 1.615,
      "grad_norm": 15.788688659667969,
      "learning_rate": 4.1935000000000004e-05,
      "loss": 1.9259,
      "step": 3230
    },
    {
      "epoch": 1.62,
      "grad_norm": 18.01934051513672,
      "learning_rate": 4.191e-05,
      "loss": 1.9767,
      "step": 3240
    },
    {
      "epoch": 1.625,
      "grad_norm": 9.772944450378418,
      "learning_rate": 4.1885e-05,
      "loss": 1.4958,
      "step": 3250
    },
    {
      "epoch": 1.63,
      "grad_norm": 9.223639488220215,
      "learning_rate": 4.186e-05,
      "loss": 1.6342,
      "step": 3260
    },
    {
      "epoch": 1.635,
      "grad_norm": 8.744850158691406,
      "learning_rate": 4.1835e-05,
      "loss": 1.5488,
      "step": 3270
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 7.283483982086182,
      "learning_rate": 4.181000000000001e-05,
      "loss": 1.7255,
      "step": 3280
    },
    {
      "epoch": 1.645,
      "grad_norm": 11.014413833618164,
      "learning_rate": 4.1785e-05,
      "loss": 1.6099,
      "step": 3290
    },
    {
      "epoch": 1.65,
      "grad_norm": 13.14474868774414,
      "learning_rate": 4.176000000000001e-05,
      "loss": 1.6958,
      "step": 3300
    },
    {
      "epoch": 1.655,
      "grad_norm": 7.5278801918029785,
      "learning_rate": 4.1735e-05,
      "loss": 1.7309,
      "step": 3310
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 7.193887710571289,
      "learning_rate": 4.1710000000000006e-05,
      "loss": 1.6016,
      "step": 3320
    },
    {
      "epoch": 1.665,
      "grad_norm": 12.027106285095215,
      "learning_rate": 4.1685000000000005e-05,
      "loss": 1.4424,
      "step": 3330
    },
    {
      "epoch": 1.67,
      "grad_norm": 8.767804145812988,
      "learning_rate": 4.1660000000000004e-05,
      "loss": 1.7131,
      "step": 3340
    },
    {
      "epoch": 1.675,
      "grad_norm": 6.410271167755127,
      "learning_rate": 4.1635000000000004e-05,
      "loss": 1.7487,
      "step": 3350
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 8.830024719238281,
      "learning_rate": 4.161e-05,
      "loss": 1.88,
      "step": 3360
    },
    {
      "epoch": 1.685,
      "grad_norm": 10.799652099609375,
      "learning_rate": 4.1585e-05,
      "loss": 1.765,
      "step": 3370
    },
    {
      "epoch": 1.69,
      "grad_norm": 10.734052658081055,
      "learning_rate": 4.156e-05,
      "loss": 1.6707,
      "step": 3380
    },
    {
      "epoch": 1.6949999999999998,
      "grad_norm": 10.24603271484375,
      "learning_rate": 4.1535e-05,
      "loss": 1.6079,
      "step": 3390
    },
    {
      "epoch": 1.7,
      "grad_norm": 10.355643272399902,
      "learning_rate": 4.151000000000001e-05,
      "loss": 1.3445,
      "step": 3400
    },
    {
      "epoch": 1.705,
      "grad_norm": 11.229644775390625,
      "learning_rate": 4.1485e-05,
      "loss": 1.5602,
      "step": 3410
    },
    {
      "epoch": 1.71,
      "grad_norm": 8.431278228759766,
      "learning_rate": 4.1460000000000006e-05,
      "loss": 1.6969,
      "step": 3420
    },
    {
      "epoch": 1.7149999999999999,
      "grad_norm": 7.937234401702881,
      "learning_rate": 4.1435e-05,
      "loss": 1.7231,
      "step": 3430
    },
    {
      "epoch": 1.72,
      "grad_norm": 10.238293647766113,
      "learning_rate": 4.1410000000000005e-05,
      "loss": 1.3849,
      "step": 3440
    },
    {
      "epoch": 1.725,
      "grad_norm": 6.602165699005127,
      "learning_rate": 4.1385000000000004e-05,
      "loss": 1.7867,
      "step": 3450
    },
    {
      "epoch": 1.73,
      "grad_norm": 7.014896869659424,
      "learning_rate": 4.1360000000000004e-05,
      "loss": 1.7002,
      "step": 3460
    },
    {
      "epoch": 1.7349999999999999,
      "grad_norm": 16.535539627075195,
      "learning_rate": 4.1335e-05,
      "loss": 2.052,
      "step": 3470
    },
    {
      "epoch": 1.74,
      "grad_norm": 7.908024311065674,
      "learning_rate": 4.131e-05,
      "loss": 1.8041,
      "step": 3480
    },
    {
      "epoch": 1.745,
      "grad_norm": 12.01170825958252,
      "learning_rate": 4.1285e-05,
      "loss": 1.6777,
      "step": 3490
    },
    {
      "epoch": 1.75,
      "grad_norm": 11.231730461120605,
      "learning_rate": 4.126e-05,
      "loss": 1.7367,
      "step": 3500
    },
    {
      "epoch": 1.755,
      "grad_norm": 10.20438289642334,
      "learning_rate": 4.1235e-05,
      "loss": 1.5177,
      "step": 3510
    },
    {
      "epoch": 1.76,
      "grad_norm": 6.296086311340332,
      "learning_rate": 4.121000000000001e-05,
      "loss": 1.5128,
      "step": 3520
    },
    {
      "epoch": 1.7650000000000001,
      "grad_norm": 8.817604064941406,
      "learning_rate": 4.1185e-05,
      "loss": 1.4021,
      "step": 3530
    },
    {
      "epoch": 1.77,
      "grad_norm": 18.34825897216797,
      "learning_rate": 4.1160000000000006e-05,
      "loss": 1.4583,
      "step": 3540
    },
    {
      "epoch": 1.775,
      "grad_norm": 10.351868629455566,
      "learning_rate": 4.1135e-05,
      "loss": 1.3592,
      "step": 3550
    },
    {
      "epoch": 1.78,
      "grad_norm": 13.465177536010742,
      "learning_rate": 4.1110000000000005e-05,
      "loss": 1.3432,
      "step": 3560
    },
    {
      "epoch": 1.7850000000000001,
      "grad_norm": 12.5111083984375,
      "learning_rate": 4.1085000000000004e-05,
      "loss": 1.5179,
      "step": 3570
    },
    {
      "epoch": 1.79,
      "grad_norm": 7.632635593414307,
      "learning_rate": 4.106e-05,
      "loss": 1.256,
      "step": 3580
    },
    {
      "epoch": 1.795,
      "grad_norm": 7.48136568069458,
      "learning_rate": 4.1035e-05,
      "loss": 1.7148,
      "step": 3590
    },
    {
      "epoch": 1.8,
      "grad_norm": 9.88239860534668,
      "learning_rate": 4.101e-05,
      "loss": 1.9178,
      "step": 3600
    },
    {
      "epoch": 1.8050000000000002,
      "grad_norm": 12.105342864990234,
      "learning_rate": 4.0985e-05,
      "loss": 1.8315,
      "step": 3610
    },
    {
      "epoch": 1.81,
      "grad_norm": 6.993898868560791,
      "learning_rate": 4.096e-05,
      "loss": 1.6835,
      "step": 3620
    },
    {
      "epoch": 1.815,
      "grad_norm": 9.919766426086426,
      "learning_rate": 4.0935e-05,
      "loss": 1.5312,
      "step": 3630
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 11.109541893005371,
      "learning_rate": 4.0910000000000006e-05,
      "loss": 1.322,
      "step": 3640
    },
    {
      "epoch": 1.825,
      "grad_norm": 10.122454643249512,
      "learning_rate": 4.0885e-05,
      "loss": 1.6034,
      "step": 3650
    },
    {
      "epoch": 1.83,
      "grad_norm": 6.761075973510742,
      "learning_rate": 4.0860000000000005e-05,
      "loss": 1.5545,
      "step": 3660
    },
    {
      "epoch": 1.835,
      "grad_norm": 10.046451568603516,
      "learning_rate": 4.0835e-05,
      "loss": 1.4092,
      "step": 3670
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 7.729410171508789,
      "learning_rate": 4.0810000000000004e-05,
      "loss": 1.3464,
      "step": 3680
    },
    {
      "epoch": 1.845,
      "grad_norm": 7.231525897979736,
      "learning_rate": 4.0785e-05,
      "loss": 1.4946,
      "step": 3690
    },
    {
      "epoch": 1.85,
      "grad_norm": 17.638866424560547,
      "learning_rate": 4.076e-05,
      "loss": 1.6207,
      "step": 3700
    },
    {
      "epoch": 1.855,
      "grad_norm": 8.306151390075684,
      "learning_rate": 4.0735e-05,
      "loss": 1.5969,
      "step": 3710
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 5.666535377502441,
      "learning_rate": 4.071e-05,
      "loss": 1.6069,
      "step": 3720
    },
    {
      "epoch": 1.865,
      "grad_norm": 6.6701741218566895,
      "learning_rate": 4.0685e-05,
      "loss": 1.7103,
      "step": 3730
    },
    {
      "epoch": 1.87,
      "grad_norm": 9.245301246643066,
      "learning_rate": 4.066e-05,
      "loss": 1.6169,
      "step": 3740
    },
    {
      "epoch": 1.875,
      "grad_norm": 7.419826984405518,
      "learning_rate": 4.0635e-05,
      "loss": 1.969,
      "step": 3750
    },
    {
      "epoch": 1.88,
      "grad_norm": 6.548005104064941,
      "learning_rate": 4.0610000000000006e-05,
      "loss": 1.5928,
      "step": 3760
    },
    {
      "epoch": 1.885,
      "grad_norm": 9.63798999786377,
      "learning_rate": 4.0585e-05,
      "loss": 1.7396,
      "step": 3770
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 13.221266746520996,
      "learning_rate": 4.0560000000000005e-05,
      "loss": 1.5667,
      "step": 3780
    },
    {
      "epoch": 1.895,
      "grad_norm": 9.631896018981934,
      "learning_rate": 4.0535000000000004e-05,
      "loss": 1.5712,
      "step": 3790
    },
    {
      "epoch": 1.9,
      "grad_norm": 27.075136184692383,
      "learning_rate": 4.0510000000000003e-05,
      "loss": 1.5551,
      "step": 3800
    },
    {
      "epoch": 1.905,
      "grad_norm": 9.434245109558105,
      "learning_rate": 4.0485e-05,
      "loss": 1.7692,
      "step": 3810
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 12.278076171875,
      "learning_rate": 4.046e-05,
      "loss": 1.7487,
      "step": 3820
    },
    {
      "epoch": 1.915,
      "grad_norm": 9.249336242675781,
      "learning_rate": 4.0435e-05,
      "loss": 1.6608,
      "step": 3830
    },
    {
      "epoch": 1.92,
      "grad_norm": 21.361221313476562,
      "learning_rate": 4.041e-05,
      "loss": 1.8323,
      "step": 3840
    },
    {
      "epoch": 1.925,
      "grad_norm": 9.500113487243652,
      "learning_rate": 4.0385e-05,
      "loss": 1.5946,
      "step": 3850
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 7.336633682250977,
      "learning_rate": 4.0360000000000007e-05,
      "loss": 1.393,
      "step": 3860
    },
    {
      "epoch": 1.935,
      "grad_norm": 5.940166473388672,
      "learning_rate": 4.0335e-05,
      "loss": 1.9947,
      "step": 3870
    },
    {
      "epoch": 1.94,
      "grad_norm": 13.233604431152344,
      "learning_rate": 4.0310000000000005e-05,
      "loss": 1.8418,
      "step": 3880
    },
    {
      "epoch": 1.9449999999999998,
      "grad_norm": 7.061764717102051,
      "learning_rate": 4.0285e-05,
      "loss": 1.6563,
      "step": 3890
    },
    {
      "epoch": 1.95,
      "grad_norm": 16.69906997680664,
      "learning_rate": 4.0260000000000004e-05,
      "loss": 1.6796,
      "step": 3900
    },
    {
      "epoch": 1.955,
      "grad_norm": 8.797480583190918,
      "learning_rate": 4.0235000000000004e-05,
      "loss": 1.8193,
      "step": 3910
    },
    {
      "epoch": 1.96,
      "grad_norm": 12.814628601074219,
      "learning_rate": 4.021e-05,
      "loss": 1.5859,
      "step": 3920
    },
    {
      "epoch": 1.9649999999999999,
      "grad_norm": 7.598232746124268,
      "learning_rate": 4.0185e-05,
      "loss": 1.9381,
      "step": 3930
    },
    {
      "epoch": 1.97,
      "grad_norm": 8.910966873168945,
      "learning_rate": 4.016e-05,
      "loss": 1.4107,
      "step": 3940
    },
    {
      "epoch": 1.975,
      "grad_norm": 8.870133399963379,
      "learning_rate": 4.0135e-05,
      "loss": 1.6569,
      "step": 3950
    },
    {
      "epoch": 1.98,
      "grad_norm": 15.0877103805542,
      "learning_rate": 4.011e-05,
      "loss": 1.6896,
      "step": 3960
    },
    {
      "epoch": 1.9849999999999999,
      "grad_norm": 7.655755996704102,
      "learning_rate": 4.0085e-05,
      "loss": 1.4511,
      "step": 3970
    },
    {
      "epoch": 1.99,
      "grad_norm": 10.301888465881348,
      "learning_rate": 4.0060000000000006e-05,
      "loss": 1.3849,
      "step": 3980
    },
    {
      "epoch": 1.995,
      "grad_norm": 7.997803688049316,
      "learning_rate": 4.0035e-05,
      "loss": 1.3714,
      "step": 3990
    },
    {
      "epoch": 2.0,
      "grad_norm": 7.517192840576172,
      "learning_rate": 4.0010000000000005e-05,
      "loss": 1.6337,
      "step": 4000
    },
    {
      "epoch": 2.005,
      "grad_norm": 29.548274993896484,
      "learning_rate": 3.9985e-05,
      "loss": 2.0132,
      "step": 4010
    },
    {
      "epoch": 2.01,
      "grad_norm": 10.380570411682129,
      "learning_rate": 3.9960000000000004e-05,
      "loss": 1.5342,
      "step": 4020
    },
    {
      "epoch": 2.015,
      "grad_norm": 12.428079605102539,
      "learning_rate": 3.9935e-05,
      "loss": 1.9041,
      "step": 4030
    },
    {
      "epoch": 2.02,
      "grad_norm": 5.915400505065918,
      "learning_rate": 3.991e-05,
      "loss": 1.7024,
      "step": 4040
    },
    {
      "epoch": 2.025,
      "grad_norm": 12.311240196228027,
      "learning_rate": 3.9885e-05,
      "loss": 1.6298,
      "step": 4050
    },
    {
      "epoch": 2.03,
      "grad_norm": 10.885382652282715,
      "learning_rate": 3.986e-05,
      "loss": 1.7841,
      "step": 4060
    },
    {
      "epoch": 2.035,
      "grad_norm": 6.884726524353027,
      "learning_rate": 3.9835e-05,
      "loss": 1.3277,
      "step": 4070
    },
    {
      "epoch": 2.04,
      "grad_norm": 8.077675819396973,
      "learning_rate": 3.981e-05,
      "loss": 1.5913,
      "step": 4080
    },
    {
      "epoch": 2.045,
      "grad_norm": 12.003521919250488,
      "learning_rate": 3.9785e-05,
      "loss": 1.6317,
      "step": 4090
    },
    {
      "epoch": 2.05,
      "grad_norm": 10.22240924835205,
      "learning_rate": 3.9760000000000006e-05,
      "loss": 1.4197,
      "step": 4100
    },
    {
      "epoch": 2.055,
      "grad_norm": 7.89169979095459,
      "learning_rate": 3.9735e-05,
      "loss": 1.6975,
      "step": 4110
    },
    {
      "epoch": 2.06,
      "grad_norm": 7.539993762969971,
      "learning_rate": 3.9710000000000004e-05,
      "loss": 1.5273,
      "step": 4120
    },
    {
      "epoch": 2.065,
      "grad_norm": 7.813481330871582,
      "learning_rate": 3.9685e-05,
      "loss": 1.5427,
      "step": 4130
    },
    {
      "epoch": 2.07,
      "grad_norm": 9.713252067565918,
      "learning_rate": 3.966e-05,
      "loss": 1.4791,
      "step": 4140
    },
    {
      "epoch": 2.075,
      "grad_norm": 12.03116512298584,
      "learning_rate": 3.9635e-05,
      "loss": 1.4462,
      "step": 4150
    },
    {
      "epoch": 2.08,
      "grad_norm": 13.552224159240723,
      "learning_rate": 3.961e-05,
      "loss": 1.3845,
      "step": 4160
    },
    {
      "epoch": 2.085,
      "grad_norm": 13.89531421661377,
      "learning_rate": 3.9585e-05,
      "loss": 1.3537,
      "step": 4170
    },
    {
      "epoch": 2.09,
      "grad_norm": 17.605445861816406,
      "learning_rate": 3.956e-05,
      "loss": 1.5942,
      "step": 4180
    },
    {
      "epoch": 2.095,
      "grad_norm": 7.269989967346191,
      "learning_rate": 3.9535e-05,
      "loss": 1.3953,
      "step": 4190
    },
    {
      "epoch": 2.1,
      "grad_norm": 9.149984359741211,
      "learning_rate": 3.951e-05,
      "loss": 1.7413,
      "step": 4200
    },
    {
      "epoch": 2.105,
      "grad_norm": 7.4949541091918945,
      "learning_rate": 3.9485e-05,
      "loss": 1.4833,
      "step": 4210
    },
    {
      "epoch": 2.11,
      "grad_norm": 10.371108055114746,
      "learning_rate": 3.9462500000000004e-05,
      "loss": 1.4249,
      "step": 4220
    },
    {
      "epoch": 2.115,
      "grad_norm": 6.609796524047852,
      "learning_rate": 3.9437499999999996e-05,
      "loss": 1.3619,
      "step": 4230
    },
    {
      "epoch": 2.12,
      "grad_norm": 10.850663185119629,
      "learning_rate": 3.94125e-05,
      "loss": 1.3806,
      "step": 4240
    },
    {
      "epoch": 2.125,
      "grad_norm": 10.163415908813477,
      "learning_rate": 3.93875e-05,
      "loss": 1.4942,
      "step": 4250
    },
    {
      "epoch": 2.13,
      "grad_norm": 10.967293739318848,
      "learning_rate": 3.93625e-05,
      "loss": 1.8294,
      "step": 4260
    },
    {
      "epoch": 2.135,
      "grad_norm": 7.226285934448242,
      "learning_rate": 3.93375e-05,
      "loss": 1.642,
      "step": 4270
    },
    {
      "epoch": 2.14,
      "grad_norm": 9.857945442199707,
      "learning_rate": 3.93125e-05,
      "loss": 1.5216,
      "step": 4280
    },
    {
      "epoch": 2.145,
      "grad_norm": 6.295992374420166,
      "learning_rate": 3.92875e-05,
      "loss": 1.7964,
      "step": 4290
    },
    {
      "epoch": 2.15,
      "grad_norm": 8.677556991577148,
      "learning_rate": 3.92625e-05,
      "loss": 1.4066,
      "step": 4300
    },
    {
      "epoch": 2.155,
      "grad_norm": 8.695527076721191,
      "learning_rate": 3.92375e-05,
      "loss": 1.8134,
      "step": 4310
    },
    {
      "epoch": 2.16,
      "grad_norm": 13.010847091674805,
      "learning_rate": 3.9212500000000004e-05,
      "loss": 1.6422,
      "step": 4320
    },
    {
      "epoch": 2.165,
      "grad_norm": 7.156426906585693,
      "learning_rate": 3.91875e-05,
      "loss": 1.602,
      "step": 4330
    },
    {
      "epoch": 2.17,
      "grad_norm": 9.071676254272461,
      "learning_rate": 3.91625e-05,
      "loss": 1.528,
      "step": 4340
    },
    {
      "epoch": 2.175,
      "grad_norm": 15.54190444946289,
      "learning_rate": 3.9137499999999996e-05,
      "loss": 1.2373,
      "step": 4350
    },
    {
      "epoch": 2.18,
      "grad_norm": 10.401132583618164,
      "learning_rate": 3.91125e-05,
      "loss": 1.2024,
      "step": 4360
    },
    {
      "epoch": 2.185,
      "grad_norm": 6.97878360748291,
      "learning_rate": 3.90875e-05,
      "loss": 1.7047,
      "step": 4370
    },
    {
      "epoch": 2.19,
      "grad_norm": 9.460738182067871,
      "learning_rate": 3.90625e-05,
      "loss": 1.6141,
      "step": 4380
    },
    {
      "epoch": 2.195,
      "grad_norm": 8.147629737854004,
      "learning_rate": 3.903750000000001e-05,
      "loss": 1.3078,
      "step": 4390
    },
    {
      "epoch": 2.2,
      "grad_norm": 10.385946273803711,
      "learning_rate": 3.90125e-05,
      "loss": 1.7757,
      "step": 4400
    },
    {
      "epoch": 2.205,
      "grad_norm": 11.845568656921387,
      "learning_rate": 3.8987500000000006e-05,
      "loss": 1.6487,
      "step": 4410
    },
    {
      "epoch": 2.21,
      "grad_norm": 10.983026504516602,
      "learning_rate": 3.8962500000000005e-05,
      "loss": 1.3203,
      "step": 4420
    },
    {
      "epoch": 2.215,
      "grad_norm": 11.399007797241211,
      "learning_rate": 3.8937500000000005e-05,
      "loss": 1.5768,
      "step": 4430
    },
    {
      "epoch": 2.22,
      "grad_norm": 7.355456829071045,
      "learning_rate": 3.8912500000000004e-05,
      "loss": 1.7018,
      "step": 4440
    },
    {
      "epoch": 2.225,
      "grad_norm": 6.427388668060303,
      "learning_rate": 3.88875e-05,
      "loss": 1.5899,
      "step": 4450
    },
    {
      "epoch": 2.23,
      "grad_norm": 13.634779930114746,
      "learning_rate": 3.88625e-05,
      "loss": 1.3747,
      "step": 4460
    },
    {
      "epoch": 2.235,
      "grad_norm": 11.668275833129883,
      "learning_rate": 3.88375e-05,
      "loss": 1.7687,
      "step": 4470
    },
    {
      "epoch": 2.24,
      "grad_norm": 9.957845687866211,
      "learning_rate": 3.88125e-05,
      "loss": 1.3568,
      "step": 4480
    },
    {
      "epoch": 2.245,
      "grad_norm": 12.424996376037598,
      "learning_rate": 3.878750000000001e-05,
      "loss": 1.2784,
      "step": 4490
    },
    {
      "epoch": 2.25,
      "grad_norm": 13.858396530151367,
      "learning_rate": 3.87625e-05,
      "loss": 1.6106,
      "step": 4500
    },
    {
      "epoch": 2.255,
      "grad_norm": 14.10987377166748,
      "learning_rate": 3.8737500000000006e-05,
      "loss": 1.3419,
      "step": 4510
    },
    {
      "epoch": 2.26,
      "grad_norm": 7.588776588439941,
      "learning_rate": 3.87125e-05,
      "loss": 1.5387,
      "step": 4520
    },
    {
      "epoch": 2.265,
      "grad_norm": 12.558246612548828,
      "learning_rate": 3.8687500000000005e-05,
      "loss": 1.4466,
      "step": 4530
    },
    {
      "epoch": 2.27,
      "grad_norm": 7.869293212890625,
      "learning_rate": 3.8662500000000005e-05,
      "loss": 1.4654,
      "step": 4540
    },
    {
      "epoch": 2.275,
      "grad_norm": 27.4487361907959,
      "learning_rate": 3.8637500000000004e-05,
      "loss": 1.6568,
      "step": 4550
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 7.358959674835205,
      "learning_rate": 3.8612500000000003e-05,
      "loss": 1.6387,
      "step": 4560
    },
    {
      "epoch": 2.285,
      "grad_norm": 9.398415565490723,
      "learning_rate": 3.85875e-05,
      "loss": 1.4819,
      "step": 4570
    },
    {
      "epoch": 2.29,
      "grad_norm": 11.713688850402832,
      "learning_rate": 3.85625e-05,
      "loss": 1.4467,
      "step": 4580
    },
    {
      "epoch": 2.295,
      "grad_norm": 7.5950775146484375,
      "learning_rate": 3.85375e-05,
      "loss": 1.6601,
      "step": 4590
    },
    {
      "epoch": 2.3,
      "grad_norm": 8.313240051269531,
      "learning_rate": 3.85125e-05,
      "loss": 1.4401,
      "step": 4600
    },
    {
      "epoch": 2.305,
      "grad_norm": 8.153502464294434,
      "learning_rate": 3.848750000000001e-05,
      "loss": 1.7567,
      "step": 4610
    },
    {
      "epoch": 2.31,
      "grad_norm": 10.378583908081055,
      "learning_rate": 3.84625e-05,
      "loss": 1.7302,
      "step": 4620
    },
    {
      "epoch": 2.315,
      "grad_norm": 19.998022079467773,
      "learning_rate": 3.8437500000000006e-05,
      "loss": 1.4159,
      "step": 4630
    },
    {
      "epoch": 2.32,
      "grad_norm": 12.366617202758789,
      "learning_rate": 3.84125e-05,
      "loss": 1.567,
      "step": 4640
    },
    {
      "epoch": 2.325,
      "grad_norm": 3.8003106117248535,
      "learning_rate": 3.8387500000000005e-05,
      "loss": 1.3669,
      "step": 4650
    },
    {
      "epoch": 2.33,
      "grad_norm": 6.428892135620117,
      "learning_rate": 3.8362500000000004e-05,
      "loss": 1.5859,
      "step": 4660
    },
    {
      "epoch": 2.335,
      "grad_norm": 8.83416748046875,
      "learning_rate": 3.8337500000000004e-05,
      "loss": 1.161,
      "step": 4670
    },
    {
      "epoch": 2.34,
      "grad_norm": 7.0275468826293945,
      "learning_rate": 3.83125e-05,
      "loss": 1.5211,
      "step": 4680
    },
    {
      "epoch": 2.3449999999999998,
      "grad_norm": 3.104827404022217,
      "learning_rate": 3.82875e-05,
      "loss": 1.4275,
      "step": 4690
    },
    {
      "epoch": 2.35,
      "grad_norm": 6.70772123336792,
      "learning_rate": 3.82625e-05,
      "loss": 1.144,
      "step": 4700
    },
    {
      "epoch": 2.355,
      "grad_norm": 10.61034870147705,
      "learning_rate": 3.82375e-05,
      "loss": 1.4912,
      "step": 4710
    },
    {
      "epoch": 2.36,
      "grad_norm": 16.109777450561523,
      "learning_rate": 3.82125e-05,
      "loss": 1.591,
      "step": 4720
    },
    {
      "epoch": 2.365,
      "grad_norm": 17.644929885864258,
      "learning_rate": 3.818750000000001e-05,
      "loss": 1.3561,
      "step": 4730
    },
    {
      "epoch": 2.37,
      "grad_norm": 10.277438163757324,
      "learning_rate": 3.81625e-05,
      "loss": 1.7342,
      "step": 4740
    },
    {
      "epoch": 2.375,
      "grad_norm": 7.217495441436768,
      "learning_rate": 3.8137500000000005e-05,
      "loss": 1.52,
      "step": 4750
    },
    {
      "epoch": 2.38,
      "grad_norm": 10.29116439819336,
      "learning_rate": 3.81125e-05,
      "loss": 1.3457,
      "step": 4760
    },
    {
      "epoch": 2.385,
      "grad_norm": 6.446997165679932,
      "learning_rate": 3.8087500000000004e-05,
      "loss": 1.2621,
      "step": 4770
    },
    {
      "epoch": 2.39,
      "grad_norm": 21.531620025634766,
      "learning_rate": 3.8062500000000004e-05,
      "loss": 1.7832,
      "step": 4780
    },
    {
      "epoch": 2.395,
      "grad_norm": 8.046610832214355,
      "learning_rate": 3.80375e-05,
      "loss": 1.4255,
      "step": 4790
    },
    {
      "epoch": 2.4,
      "grad_norm": 10.635957717895508,
      "learning_rate": 3.80125e-05,
      "loss": 1.473,
      "step": 4800
    },
    {
      "epoch": 2.4050000000000002,
      "grad_norm": 9.349068641662598,
      "learning_rate": 3.79875e-05,
      "loss": 1.5096,
      "step": 4810
    },
    {
      "epoch": 2.41,
      "grad_norm": 7.659237384796143,
      "learning_rate": 3.79625e-05,
      "loss": 1.722,
      "step": 4820
    },
    {
      "epoch": 2.415,
      "grad_norm": 7.550572395324707,
      "learning_rate": 3.79375e-05,
      "loss": 1.2722,
      "step": 4830
    },
    {
      "epoch": 2.42,
      "grad_norm": 8.967445373535156,
      "learning_rate": 3.79125e-05,
      "loss": 1.6209,
      "step": 4840
    },
    {
      "epoch": 2.425,
      "grad_norm": 10.268033981323242,
      "learning_rate": 3.7887500000000006e-05,
      "loss": 1.7607,
      "step": 4850
    },
    {
      "epoch": 2.43,
      "grad_norm": 8.505724906921387,
      "learning_rate": 3.78625e-05,
      "loss": 1.3511,
      "step": 4860
    },
    {
      "epoch": 2.435,
      "grad_norm": 7.206505298614502,
      "learning_rate": 3.7837500000000005e-05,
      "loss": 1.6351,
      "step": 4870
    },
    {
      "epoch": 2.44,
      "grad_norm": 7.493270397186279,
      "learning_rate": 3.78125e-05,
      "loss": 1.791,
      "step": 4880
    },
    {
      "epoch": 2.445,
      "grad_norm": 8.978544235229492,
      "learning_rate": 3.7787500000000004e-05,
      "loss": 1.4744,
      "step": 4890
    },
    {
      "epoch": 2.45,
      "grad_norm": 10.823158264160156,
      "learning_rate": 3.77625e-05,
      "loss": 1.283,
      "step": 4900
    },
    {
      "epoch": 2.455,
      "grad_norm": 7.909604549407959,
      "learning_rate": 3.77375e-05,
      "loss": 1.4227,
      "step": 4910
    },
    {
      "epoch": 2.46,
      "grad_norm": 10.411463737487793,
      "learning_rate": 3.77125e-05,
      "loss": 1.7231,
      "step": 4920
    },
    {
      "epoch": 2.465,
      "grad_norm": 3.200828790664673,
      "learning_rate": 3.76875e-05,
      "loss": 1.1883,
      "step": 4930
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 6.714842796325684,
      "learning_rate": 3.76625e-05,
      "loss": 1.765,
      "step": 4940
    },
    {
      "epoch": 2.475,
      "grad_norm": 8.452503204345703,
      "learning_rate": 3.76375e-05,
      "loss": 1.7745,
      "step": 4950
    },
    {
      "epoch": 2.48,
      "grad_norm": 14.474701881408691,
      "learning_rate": 3.76125e-05,
      "loss": 1.6069,
      "step": 4960
    },
    {
      "epoch": 2.485,
      "grad_norm": 8.392477035522461,
      "learning_rate": 3.7587500000000006e-05,
      "loss": 1.5255,
      "step": 4970
    },
    {
      "epoch": 2.49,
      "grad_norm": 12.97439956665039,
      "learning_rate": 3.75625e-05,
      "loss": 1.7018,
      "step": 4980
    },
    {
      "epoch": 2.495,
      "grad_norm": 13.606085777282715,
      "learning_rate": 3.7537500000000004e-05,
      "loss": 1.8226,
      "step": 4990
    },
    {
      "epoch": 2.5,
      "grad_norm": 9.132789611816406,
      "learning_rate": 3.7512500000000004e-05,
      "loss": 1.4645,
      "step": 5000
    },
    {
      "epoch": 2.505,
      "grad_norm": 6.7923502922058105,
      "learning_rate": 3.74875e-05,
      "loss": 1.6015,
      "step": 5010
    },
    {
      "epoch": 2.51,
      "grad_norm": 6.346796035766602,
      "learning_rate": 3.74625e-05,
      "loss": 1.5418,
      "step": 5020
    },
    {
      "epoch": 2.515,
      "grad_norm": 9.964658737182617,
      "learning_rate": 3.74375e-05,
      "loss": 1.2495,
      "step": 5030
    },
    {
      "epoch": 2.52,
      "grad_norm": 14.517550468444824,
      "learning_rate": 3.74125e-05,
      "loss": 1.6656,
      "step": 5040
    },
    {
      "epoch": 2.525,
      "grad_norm": 7.182751655578613,
      "learning_rate": 3.73875e-05,
      "loss": 1.583,
      "step": 5050
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 8.487001419067383,
      "learning_rate": 3.73625e-05,
      "loss": 1.4032,
      "step": 5060
    },
    {
      "epoch": 2.535,
      "grad_norm": 11.140013694763184,
      "learning_rate": 3.7337500000000006e-05,
      "loss": 1.4809,
      "step": 5070
    },
    {
      "epoch": 2.54,
      "grad_norm": 9.42970085144043,
      "learning_rate": 3.73125e-05,
      "loss": 1.501,
      "step": 5080
    },
    {
      "epoch": 2.545,
      "grad_norm": 7.630992889404297,
      "learning_rate": 3.7287500000000005e-05,
      "loss": 1.8725,
      "step": 5090
    },
    {
      "epoch": 2.55,
      "grad_norm": 23.214824676513672,
      "learning_rate": 3.72625e-05,
      "loss": 1.4854,
      "step": 5100
    },
    {
      "epoch": 2.555,
      "grad_norm": 11.436480522155762,
      "learning_rate": 3.7237500000000004e-05,
      "loss": 1.7117,
      "step": 5110
    },
    {
      "epoch": 2.56,
      "grad_norm": 10.755311012268066,
      "learning_rate": 3.72125e-05,
      "loss": 1.7009,
      "step": 5120
    },
    {
      "epoch": 2.565,
      "grad_norm": 8.880395889282227,
      "learning_rate": 3.71875e-05,
      "loss": 1.6682,
      "step": 5130
    },
    {
      "epoch": 2.57,
      "grad_norm": 6.262720584869385,
      "learning_rate": 3.71625e-05,
      "loss": 1.7208,
      "step": 5140
    },
    {
      "epoch": 2.575,
      "grad_norm": 10.652743339538574,
      "learning_rate": 3.71375e-05,
      "loss": 1.7341,
      "step": 5150
    },
    {
      "epoch": 2.58,
      "grad_norm": 7.125066757202148,
      "learning_rate": 3.71125e-05,
      "loss": 1.5858,
      "step": 5160
    },
    {
      "epoch": 2.585,
      "grad_norm": 12.48741626739502,
      "learning_rate": 3.70875e-05,
      "loss": 1.5241,
      "step": 5170
    },
    {
      "epoch": 2.59,
      "grad_norm": 20.06245994567871,
      "learning_rate": 3.70625e-05,
      "loss": 1.5709,
      "step": 5180
    },
    {
      "epoch": 2.5949999999999998,
      "grad_norm": 6.837488651275635,
      "learning_rate": 3.7037500000000006e-05,
      "loss": 1.6013,
      "step": 5190
    },
    {
      "epoch": 2.6,
      "grad_norm": 6.3693437576293945,
      "learning_rate": 3.70125e-05,
      "loss": 1.451,
      "step": 5200
    },
    {
      "epoch": 2.605,
      "grad_norm": 17.83270835876465,
      "learning_rate": 3.6987500000000005e-05,
      "loss": 1.443,
      "step": 5210
    },
    {
      "epoch": 2.61,
      "grad_norm": 9.77480411529541,
      "learning_rate": 3.69625e-05,
      "loss": 1.2307,
      "step": 5220
    },
    {
      "epoch": 2.615,
      "grad_norm": 11.153515815734863,
      "learning_rate": 3.69375e-05,
      "loss": 1.6778,
      "step": 5230
    },
    {
      "epoch": 2.62,
      "grad_norm": 12.5931396484375,
      "learning_rate": 3.69125e-05,
      "loss": 1.5133,
      "step": 5240
    },
    {
      "epoch": 2.625,
      "grad_norm": 6.385532379150391,
      "learning_rate": 3.68875e-05,
      "loss": 1.4616,
      "step": 5250
    },
    {
      "epoch": 2.63,
      "grad_norm": 6.501345157623291,
      "learning_rate": 3.68625e-05,
      "loss": 1.8362,
      "step": 5260
    },
    {
      "epoch": 2.635,
      "grad_norm": 15.829642295837402,
      "learning_rate": 3.68375e-05,
      "loss": 1.1934,
      "step": 5270
    },
    {
      "epoch": 2.64,
      "grad_norm": 7.107297420501709,
      "learning_rate": 3.68125e-05,
      "loss": 1.6007,
      "step": 5280
    },
    {
      "epoch": 2.645,
      "grad_norm": 19.46446418762207,
      "learning_rate": 3.67875e-05,
      "loss": 1.4911,
      "step": 5290
    },
    {
      "epoch": 2.65,
      "grad_norm": 18.337730407714844,
      "learning_rate": 3.67625e-05,
      "loss": 1.5984,
      "step": 5300
    },
    {
      "epoch": 2.6550000000000002,
      "grad_norm": 6.712299346923828,
      "learning_rate": 3.6737500000000005e-05,
      "loss": 1.2112,
      "step": 5310
    },
    {
      "epoch": 2.66,
      "grad_norm": 10.290264129638672,
      "learning_rate": 3.67125e-05,
      "loss": 1.4425,
      "step": 5320
    },
    {
      "epoch": 2.665,
      "grad_norm": 8.21638011932373,
      "learning_rate": 3.6687500000000004e-05,
      "loss": 1.5339,
      "step": 5330
    },
    {
      "epoch": 2.67,
      "grad_norm": 7.051238536834717,
      "learning_rate": 3.66625e-05,
      "loss": 1.5631,
      "step": 5340
    },
    {
      "epoch": 2.675,
      "grad_norm": 17.11984634399414,
      "learning_rate": 3.66375e-05,
      "loss": 1.5312,
      "step": 5350
    },
    {
      "epoch": 2.68,
      "grad_norm": 10.328176498413086,
      "learning_rate": 3.66125e-05,
      "loss": 2.0191,
      "step": 5360
    },
    {
      "epoch": 2.685,
      "grad_norm": 6.272273540496826,
      "learning_rate": 3.65875e-05,
      "loss": 1.7432,
      "step": 5370
    },
    {
      "epoch": 2.69,
      "grad_norm": 7.402473449707031,
      "learning_rate": 3.65625e-05,
      "loss": 1.7238,
      "step": 5380
    },
    {
      "epoch": 2.695,
      "grad_norm": 13.839458465576172,
      "learning_rate": 3.65375e-05,
      "loss": 1.6417,
      "step": 5390
    },
    {
      "epoch": 2.7,
      "grad_norm": 8.700540542602539,
      "learning_rate": 3.65125e-05,
      "loss": 1.4645,
      "step": 5400
    },
    {
      "epoch": 2.705,
      "grad_norm": 6.619481563568115,
      "learning_rate": 3.64875e-05,
      "loss": 1.6594,
      "step": 5410
    },
    {
      "epoch": 2.71,
      "grad_norm": 7.70797872543335,
      "learning_rate": 3.64625e-05,
      "loss": 1.4437,
      "step": 5420
    },
    {
      "epoch": 2.715,
      "grad_norm": 6.883327484130859,
      "learning_rate": 3.6437500000000005e-05,
      "loss": 1.5042,
      "step": 5430
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 14.204277038574219,
      "learning_rate": 3.64125e-05,
      "loss": 1.7248,
      "step": 5440
    },
    {
      "epoch": 2.725,
      "grad_norm": 11.851014137268066,
      "learning_rate": 3.6387500000000004e-05,
      "loss": 1.5881,
      "step": 5450
    },
    {
      "epoch": 2.73,
      "grad_norm": 8.693702697753906,
      "learning_rate": 3.6362499999999996e-05,
      "loss": 1.4178,
      "step": 5460
    },
    {
      "epoch": 2.735,
      "grad_norm": 26.9830379486084,
      "learning_rate": 3.63375e-05,
      "loss": 1.5295,
      "step": 5470
    },
    {
      "epoch": 2.74,
      "grad_norm": 8.784981727600098,
      "learning_rate": 3.63125e-05,
      "loss": 1.5775,
      "step": 5480
    },
    {
      "epoch": 2.745,
      "grad_norm": 13.652511596679688,
      "learning_rate": 3.62875e-05,
      "loss": 1.678,
      "step": 5490
    },
    {
      "epoch": 2.75,
      "grad_norm": 8.157998085021973,
      "learning_rate": 3.62625e-05,
      "loss": 1.5002,
      "step": 5500
    },
    {
      "epoch": 2.755,
      "grad_norm": 9.399581909179688,
      "learning_rate": 3.62375e-05,
      "loss": 1.713,
      "step": 5510
    },
    {
      "epoch": 2.76,
      "grad_norm": 13.437451362609863,
      "learning_rate": 3.62125e-05,
      "loss": 1.8822,
      "step": 5520
    },
    {
      "epoch": 2.765,
      "grad_norm": 9.201004981994629,
      "learning_rate": 3.61875e-05,
      "loss": 1.8421,
      "step": 5530
    },
    {
      "epoch": 2.77,
      "grad_norm": 14.131949424743652,
      "learning_rate": 3.61625e-05,
      "loss": 1.4323,
      "step": 5540
    },
    {
      "epoch": 2.775,
      "grad_norm": 14.167556762695312,
      "learning_rate": 3.6137500000000004e-05,
      "loss": 1.5126,
      "step": 5550
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 15.826218605041504,
      "learning_rate": 3.61125e-05,
      "loss": 1.3248,
      "step": 5560
    },
    {
      "epoch": 2.785,
      "grad_norm": 6.577475070953369,
      "learning_rate": 3.60875e-05,
      "loss": 1.5569,
      "step": 5570
    },
    {
      "epoch": 2.79,
      "grad_norm": 11.126265525817871,
      "learning_rate": 3.60625e-05,
      "loss": 1.2916,
      "step": 5580
    },
    {
      "epoch": 2.795,
      "grad_norm": 7.059723854064941,
      "learning_rate": 3.60375e-05,
      "loss": 1.5562,
      "step": 5590
    },
    {
      "epoch": 2.8,
      "grad_norm": 13.411592483520508,
      "learning_rate": 3.60125e-05,
      "loss": 1.4586,
      "step": 5600
    },
    {
      "epoch": 2.805,
      "grad_norm": 6.998680591583252,
      "learning_rate": 3.59875e-05,
      "loss": 1.7027,
      "step": 5610
    },
    {
      "epoch": 2.81,
      "grad_norm": 13.559686660766602,
      "learning_rate": 3.59625e-05,
      "loss": 1.6793,
      "step": 5620
    },
    {
      "epoch": 2.815,
      "grad_norm": 6.395432472229004,
      "learning_rate": 3.59375e-05,
      "loss": 1.3253,
      "step": 5630
    },
    {
      "epoch": 2.82,
      "grad_norm": 10.542591094970703,
      "learning_rate": 3.5912500000000006e-05,
      "loss": 1.7066,
      "step": 5640
    },
    {
      "epoch": 2.825,
      "grad_norm": 12.481143951416016,
      "learning_rate": 3.5887500000000005e-05,
      "loss": 1.3092,
      "step": 5650
    },
    {
      "epoch": 2.83,
      "grad_norm": 12.079197883605957,
      "learning_rate": 3.5862500000000004e-05,
      "loss": 1.5703,
      "step": 5660
    },
    {
      "epoch": 2.835,
      "grad_norm": 12.589863777160645,
      "learning_rate": 3.5837500000000004e-05,
      "loss": 1.5345,
      "step": 5670
    },
    {
      "epoch": 2.84,
      "grad_norm": 8.965704917907715,
      "learning_rate": 3.58125e-05,
      "loss": 1.5155,
      "step": 5680
    },
    {
      "epoch": 2.8449999999999998,
      "grad_norm": 7.513121128082275,
      "learning_rate": 3.57875e-05,
      "loss": 1.6213,
      "step": 5690
    },
    {
      "epoch": 2.85,
      "grad_norm": 7.742425918579102,
      "learning_rate": 3.57625e-05,
      "loss": 1.5167,
      "step": 5700
    },
    {
      "epoch": 2.855,
      "grad_norm": 10.342485427856445,
      "learning_rate": 3.57375e-05,
      "loss": 1.6813,
      "step": 5710
    },
    {
      "epoch": 2.86,
      "grad_norm": 7.990780830383301,
      "learning_rate": 3.571250000000001e-05,
      "loss": 1.4238,
      "step": 5720
    },
    {
      "epoch": 2.865,
      "grad_norm": 10.667190551757812,
      "learning_rate": 3.56875e-05,
      "loss": 1.5493,
      "step": 5730
    },
    {
      "epoch": 2.87,
      "grad_norm": 8.068975448608398,
      "learning_rate": 3.5662500000000006e-05,
      "loss": 1.1826,
      "step": 5740
    },
    {
      "epoch": 2.875,
      "grad_norm": 6.719467639923096,
      "learning_rate": 3.56375e-05,
      "loss": 1.5115,
      "step": 5750
    },
    {
      "epoch": 2.88,
      "grad_norm": 9.285650253295898,
      "learning_rate": 3.5612500000000005e-05,
      "loss": 1.6639,
      "step": 5760
    },
    {
      "epoch": 2.885,
      "grad_norm": 7.456259727478027,
      "learning_rate": 3.5587500000000004e-05,
      "loss": 1.516,
      "step": 5770
    },
    {
      "epoch": 2.89,
      "grad_norm": 12.072890281677246,
      "learning_rate": 3.5562500000000004e-05,
      "loss": 1.2158,
      "step": 5780
    },
    {
      "epoch": 2.895,
      "grad_norm": 7.185075759887695,
      "learning_rate": 3.55375e-05,
      "loss": 1.5966,
      "step": 5790
    },
    {
      "epoch": 2.9,
      "grad_norm": 10.75937557220459,
      "learning_rate": 3.55125e-05,
      "loss": 1.4333,
      "step": 5800
    },
    {
      "epoch": 2.9050000000000002,
      "grad_norm": 7.738787651062012,
      "learning_rate": 3.54875e-05,
      "loss": 1.4248,
      "step": 5810
    },
    {
      "epoch": 2.91,
      "grad_norm": 7.927933692932129,
      "learning_rate": 3.54625e-05,
      "loss": 1.3463,
      "step": 5820
    },
    {
      "epoch": 2.915,
      "grad_norm": 5.464616298675537,
      "learning_rate": 3.54375e-05,
      "loss": 1.5295,
      "step": 5830
    },
    {
      "epoch": 2.92,
      "grad_norm": 9.521881103515625,
      "learning_rate": 3.541250000000001e-05,
      "loss": 1.685,
      "step": 5840
    },
    {
      "epoch": 2.925,
      "grad_norm": 21.973142623901367,
      "learning_rate": 3.53875e-05,
      "loss": 1.3176,
      "step": 5850
    },
    {
      "epoch": 2.93,
      "grad_norm": 7.824239253997803,
      "learning_rate": 3.5362500000000006e-05,
      "loss": 1.5446,
      "step": 5860
    },
    {
      "epoch": 2.935,
      "grad_norm": 11.498483657836914,
      "learning_rate": 3.53375e-05,
      "loss": 1.427,
      "step": 5870
    },
    {
      "epoch": 2.94,
      "grad_norm": 18.514488220214844,
      "learning_rate": 3.5312500000000005e-05,
      "loss": 1.575,
      "step": 5880
    },
    {
      "epoch": 2.945,
      "grad_norm": 7.487447261810303,
      "learning_rate": 3.5287500000000004e-05,
      "loss": 1.7185,
      "step": 5890
    },
    {
      "epoch": 2.95,
      "grad_norm": 10.07534122467041,
      "learning_rate": 3.52625e-05,
      "loss": 1.4372,
      "step": 5900
    },
    {
      "epoch": 2.955,
      "grad_norm": 8.925116539001465,
      "learning_rate": 3.52375e-05,
      "loss": 1.6598,
      "step": 5910
    },
    {
      "epoch": 2.96,
      "grad_norm": 9.632100105285645,
      "learning_rate": 3.52125e-05,
      "loss": 1.4584,
      "step": 5920
    },
    {
      "epoch": 2.965,
      "grad_norm": 10.496789932250977,
      "learning_rate": 3.51875e-05,
      "loss": 1.4087,
      "step": 5930
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 13.588639259338379,
      "learning_rate": 3.51625e-05,
      "loss": 1.4756,
      "step": 5940
    },
    {
      "epoch": 2.975,
      "grad_norm": 10.659615516662598,
      "learning_rate": 3.51375e-05,
      "loss": 1.6285,
      "step": 5950
    },
    {
      "epoch": 2.98,
      "grad_norm": 30.02465057373047,
      "learning_rate": 3.5112500000000006e-05,
      "loss": 1.3153,
      "step": 5960
    },
    {
      "epoch": 2.985,
      "grad_norm": 7.24056339263916,
      "learning_rate": 3.50875e-05,
      "loss": 1.2392,
      "step": 5970
    },
    {
      "epoch": 2.99,
      "grad_norm": 6.489123344421387,
      "learning_rate": 3.5062500000000005e-05,
      "loss": 1.5096,
      "step": 5980
    },
    {
      "epoch": 2.995,
      "grad_norm": 7.744417190551758,
      "learning_rate": 3.50375e-05,
      "loss": 1.6413,
      "step": 5990
    },
    {
      "epoch": 3.0,
      "grad_norm": 13.638358116149902,
      "learning_rate": 3.5012500000000004e-05,
      "loss": 1.2032,
      "step": 6000
    },
    {
      "epoch": 3.005,
      "grad_norm": 9.783443450927734,
      "learning_rate": 3.4987500000000003e-05,
      "loss": 1.2334,
      "step": 6010
    },
    {
      "epoch": 3.01,
      "grad_norm": 9.56747055053711,
      "learning_rate": 3.49625e-05,
      "loss": 1.4983,
      "step": 6020
    },
    {
      "epoch": 3.015,
      "grad_norm": 10.66152572631836,
      "learning_rate": 3.49375e-05,
      "loss": 1.5479,
      "step": 6030
    },
    {
      "epoch": 3.02,
      "grad_norm": 8.247488975524902,
      "learning_rate": 3.49125e-05,
      "loss": 1.4563,
      "step": 6040
    },
    {
      "epoch": 3.025,
      "grad_norm": 7.788524150848389,
      "learning_rate": 3.48875e-05,
      "loss": 1.4801,
      "step": 6050
    },
    {
      "epoch": 3.03,
      "grad_norm": 11.99933910369873,
      "learning_rate": 3.48625e-05,
      "loss": 1.6373,
      "step": 6060
    },
    {
      "epoch": 3.035,
      "grad_norm": 10.22061538696289,
      "learning_rate": 3.48375e-05,
      "loss": 1.2508,
      "step": 6070
    },
    {
      "epoch": 3.04,
      "grad_norm": 13.258261680603027,
      "learning_rate": 3.4812500000000006e-05,
      "loss": 1.3927,
      "step": 6080
    },
    {
      "epoch": 3.045,
      "grad_norm": 27.927913665771484,
      "learning_rate": 3.47875e-05,
      "loss": 1.7882,
      "step": 6090
    },
    {
      "epoch": 3.05,
      "grad_norm": 8.431940078735352,
      "learning_rate": 3.4762500000000005e-05,
      "loss": 1.4172,
      "step": 6100
    },
    {
      "epoch": 3.055,
      "grad_norm": 14.80800724029541,
      "learning_rate": 3.47375e-05,
      "loss": 1.5434,
      "step": 6110
    },
    {
      "epoch": 3.06,
      "grad_norm": 7.587470531463623,
      "learning_rate": 3.4712500000000003e-05,
      "loss": 1.4945,
      "step": 6120
    },
    {
      "epoch": 3.065,
      "grad_norm": 7.420233726501465,
      "learning_rate": 3.46875e-05,
      "loss": 1.246,
      "step": 6130
    },
    {
      "epoch": 3.07,
      "grad_norm": 10.323399543762207,
      "learning_rate": 3.46625e-05,
      "loss": 1.575,
      "step": 6140
    },
    {
      "epoch": 3.075,
      "grad_norm": 17.72161293029785,
      "learning_rate": 3.46375e-05,
      "loss": 1.4162,
      "step": 6150
    },
    {
      "epoch": 3.08,
      "grad_norm": 9.265731811523438,
      "learning_rate": 3.46125e-05,
      "loss": 1.4005,
      "step": 6160
    },
    {
      "epoch": 3.085,
      "grad_norm": 11.533852577209473,
      "learning_rate": 3.45875e-05,
      "loss": 1.446,
      "step": 6170
    },
    {
      "epoch": 3.09,
      "grad_norm": 9.429510116577148,
      "learning_rate": 3.45625e-05,
      "loss": 1.5549,
      "step": 6180
    },
    {
      "epoch": 3.095,
      "grad_norm": 8.082201957702637,
      "learning_rate": 3.45375e-05,
      "loss": 1.3687,
      "step": 6190
    },
    {
      "epoch": 3.1,
      "grad_norm": 7.704873085021973,
      "learning_rate": 3.4512500000000005e-05,
      "loss": 1.7888,
      "step": 6200
    },
    {
      "epoch": 3.105,
      "grad_norm": 14.11915111541748,
      "learning_rate": 3.44875e-05,
      "loss": 1.4232,
      "step": 6210
    },
    {
      "epoch": 3.11,
      "grad_norm": 8.965232849121094,
      "learning_rate": 3.4462500000000004e-05,
      "loss": 1.2464,
      "step": 6220
    },
    {
      "epoch": 3.115,
      "grad_norm": 8.876532554626465,
      "learning_rate": 3.4437500000000004e-05,
      "loss": 1.4349,
      "step": 6230
    },
    {
      "epoch": 3.12,
      "grad_norm": 2.7262778282165527,
      "learning_rate": 3.44125e-05,
      "loss": 1.4789,
      "step": 6240
    },
    {
      "epoch": 3.125,
      "grad_norm": 6.0449419021606445,
      "learning_rate": 3.43875e-05,
      "loss": 1.3081,
      "step": 6250
    },
    {
      "epoch": 3.13,
      "grad_norm": 10.121428489685059,
      "learning_rate": 3.43625e-05,
      "loss": 1.545,
      "step": 6260
    },
    {
      "epoch": 3.135,
      "grad_norm": 8.831905364990234,
      "learning_rate": 3.43375e-05,
      "loss": 1.3825,
      "step": 6270
    },
    {
      "epoch": 3.14,
      "grad_norm": 8.888433456420898,
      "learning_rate": 3.43125e-05,
      "loss": 1.1855,
      "step": 6280
    },
    {
      "epoch": 3.145,
      "grad_norm": 12.898433685302734,
      "learning_rate": 3.42875e-05,
      "loss": 1.2013,
      "step": 6290
    },
    {
      "epoch": 3.15,
      "grad_norm": 9.36144733428955,
      "learning_rate": 3.4262500000000006e-05,
      "loss": 1.4578,
      "step": 6300
    },
    {
      "epoch": 3.155,
      "grad_norm": 7.780472755432129,
      "learning_rate": 3.42375e-05,
      "loss": 1.4316,
      "step": 6310
    },
    {
      "epoch": 3.16,
      "grad_norm": 10.061441421508789,
      "learning_rate": 3.4212500000000005e-05,
      "loss": 1.433,
      "step": 6320
    },
    {
      "epoch": 3.165,
      "grad_norm": 7.9719061851501465,
      "learning_rate": 3.41875e-05,
      "loss": 1.0331,
      "step": 6330
    },
    {
      "epoch": 3.17,
      "grad_norm": 10.46671199798584,
      "learning_rate": 3.4162500000000004e-05,
      "loss": 1.7022,
      "step": 6340
    },
    {
      "epoch": 3.175,
      "grad_norm": 7.462320327758789,
      "learning_rate": 3.41375e-05,
      "loss": 1.4365,
      "step": 6350
    },
    {
      "epoch": 3.18,
      "grad_norm": 9.362756729125977,
      "learning_rate": 3.41125e-05,
      "loss": 1.5774,
      "step": 6360
    },
    {
      "epoch": 3.185,
      "grad_norm": 7.7195963859558105,
      "learning_rate": 3.40875e-05,
      "loss": 1.7628,
      "step": 6370
    },
    {
      "epoch": 3.19,
      "grad_norm": 12.959572792053223,
      "learning_rate": 3.40625e-05,
      "loss": 1.5224,
      "step": 6380
    },
    {
      "epoch": 3.195,
      "grad_norm": 14.472750663757324,
      "learning_rate": 3.40375e-05,
      "loss": 1.6128,
      "step": 6390
    },
    {
      "epoch": 3.2,
      "grad_norm": 8.6771821975708,
      "learning_rate": 3.40125e-05,
      "loss": 1.4926,
      "step": 6400
    },
    {
      "epoch": 3.205,
      "grad_norm": 8.245676040649414,
      "learning_rate": 3.39875e-05,
      "loss": 1.3858,
      "step": 6410
    },
    {
      "epoch": 3.21,
      "grad_norm": 9.202908515930176,
      "learning_rate": 3.3962500000000006e-05,
      "loss": 1.5843,
      "step": 6420
    },
    {
      "epoch": 3.215,
      "grad_norm": 7.183185577392578,
      "learning_rate": 3.39375e-05,
      "loss": 1.5241,
      "step": 6430
    },
    {
      "epoch": 3.22,
      "grad_norm": 14.1058931350708,
      "learning_rate": 3.3912500000000004e-05,
      "loss": 1.5165,
      "step": 6440
    },
    {
      "epoch": 3.225,
      "grad_norm": 13.440195083618164,
      "learning_rate": 3.38875e-05,
      "loss": 1.4779,
      "step": 6450
    },
    {
      "epoch": 3.23,
      "grad_norm": 9.209671020507812,
      "learning_rate": 3.38625e-05,
      "loss": 1.5562,
      "step": 6460
    },
    {
      "epoch": 3.235,
      "grad_norm": 6.403295040130615,
      "learning_rate": 3.38375e-05,
      "loss": 1.5925,
      "step": 6470
    },
    {
      "epoch": 3.24,
      "grad_norm": 9.134736061096191,
      "learning_rate": 3.38125e-05,
      "loss": 1.5468,
      "step": 6480
    },
    {
      "epoch": 3.245,
      "grad_norm": 7.399838924407959,
      "learning_rate": 3.37875e-05,
      "loss": 1.4887,
      "step": 6490
    },
    {
      "epoch": 3.25,
      "grad_norm": 9.731321334838867,
      "learning_rate": 3.37625e-05,
      "loss": 1.4285,
      "step": 6500
    },
    {
      "epoch": 3.255,
      "grad_norm": 9.968575477600098,
      "learning_rate": 3.37375e-05,
      "loss": 1.3418,
      "step": 6510
    },
    {
      "epoch": 3.26,
      "grad_norm": 8.861763000488281,
      "learning_rate": 3.37125e-05,
      "loss": 1.5323,
      "step": 6520
    },
    {
      "epoch": 3.265,
      "grad_norm": 8.354070663452148,
      "learning_rate": 3.36875e-05,
      "loss": 1.6895,
      "step": 6530
    },
    {
      "epoch": 3.27,
      "grad_norm": 11.177382469177246,
      "learning_rate": 3.3662500000000005e-05,
      "loss": 1.7349,
      "step": 6540
    },
    {
      "epoch": 3.275,
      "grad_norm": 9.375929832458496,
      "learning_rate": 3.36375e-05,
      "loss": 1.3005,
      "step": 6550
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 11.09555721282959,
      "learning_rate": 3.3612500000000004e-05,
      "loss": 1.8578,
      "step": 6560
    },
    {
      "epoch": 3.285,
      "grad_norm": 9.180990219116211,
      "learning_rate": 3.3587499999999996e-05,
      "loss": 1.6583,
      "step": 6570
    },
    {
      "epoch": 3.29,
      "grad_norm": 10.256180763244629,
      "learning_rate": 3.35625e-05,
      "loss": 1.2409,
      "step": 6580
    },
    {
      "epoch": 3.295,
      "grad_norm": 10.96385383605957,
      "learning_rate": 3.35375e-05,
      "loss": 1.7734,
      "step": 6590
    },
    {
      "epoch": 3.3,
      "grad_norm": 8.64236068725586,
      "learning_rate": 3.35125e-05,
      "loss": 1.4966,
      "step": 6600
    },
    {
      "epoch": 3.305,
      "grad_norm": 16.558361053466797,
      "learning_rate": 3.34875e-05,
      "loss": 1.3647,
      "step": 6610
    },
    {
      "epoch": 3.31,
      "grad_norm": 16.048553466796875,
      "learning_rate": 3.34625e-05,
      "loss": 1.7215,
      "step": 6620
    },
    {
      "epoch": 3.315,
      "grad_norm": 12.860170364379883,
      "learning_rate": 3.34375e-05,
      "loss": 1.5705,
      "step": 6630
    },
    {
      "epoch": 3.32,
      "grad_norm": 12.815184593200684,
      "learning_rate": 3.34125e-05,
      "loss": 1.3468,
      "step": 6640
    },
    {
      "epoch": 3.325,
      "grad_norm": 6.468233585357666,
      "learning_rate": 3.33875e-05,
      "loss": 1.1501,
      "step": 6650
    },
    {
      "epoch": 3.33,
      "grad_norm": 16.12456512451172,
      "learning_rate": 3.3362500000000005e-05,
      "loss": 1.5986,
      "step": 6660
    },
    {
      "epoch": 3.335,
      "grad_norm": 6.877935886383057,
      "learning_rate": 3.33375e-05,
      "loss": 1.6318,
      "step": 6670
    },
    {
      "epoch": 3.34,
      "grad_norm": 11.846417427062988,
      "learning_rate": 3.33125e-05,
      "loss": 1.4957,
      "step": 6680
    },
    {
      "epoch": 3.3449999999999998,
      "grad_norm": 17.55926513671875,
      "learning_rate": 3.3287499999999996e-05,
      "loss": 1.4368,
      "step": 6690
    },
    {
      "epoch": 3.35,
      "grad_norm": 14.056705474853516,
      "learning_rate": 3.32625e-05,
      "loss": 1.2836,
      "step": 6700
    },
    {
      "epoch": 3.355,
      "grad_norm": 7.565529823303223,
      "learning_rate": 3.32375e-05,
      "loss": 1.7152,
      "step": 6710
    },
    {
      "epoch": 3.36,
      "grad_norm": 8.032981872558594,
      "learning_rate": 3.32125e-05,
      "loss": 1.5327,
      "step": 6720
    },
    {
      "epoch": 3.365,
      "grad_norm": 7.983737945556641,
      "learning_rate": 3.31875e-05,
      "loss": 1.4081,
      "step": 6730
    },
    {
      "epoch": 3.37,
      "grad_norm": 5.575540542602539,
      "learning_rate": 3.31625e-05,
      "loss": 1.4978,
      "step": 6740
    },
    {
      "epoch": 3.375,
      "grad_norm": 13.844257354736328,
      "learning_rate": 3.31375e-05,
      "loss": 1.3919,
      "step": 6750
    },
    {
      "epoch": 3.38,
      "grad_norm": 7.124547958374023,
      "learning_rate": 3.31125e-05,
      "loss": 1.3947,
      "step": 6760
    },
    {
      "epoch": 3.385,
      "grad_norm": 8.911617279052734,
      "learning_rate": 3.30875e-05,
      "loss": 1.5972,
      "step": 6770
    },
    {
      "epoch": 3.39,
      "grad_norm": 12.125348091125488,
      "learning_rate": 3.3062500000000004e-05,
      "loss": 1.4289,
      "step": 6780
    },
    {
      "epoch": 3.395,
      "grad_norm": 14.228611946105957,
      "learning_rate": 3.30375e-05,
      "loss": 1.515,
      "step": 6790
    },
    {
      "epoch": 3.4,
      "grad_norm": 16.270803451538086,
      "learning_rate": 3.30125e-05,
      "loss": 1.2181,
      "step": 6800
    },
    {
      "epoch": 3.4050000000000002,
      "grad_norm": 10.768247604370117,
      "learning_rate": 3.29875e-05,
      "loss": 1.4119,
      "step": 6810
    },
    {
      "epoch": 3.41,
      "grad_norm": 6.315503120422363,
      "learning_rate": 3.29625e-05,
      "loss": 1.4148,
      "step": 6820
    },
    {
      "epoch": 3.415,
      "grad_norm": 7.540452003479004,
      "learning_rate": 3.29375e-05,
      "loss": 1.8251,
      "step": 6830
    },
    {
      "epoch": 3.42,
      "grad_norm": 11.427781105041504,
      "learning_rate": 3.29125e-05,
      "loss": 1.5761,
      "step": 6840
    },
    {
      "epoch": 3.425,
      "grad_norm": 8.088351249694824,
      "learning_rate": 3.28875e-05,
      "loss": 1.6982,
      "step": 6850
    },
    {
      "epoch": 3.43,
      "grad_norm": 17.09528923034668,
      "learning_rate": 3.28625e-05,
      "loss": 1.4476,
      "step": 6860
    },
    {
      "epoch": 3.435,
      "grad_norm": 13.7783203125,
      "learning_rate": 3.28375e-05,
      "loss": 1.5685,
      "step": 6870
    },
    {
      "epoch": 3.44,
      "grad_norm": 14.991658210754395,
      "learning_rate": 3.2812500000000005e-05,
      "loss": 1.5433,
      "step": 6880
    },
    {
      "epoch": 3.445,
      "grad_norm": 6.353715896606445,
      "learning_rate": 3.2787500000000004e-05,
      "loss": 1.4534,
      "step": 6890
    },
    {
      "epoch": 3.45,
      "grad_norm": 7.256793975830078,
      "learning_rate": 3.2762500000000004e-05,
      "loss": 1.4848,
      "step": 6900
    },
    {
      "epoch": 3.455,
      "grad_norm": 9.345955848693848,
      "learning_rate": 3.27375e-05,
      "loss": 1.6163,
      "step": 6910
    },
    {
      "epoch": 3.46,
      "grad_norm": 6.4056572914123535,
      "learning_rate": 3.27125e-05,
      "loss": 1.5087,
      "step": 6920
    },
    {
      "epoch": 3.465,
      "grad_norm": 10.017553329467773,
      "learning_rate": 3.26875e-05,
      "loss": 1.3954,
      "step": 6930
    },
    {
      "epoch": 3.4699999999999998,
      "grad_norm": 9.345970153808594,
      "learning_rate": 3.26625e-05,
      "loss": 1.6048,
      "step": 6940
    },
    {
      "epoch": 3.475,
      "grad_norm": 7.367053508758545,
      "learning_rate": 3.263750000000001e-05,
      "loss": 1.634,
      "step": 6950
    },
    {
      "epoch": 3.48,
      "grad_norm": 10.181425094604492,
      "learning_rate": 3.26125e-05,
      "loss": 1.7888,
      "step": 6960
    },
    {
      "epoch": 3.485,
      "grad_norm": 10.759171485900879,
      "learning_rate": 3.2587500000000006e-05,
      "loss": 1.1763,
      "step": 6970
    },
    {
      "epoch": 3.49,
      "grad_norm": 11.222421646118164,
      "learning_rate": 3.25625e-05,
      "loss": 1.429,
      "step": 6980
    },
    {
      "epoch": 3.495,
      "grad_norm": 13.95791244506836,
      "learning_rate": 3.2537500000000005e-05,
      "loss": 1.4954,
      "step": 6990
    },
    {
      "epoch": 3.5,
      "grad_norm": 18.3088436126709,
      "learning_rate": 3.2512500000000004e-05,
      "loss": 1.5517,
      "step": 7000
    },
    {
      "epoch": 3.505,
      "grad_norm": 7.23599910736084,
      "learning_rate": 3.2487500000000004e-05,
      "loss": 1.4749,
      "step": 7010
    },
    {
      "epoch": 3.51,
      "grad_norm": 7.924882888793945,
      "learning_rate": 3.24625e-05,
      "loss": 1.6238,
      "step": 7020
    },
    {
      "epoch": 3.515,
      "grad_norm": 8.641765594482422,
      "learning_rate": 3.24375e-05,
      "loss": 1.3699,
      "step": 7030
    },
    {
      "epoch": 3.52,
      "grad_norm": 7.746077537536621,
      "learning_rate": 3.24125e-05,
      "loss": 1.3022,
      "step": 7040
    },
    {
      "epoch": 3.525,
      "grad_norm": 11.63641357421875,
      "learning_rate": 3.23875e-05,
      "loss": 1.394,
      "step": 7050
    },
    {
      "epoch": 3.5300000000000002,
      "grad_norm": 12.586685180664062,
      "learning_rate": 3.23625e-05,
      "loss": 1.4967,
      "step": 7060
    },
    {
      "epoch": 3.535,
      "grad_norm": 11.516125679016113,
      "learning_rate": 3.233750000000001e-05,
      "loss": 1.2507,
      "step": 7070
    },
    {
      "epoch": 3.54,
      "grad_norm": 13.643319129943848,
      "learning_rate": 3.23125e-05,
      "loss": 1.3823,
      "step": 7080
    },
    {
      "epoch": 3.545,
      "grad_norm": 8.857266426086426,
      "learning_rate": 3.2287500000000006e-05,
      "loss": 1.2827,
      "step": 7090
    },
    {
      "epoch": 3.55,
      "grad_norm": 18.693864822387695,
      "learning_rate": 3.22625e-05,
      "loss": 1.6243,
      "step": 7100
    },
    {
      "epoch": 3.555,
      "grad_norm": 8.770145416259766,
      "learning_rate": 3.2237500000000004e-05,
      "loss": 1.4349,
      "step": 7110
    },
    {
      "epoch": 3.56,
      "grad_norm": 7.593753814697266,
      "learning_rate": 3.2212500000000004e-05,
      "loss": 1.6155,
      "step": 7120
    },
    {
      "epoch": 3.565,
      "grad_norm": 4.335890769958496,
      "learning_rate": 3.21875e-05,
      "loss": 1.5332,
      "step": 7130
    },
    {
      "epoch": 3.57,
      "grad_norm": 10.287318229675293,
      "learning_rate": 3.21625e-05,
      "loss": 1.4753,
      "step": 7140
    },
    {
      "epoch": 3.575,
      "grad_norm": 9.64264965057373,
      "learning_rate": 3.21375e-05,
      "loss": 1.4184,
      "step": 7150
    },
    {
      "epoch": 3.58,
      "grad_norm": 10.572941780090332,
      "learning_rate": 3.21125e-05,
      "loss": 1.2607,
      "step": 7160
    },
    {
      "epoch": 3.585,
      "grad_norm": 14.219893455505371,
      "learning_rate": 3.20875e-05,
      "loss": 1.544,
      "step": 7170
    },
    {
      "epoch": 3.59,
      "grad_norm": 9.331380844116211,
      "learning_rate": 3.20625e-05,
      "loss": 1.5656,
      "step": 7180
    },
    {
      "epoch": 3.5949999999999998,
      "grad_norm": 14.987732887268066,
      "learning_rate": 3.2037500000000006e-05,
      "loss": 1.5258,
      "step": 7190
    },
    {
      "epoch": 3.6,
      "grad_norm": 7.685049057006836,
      "learning_rate": 3.20125e-05,
      "loss": 1.387,
      "step": 7200
    },
    {
      "epoch": 3.605,
      "grad_norm": 11.469956398010254,
      "learning_rate": 3.1987500000000005e-05,
      "loss": 1.2667,
      "step": 7210
    },
    {
      "epoch": 3.61,
      "grad_norm": 13.915786743164062,
      "learning_rate": 3.19625e-05,
      "loss": 1.346,
      "step": 7220
    },
    {
      "epoch": 3.615,
      "grad_norm": 10.41848087310791,
      "learning_rate": 3.1937500000000004e-05,
      "loss": 1.4009,
      "step": 7230
    },
    {
      "epoch": 3.62,
      "grad_norm": 7.724739074707031,
      "learning_rate": 3.19125e-05,
      "loss": 1.4732,
      "step": 7240
    },
    {
      "epoch": 3.625,
      "grad_norm": 10.31872844696045,
      "learning_rate": 3.18875e-05,
      "loss": 1.505,
      "step": 7250
    },
    {
      "epoch": 3.63,
      "grad_norm": 5.613351345062256,
      "learning_rate": 3.18625e-05,
      "loss": 1.4596,
      "step": 7260
    },
    {
      "epoch": 3.635,
      "grad_norm": 6.643160820007324,
      "learning_rate": 3.18375e-05,
      "loss": 1.428,
      "step": 7270
    },
    {
      "epoch": 3.64,
      "grad_norm": 8.886990547180176,
      "learning_rate": 3.18125e-05,
      "loss": 1.2908,
      "step": 7280
    },
    {
      "epoch": 3.645,
      "grad_norm": 7.079130172729492,
      "learning_rate": 3.17875e-05,
      "loss": 1.273,
      "step": 7290
    },
    {
      "epoch": 3.65,
      "grad_norm": 7.607433795928955,
      "learning_rate": 3.17625e-05,
      "loss": 1.3718,
      "step": 7300
    },
    {
      "epoch": 3.6550000000000002,
      "grad_norm": 14.307188987731934,
      "learning_rate": 3.1737500000000006e-05,
      "loss": 1.4991,
      "step": 7310
    },
    {
      "epoch": 3.66,
      "grad_norm": 8.406176567077637,
      "learning_rate": 3.17125e-05,
      "loss": 1.4142,
      "step": 7320
    },
    {
      "epoch": 3.665,
      "grad_norm": 10.910569190979004,
      "learning_rate": 3.1687500000000005e-05,
      "loss": 1.2313,
      "step": 7330
    },
    {
      "epoch": 3.67,
      "grad_norm": 7.0431623458862305,
      "learning_rate": 3.16625e-05,
      "loss": 1.7936,
      "step": 7340
    },
    {
      "epoch": 3.675,
      "grad_norm": 9.242864608764648,
      "learning_rate": 3.16375e-05,
      "loss": 1.5211,
      "step": 7350
    },
    {
      "epoch": 3.68,
      "grad_norm": 7.996696949005127,
      "learning_rate": 3.16125e-05,
      "loss": 1.5104,
      "step": 7360
    },
    {
      "epoch": 3.685,
      "grad_norm": 9.423362731933594,
      "learning_rate": 3.15875e-05,
      "loss": 1.5676,
      "step": 7370
    },
    {
      "epoch": 3.69,
      "grad_norm": 17.30415916442871,
      "learning_rate": 3.15625e-05,
      "loss": 1.0101,
      "step": 7380
    },
    {
      "epoch": 3.695,
      "grad_norm": 8.607843399047852,
      "learning_rate": 3.15375e-05,
      "loss": 1.7147,
      "step": 7390
    },
    {
      "epoch": 3.7,
      "grad_norm": 9.23193073272705,
      "learning_rate": 3.15125e-05,
      "loss": 1.8256,
      "step": 7400
    },
    {
      "epoch": 3.705,
      "grad_norm": 7.30562162399292,
      "learning_rate": 3.1487500000000006e-05,
      "loss": 1.3995,
      "step": 7410
    },
    {
      "epoch": 3.71,
      "grad_norm": 21.09615707397461,
      "learning_rate": 3.14625e-05,
      "loss": 1.3866,
      "step": 7420
    },
    {
      "epoch": 3.715,
      "grad_norm": 8.645567893981934,
      "learning_rate": 3.1437500000000005e-05,
      "loss": 1.5489,
      "step": 7430
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 7.738624095916748,
      "learning_rate": 3.14125e-05,
      "loss": 1.3699,
      "step": 7440
    },
    {
      "epoch": 3.725,
      "grad_norm": 35.24210739135742,
      "learning_rate": 3.1387500000000004e-05,
      "loss": 1.7159,
      "step": 7450
    },
    {
      "epoch": 3.73,
      "grad_norm": 7.836143970489502,
      "learning_rate": 3.13625e-05,
      "loss": 1.1233,
      "step": 7460
    },
    {
      "epoch": 3.735,
      "grad_norm": 9.440567016601562,
      "learning_rate": 3.13375e-05,
      "loss": 1.761,
      "step": 7470
    },
    {
      "epoch": 3.74,
      "grad_norm": 8.033981323242188,
      "learning_rate": 3.13125e-05,
      "loss": 1.422,
      "step": 7480
    },
    {
      "epoch": 3.745,
      "grad_norm": 9.278284072875977,
      "learning_rate": 3.12875e-05,
      "loss": 1.2748,
      "step": 7490
    },
    {
      "epoch": 3.75,
      "grad_norm": 6.362163543701172,
      "learning_rate": 3.12625e-05,
      "loss": 1.5887,
      "step": 7500
    },
    {
      "epoch": 3.755,
      "grad_norm": 10.811993598937988,
      "learning_rate": 3.12375e-05,
      "loss": 1.5662,
      "step": 7510
    },
    {
      "epoch": 3.76,
      "grad_norm": 12.602799415588379,
      "learning_rate": 3.12125e-05,
      "loss": 1.5599,
      "step": 7520
    },
    {
      "epoch": 3.765,
      "grad_norm": 6.616158962249756,
      "learning_rate": 3.1187500000000006e-05,
      "loss": 1.6863,
      "step": 7530
    },
    {
      "epoch": 3.77,
      "grad_norm": 11.74810791015625,
      "learning_rate": 3.11625e-05,
      "loss": 1.3163,
      "step": 7540
    },
    {
      "epoch": 3.775,
      "grad_norm": 17.236942291259766,
      "learning_rate": 3.1137500000000005e-05,
      "loss": 1.439,
      "step": 7550
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 9.66298770904541,
      "learning_rate": 3.11125e-05,
      "loss": 1.6422,
      "step": 7560
    },
    {
      "epoch": 3.785,
      "grad_norm": 8.999998092651367,
      "learning_rate": 3.1087500000000003e-05,
      "loss": 1.3642,
      "step": 7570
    },
    {
      "epoch": 3.79,
      "grad_norm": 14.84420108795166,
      "learning_rate": 3.10625e-05,
      "loss": 1.4773,
      "step": 7580
    },
    {
      "epoch": 3.795,
      "grad_norm": 6.210453987121582,
      "learning_rate": 3.10375e-05,
      "loss": 1.7233,
      "step": 7590
    },
    {
      "epoch": 3.8,
      "grad_norm": 11.50135326385498,
      "learning_rate": 3.10125e-05,
      "loss": 1.2994,
      "step": 7600
    },
    {
      "epoch": 3.805,
      "grad_norm": 8.697305679321289,
      "learning_rate": 3.09875e-05,
      "loss": 1.2354,
      "step": 7610
    },
    {
      "epoch": 3.81,
      "grad_norm": 9.078288078308105,
      "learning_rate": 3.09625e-05,
      "loss": 1.5251,
      "step": 7620
    },
    {
      "epoch": 3.815,
      "grad_norm": 6.980016231536865,
      "learning_rate": 3.09375e-05,
      "loss": 1.4694,
      "step": 7630
    },
    {
      "epoch": 3.82,
      "grad_norm": 9.234513282775879,
      "learning_rate": 3.09125e-05,
      "loss": 1.3563,
      "step": 7640
    },
    {
      "epoch": 3.825,
      "grad_norm": 9.359816551208496,
      "learning_rate": 3.0887500000000005e-05,
      "loss": 1.4037,
      "step": 7650
    },
    {
      "epoch": 3.83,
      "grad_norm": 14.140165328979492,
      "learning_rate": 3.08625e-05,
      "loss": 1.511,
      "step": 7660
    },
    {
      "epoch": 3.835,
      "grad_norm": 13.281344413757324,
      "learning_rate": 3.0837500000000004e-05,
      "loss": 1.4634,
      "step": 7670
    },
    {
      "epoch": 3.84,
      "grad_norm": 8.232341766357422,
      "learning_rate": 3.08125e-05,
      "loss": 1.6915,
      "step": 7680
    },
    {
      "epoch": 3.8449999999999998,
      "grad_norm": 10.873830795288086,
      "learning_rate": 3.07875e-05,
      "loss": 1.3105,
      "step": 7690
    },
    {
      "epoch": 3.85,
      "grad_norm": 10.118374824523926,
      "learning_rate": 3.07625e-05,
      "loss": 1.4984,
      "step": 7700
    },
    {
      "epoch": 3.855,
      "grad_norm": 18.75079345703125,
      "learning_rate": 3.07375e-05,
      "loss": 1.361,
      "step": 7710
    },
    {
      "epoch": 3.86,
      "grad_norm": 27.13139533996582,
      "learning_rate": 3.07125e-05,
      "loss": 1.6084,
      "step": 7720
    },
    {
      "epoch": 3.865,
      "grad_norm": 9.186125755310059,
      "learning_rate": 3.06875e-05,
      "loss": 1.5485,
      "step": 7730
    },
    {
      "epoch": 3.87,
      "grad_norm": 8.785423278808594,
      "learning_rate": 3.06625e-05,
      "loss": 1.5822,
      "step": 7740
    },
    {
      "epoch": 3.875,
      "grad_norm": 10.409818649291992,
      "learning_rate": 3.06375e-05,
      "loss": 1.7217,
      "step": 7750
    },
    {
      "epoch": 3.88,
      "grad_norm": 8.406376838684082,
      "learning_rate": 3.06125e-05,
      "loss": 1.4682,
      "step": 7760
    },
    {
      "epoch": 3.885,
      "grad_norm": 21.687976837158203,
      "learning_rate": 3.0587500000000005e-05,
      "loss": 1.6775,
      "step": 7770
    },
    {
      "epoch": 3.89,
      "grad_norm": 16.37859535217285,
      "learning_rate": 3.05625e-05,
      "loss": 1.7339,
      "step": 7780
    },
    {
      "epoch": 3.895,
      "grad_norm": 7.624128818511963,
      "learning_rate": 3.0537500000000004e-05,
      "loss": 1.7248,
      "step": 7790
    },
    {
      "epoch": 3.9,
      "grad_norm": 20.090362548828125,
      "learning_rate": 3.05125e-05,
      "loss": 1.1023,
      "step": 7800
    },
    {
      "epoch": 3.9050000000000002,
      "grad_norm": 12.218916893005371,
      "learning_rate": 3.0487500000000002e-05,
      "loss": 1.4182,
      "step": 7810
    },
    {
      "epoch": 3.91,
      "grad_norm": 19.041316986083984,
      "learning_rate": 3.04625e-05,
      "loss": 1.3513,
      "step": 7820
    },
    {
      "epoch": 3.915,
      "grad_norm": 14.063190460205078,
      "learning_rate": 3.04375e-05,
      "loss": 1.7277,
      "step": 7830
    },
    {
      "epoch": 3.92,
      "grad_norm": 7.453534126281738,
      "learning_rate": 3.0415e-05,
      "loss": 1.2986,
      "step": 7840
    },
    {
      "epoch": 3.925,
      "grad_norm": 16.762699127197266,
      "learning_rate": 3.0390000000000002e-05,
      "loss": 1.4957,
      "step": 7850
    },
    {
      "epoch": 3.93,
      "grad_norm": 7.037383556365967,
      "learning_rate": 3.0364999999999998e-05,
      "loss": 1.6434,
      "step": 7860
    },
    {
      "epoch": 3.935,
      "grad_norm": 12.778450012207031,
      "learning_rate": 3.034e-05,
      "loss": 1.5756,
      "step": 7870
    },
    {
      "epoch": 3.94,
      "grad_norm": 11.819231986999512,
      "learning_rate": 3.0315e-05,
      "loss": 1.2387,
      "step": 7880
    },
    {
      "epoch": 3.945,
      "grad_norm": 7.75307559967041,
      "learning_rate": 3.0290000000000003e-05,
      "loss": 1.7116,
      "step": 7890
    },
    {
      "epoch": 3.95,
      "grad_norm": 11.238992691040039,
      "learning_rate": 3.0265e-05,
      "loss": 1.4801,
      "step": 7900
    },
    {
      "epoch": 3.955,
      "grad_norm": 7.341533660888672,
      "learning_rate": 3.0240000000000002e-05,
      "loss": 1.2581,
      "step": 7910
    },
    {
      "epoch": 3.96,
      "grad_norm": 9.114079475402832,
      "learning_rate": 3.0214999999999998e-05,
      "loss": 1.6202,
      "step": 7920
    },
    {
      "epoch": 3.965,
      "grad_norm": 6.360974311828613,
      "learning_rate": 3.019e-05,
      "loss": 1.4987,
      "step": 7930
    },
    {
      "epoch": 3.9699999999999998,
      "grad_norm": 9.260848045349121,
      "learning_rate": 3.0165e-05,
      "loss": 1.4337,
      "step": 7940
    },
    {
      "epoch": 3.975,
      "grad_norm": 12.248190879821777,
      "learning_rate": 3.0140000000000003e-05,
      "loss": 1.4998,
      "step": 7950
    },
    {
      "epoch": 3.98,
      "grad_norm": 9.232980728149414,
      "learning_rate": 3.0115e-05,
      "loss": 1.4604,
      "step": 7960
    },
    {
      "epoch": 3.985,
      "grad_norm": 8.967560768127441,
      "learning_rate": 3.009e-05,
      "loss": 1.7052,
      "step": 7970
    },
    {
      "epoch": 3.99,
      "grad_norm": 14.827289581298828,
      "learning_rate": 3.0064999999999998e-05,
      "loss": 1.4618,
      "step": 7980
    },
    {
      "epoch": 3.995,
      "grad_norm": 15.14310073852539,
      "learning_rate": 3.004e-05,
      "loss": 1.5429,
      "step": 7990
    },
    {
      "epoch": 4.0,
      "grad_norm": 12.129338264465332,
      "learning_rate": 3.0015e-05,
      "loss": 1.6429,
      "step": 8000
    },
    {
      "epoch": 4.005,
      "grad_norm": 10.631627082824707,
      "learning_rate": 2.9990000000000003e-05,
      "loss": 1.3299,
      "step": 8010
    },
    {
      "epoch": 4.01,
      "grad_norm": 9.124531745910645,
      "learning_rate": 2.9965000000000005e-05,
      "loss": 1.2914,
      "step": 8020
    },
    {
      "epoch": 4.015,
      "grad_norm": 7.890542030334473,
      "learning_rate": 2.994e-05,
      "loss": 1.4827,
      "step": 8030
    },
    {
      "epoch": 4.02,
      "grad_norm": 5.814443111419678,
      "learning_rate": 2.9915000000000004e-05,
      "loss": 1.6055,
      "step": 8040
    },
    {
      "epoch": 4.025,
      "grad_norm": 14.579052925109863,
      "learning_rate": 2.989e-05,
      "loss": 1.2669,
      "step": 8050
    },
    {
      "epoch": 4.03,
      "grad_norm": 15.598374366760254,
      "learning_rate": 2.9865000000000003e-05,
      "loss": 1.7858,
      "step": 8060
    },
    {
      "epoch": 4.035,
      "grad_norm": 13.952285766601562,
      "learning_rate": 2.9840000000000002e-05,
      "loss": 1.1094,
      "step": 8070
    },
    {
      "epoch": 4.04,
      "grad_norm": 10.413117408752441,
      "learning_rate": 2.9815000000000005e-05,
      "loss": 1.7733,
      "step": 8080
    },
    {
      "epoch": 4.045,
      "grad_norm": 8.528745651245117,
      "learning_rate": 2.979e-05,
      "loss": 1.5166,
      "step": 8090
    },
    {
      "epoch": 4.05,
      "grad_norm": 12.653043746948242,
      "learning_rate": 2.9765000000000004e-05,
      "loss": 1.2683,
      "step": 8100
    },
    {
      "epoch": 4.055,
      "grad_norm": 6.247384548187256,
      "learning_rate": 2.974e-05,
      "loss": 1.7607,
      "step": 8110
    },
    {
      "epoch": 4.06,
      "grad_norm": 7.218979358673096,
      "learning_rate": 2.9715000000000003e-05,
      "loss": 1.4754,
      "step": 8120
    },
    {
      "epoch": 4.065,
      "grad_norm": 9.99491024017334,
      "learning_rate": 2.9690000000000002e-05,
      "loss": 1.278,
      "step": 8130
    },
    {
      "epoch": 4.07,
      "grad_norm": 21.943565368652344,
      "learning_rate": 2.9665000000000005e-05,
      "loss": 1.4732,
      "step": 8140
    },
    {
      "epoch": 4.075,
      "grad_norm": 12.170018196105957,
      "learning_rate": 2.964e-05,
      "loss": 1.4361,
      "step": 8150
    },
    {
      "epoch": 4.08,
      "grad_norm": 7.0726518630981445,
      "learning_rate": 2.9615000000000004e-05,
      "loss": 1.5223,
      "step": 8160
    },
    {
      "epoch": 4.085,
      "grad_norm": 14.992396354675293,
      "learning_rate": 2.959e-05,
      "loss": 1.4936,
      "step": 8170
    },
    {
      "epoch": 4.09,
      "grad_norm": 11.653772354125977,
      "learning_rate": 2.9565000000000002e-05,
      "loss": 1.034,
      "step": 8180
    },
    {
      "epoch": 4.095,
      "grad_norm": 4.175417423248291,
      "learning_rate": 2.9540000000000002e-05,
      "loss": 1.3701,
      "step": 8190
    },
    {
      "epoch": 4.1,
      "grad_norm": 13.703571319580078,
      "learning_rate": 2.9515000000000005e-05,
      "loss": 1.5395,
      "step": 8200
    },
    {
      "epoch": 4.105,
      "grad_norm": 13.750367164611816,
      "learning_rate": 2.949e-05,
      "loss": 1.7113,
      "step": 8210
    },
    {
      "epoch": 4.11,
      "grad_norm": 6.148093223571777,
      "learning_rate": 2.9465000000000003e-05,
      "loss": 1.4383,
      "step": 8220
    },
    {
      "epoch": 4.115,
      "grad_norm": 7.085668087005615,
      "learning_rate": 2.944e-05,
      "loss": 1.5952,
      "step": 8230
    },
    {
      "epoch": 4.12,
      "grad_norm": 11.254873275756836,
      "learning_rate": 2.9415000000000002e-05,
      "loss": 1.4948,
      "step": 8240
    },
    {
      "epoch": 4.125,
      "grad_norm": 18.37795639038086,
      "learning_rate": 2.939e-05,
      "loss": 1.6971,
      "step": 8250
    },
    {
      "epoch": 4.13,
      "grad_norm": 8.488777160644531,
      "learning_rate": 2.9365000000000004e-05,
      "loss": 1.3776,
      "step": 8260
    },
    {
      "epoch": 4.135,
      "grad_norm": 7.424549579620361,
      "learning_rate": 2.934e-05,
      "loss": 1.6113,
      "step": 8270
    },
    {
      "epoch": 4.14,
      "grad_norm": 15.255836486816406,
      "learning_rate": 2.9315000000000003e-05,
      "loss": 1.496,
      "step": 8280
    },
    {
      "epoch": 4.145,
      "grad_norm": 7.458902359008789,
      "learning_rate": 2.929e-05,
      "loss": 1.2339,
      "step": 8290
    },
    {
      "epoch": 4.15,
      "grad_norm": 7.822423458099365,
      "learning_rate": 2.9265000000000002e-05,
      "loss": 1.5003,
      "step": 8300
    },
    {
      "epoch": 4.155,
      "grad_norm": 14.002677917480469,
      "learning_rate": 2.924e-05,
      "loss": 1.1156,
      "step": 8310
    },
    {
      "epoch": 4.16,
      "grad_norm": 11.598042488098145,
      "learning_rate": 2.9215000000000004e-05,
      "loss": 1.4446,
      "step": 8320
    },
    {
      "epoch": 4.165,
      "grad_norm": 7.0518479347229,
      "learning_rate": 2.919e-05,
      "loss": 1.1675,
      "step": 8330
    },
    {
      "epoch": 4.17,
      "grad_norm": 10.137128829956055,
      "learning_rate": 2.9165000000000003e-05,
      "loss": 1.1577,
      "step": 8340
    },
    {
      "epoch": 4.175,
      "grad_norm": 7.573564529418945,
      "learning_rate": 2.9140000000000002e-05,
      "loss": 1.1531,
      "step": 8350
    },
    {
      "epoch": 4.18,
      "grad_norm": 8.375012397766113,
      "learning_rate": 2.9115000000000005e-05,
      "loss": 1.5738,
      "step": 8360
    },
    {
      "epoch": 4.185,
      "grad_norm": 9.60884952545166,
      "learning_rate": 2.909e-05,
      "loss": 1.6538,
      "step": 8370
    },
    {
      "epoch": 4.19,
      "grad_norm": 7.582770824432373,
      "learning_rate": 2.9065000000000004e-05,
      "loss": 1.4486,
      "step": 8380
    },
    {
      "epoch": 4.195,
      "grad_norm": 10.988290786743164,
      "learning_rate": 2.904e-05,
      "loss": 1.438,
      "step": 8390
    },
    {
      "epoch": 4.2,
      "grad_norm": 18.86988067626953,
      "learning_rate": 2.9015000000000003e-05,
      "loss": 1.1139,
      "step": 8400
    },
    {
      "epoch": 4.205,
      "grad_norm": 10.646881103515625,
      "learning_rate": 2.8990000000000002e-05,
      "loss": 1.3991,
      "step": 8410
    },
    {
      "epoch": 4.21,
      "grad_norm": 15.531493186950684,
      "learning_rate": 2.8965000000000005e-05,
      "loss": 1.597,
      "step": 8420
    },
    {
      "epoch": 4.215,
      "grad_norm": 10.032851219177246,
      "learning_rate": 2.894e-05,
      "loss": 1.5218,
      "step": 8430
    },
    {
      "epoch": 4.22,
      "grad_norm": 8.057167053222656,
      "learning_rate": 2.8915000000000004e-05,
      "loss": 1.3843,
      "step": 8440
    },
    {
      "epoch": 4.225,
      "grad_norm": 11.049592971801758,
      "learning_rate": 2.889e-05,
      "loss": 1.8314,
      "step": 8450
    },
    {
      "epoch": 4.23,
      "grad_norm": 10.198443412780762,
      "learning_rate": 2.8865000000000002e-05,
      "loss": 1.5952,
      "step": 8460
    },
    {
      "epoch": 4.235,
      "grad_norm": 7.201015472412109,
      "learning_rate": 2.8840000000000002e-05,
      "loss": 1.1565,
      "step": 8470
    },
    {
      "epoch": 4.24,
      "grad_norm": 23.339218139648438,
      "learning_rate": 2.8815000000000004e-05,
      "loss": 1.8406,
      "step": 8480
    },
    {
      "epoch": 4.245,
      "grad_norm": 8.43721866607666,
      "learning_rate": 2.879e-05,
      "loss": 1.4626,
      "step": 8490
    },
    {
      "epoch": 4.25,
      "grad_norm": 13.612861633300781,
      "learning_rate": 2.8765000000000003e-05,
      "loss": 1.525,
      "step": 8500
    },
    {
      "epoch": 4.255,
      "grad_norm": 8.55197811126709,
      "learning_rate": 2.874e-05,
      "loss": 1.4832,
      "step": 8510
    },
    {
      "epoch": 4.26,
      "grad_norm": 16.765878677368164,
      "learning_rate": 2.8715000000000002e-05,
      "loss": 1.1604,
      "step": 8520
    },
    {
      "epoch": 4.265,
      "grad_norm": 9.16495418548584,
      "learning_rate": 2.869e-05,
      "loss": 1.3843,
      "step": 8530
    },
    {
      "epoch": 4.27,
      "grad_norm": 17.231416702270508,
      "learning_rate": 2.8665000000000004e-05,
      "loss": 1.6136,
      "step": 8540
    },
    {
      "epoch": 4.275,
      "grad_norm": 11.087506294250488,
      "learning_rate": 2.864e-05,
      "loss": 1.6594,
      "step": 8550
    },
    {
      "epoch": 4.28,
      "grad_norm": 21.129535675048828,
      "learning_rate": 2.8615000000000003e-05,
      "loss": 1.4008,
      "step": 8560
    },
    {
      "epoch": 4.285,
      "grad_norm": 8.734502792358398,
      "learning_rate": 2.859e-05,
      "loss": 1.4034,
      "step": 8570
    },
    {
      "epoch": 4.29,
      "grad_norm": 15.353632926940918,
      "learning_rate": 2.8565000000000002e-05,
      "loss": 1.6769,
      "step": 8580
    },
    {
      "epoch": 4.295,
      "grad_norm": 10.729330062866211,
      "learning_rate": 2.854e-05,
      "loss": 1.6646,
      "step": 8590
    },
    {
      "epoch": 4.3,
      "grad_norm": 8.96481990814209,
      "learning_rate": 2.8515000000000004e-05,
      "loss": 1.6753,
      "step": 8600
    },
    {
      "epoch": 4.305,
      "grad_norm": 10.389375686645508,
      "learning_rate": 2.849e-05,
      "loss": 1.3279,
      "step": 8610
    },
    {
      "epoch": 4.31,
      "grad_norm": 9.020069122314453,
      "learning_rate": 2.8465000000000003e-05,
      "loss": 1.1156,
      "step": 8620
    },
    {
      "epoch": 4.315,
      "grad_norm": 8.623607635498047,
      "learning_rate": 2.844e-05,
      "loss": 1.5671,
      "step": 8630
    },
    {
      "epoch": 4.32,
      "grad_norm": 9.555464744567871,
      "learning_rate": 2.8415e-05,
      "loss": 1.6171,
      "step": 8640
    },
    {
      "epoch": 4.325,
      "grad_norm": 14.222773551940918,
      "learning_rate": 2.839e-05,
      "loss": 1.5088,
      "step": 8650
    },
    {
      "epoch": 4.33,
      "grad_norm": 9.183087348937988,
      "learning_rate": 2.8365000000000004e-05,
      "loss": 1.3446,
      "step": 8660
    },
    {
      "epoch": 4.335,
      "grad_norm": 9.214982986450195,
      "learning_rate": 2.834e-05,
      "loss": 1.5345,
      "step": 8670
    },
    {
      "epoch": 4.34,
      "grad_norm": 5.462508201599121,
      "learning_rate": 2.8315000000000002e-05,
      "loss": 1.2294,
      "step": 8680
    },
    {
      "epoch": 4.345,
      "grad_norm": 12.942300796508789,
      "learning_rate": 2.829e-05,
      "loss": 1.5637,
      "step": 8690
    },
    {
      "epoch": 4.35,
      "grad_norm": 10.040660858154297,
      "learning_rate": 2.8265e-05,
      "loss": 1.7396,
      "step": 8700
    },
    {
      "epoch": 4.355,
      "grad_norm": 8.160989761352539,
      "learning_rate": 2.824e-05,
      "loss": 1.6099,
      "step": 8710
    },
    {
      "epoch": 4.36,
      "grad_norm": 18.49872398376465,
      "learning_rate": 2.8215000000000003e-05,
      "loss": 1.654,
      "step": 8720
    },
    {
      "epoch": 4.365,
      "grad_norm": 10.55043888092041,
      "learning_rate": 2.819e-05,
      "loss": 1.224,
      "step": 8730
    },
    {
      "epoch": 4.37,
      "grad_norm": 15.977619171142578,
      "learning_rate": 2.8165000000000002e-05,
      "loss": 1.5418,
      "step": 8740
    },
    {
      "epoch": 4.375,
      "grad_norm": 7.485982418060303,
      "learning_rate": 2.8139999999999998e-05,
      "loss": 1.1898,
      "step": 8750
    },
    {
      "epoch": 4.38,
      "grad_norm": 16.990264892578125,
      "learning_rate": 2.8115e-05,
      "loss": 1.3786,
      "step": 8760
    },
    {
      "epoch": 4.385,
      "grad_norm": 23.143489837646484,
      "learning_rate": 2.809e-05,
      "loss": 1.3613,
      "step": 8770
    },
    {
      "epoch": 4.39,
      "grad_norm": 16.192209243774414,
      "learning_rate": 2.8065000000000003e-05,
      "loss": 1.4124,
      "step": 8780
    },
    {
      "epoch": 4.395,
      "grad_norm": 15.185565948486328,
      "learning_rate": 2.804e-05,
      "loss": 1.404,
      "step": 8790
    },
    {
      "epoch": 4.4,
      "grad_norm": 6.825788974761963,
      "learning_rate": 2.8015000000000002e-05,
      "loss": 1.3855,
      "step": 8800
    },
    {
      "epoch": 4.405,
      "grad_norm": 8.53048038482666,
      "learning_rate": 2.7989999999999998e-05,
      "loss": 1.4012,
      "step": 8810
    },
    {
      "epoch": 4.41,
      "grad_norm": 7.170938968658447,
      "learning_rate": 2.7965e-05,
      "loss": 1.3496,
      "step": 8820
    },
    {
      "epoch": 4.415,
      "grad_norm": 8.71425724029541,
      "learning_rate": 2.794e-05,
      "loss": 1.2827,
      "step": 8830
    },
    {
      "epoch": 4.42,
      "grad_norm": 10.572220802307129,
      "learning_rate": 2.7915000000000003e-05,
      "loss": 1.0715,
      "step": 8840
    },
    {
      "epoch": 4.425,
      "grad_norm": 19.149293899536133,
      "learning_rate": 2.789e-05,
      "loss": 1.461,
      "step": 8850
    },
    {
      "epoch": 4.43,
      "grad_norm": 8.694302558898926,
      "learning_rate": 2.7865000000000002e-05,
      "loss": 1.312,
      "step": 8860
    },
    {
      "epoch": 4.435,
      "grad_norm": 8.82052993774414,
      "learning_rate": 2.7839999999999998e-05,
      "loss": 1.1994,
      "step": 8870
    },
    {
      "epoch": 4.44,
      "grad_norm": 15.654017448425293,
      "learning_rate": 2.7815e-05,
      "loss": 0.9995,
      "step": 8880
    },
    {
      "epoch": 4.445,
      "grad_norm": 8.019750595092773,
      "learning_rate": 2.779e-05,
      "loss": 1.6276,
      "step": 8890
    },
    {
      "epoch": 4.45,
      "grad_norm": 9.758489608764648,
      "learning_rate": 2.7765000000000003e-05,
      "loss": 1.5543,
      "step": 8900
    },
    {
      "epoch": 4.455,
      "grad_norm": 11.91011905670166,
      "learning_rate": 2.774e-05,
      "loss": 1.3184,
      "step": 8910
    },
    {
      "epoch": 4.46,
      "grad_norm": 8.433382987976074,
      "learning_rate": 2.7715e-05,
      "loss": 1.5348,
      "step": 8920
    },
    {
      "epoch": 4.465,
      "grad_norm": 11.351251602172852,
      "learning_rate": 2.769e-05,
      "loss": 1.4646,
      "step": 8930
    },
    {
      "epoch": 4.47,
      "grad_norm": 11.014639854431152,
      "learning_rate": 2.7665000000000004e-05,
      "loss": 1.2018,
      "step": 8940
    },
    {
      "epoch": 4.475,
      "grad_norm": 9.99691390991211,
      "learning_rate": 2.764e-05,
      "loss": 1.3271,
      "step": 8950
    },
    {
      "epoch": 4.48,
      "grad_norm": 12.251704216003418,
      "learning_rate": 2.7615000000000002e-05,
      "loss": 1.513,
      "step": 8960
    },
    {
      "epoch": 4.485,
      "grad_norm": 10.638389587402344,
      "learning_rate": 2.759e-05,
      "loss": 1.4515,
      "step": 8970
    },
    {
      "epoch": 4.49,
      "grad_norm": 11.991419792175293,
      "learning_rate": 2.7565e-05,
      "loss": 1.5583,
      "step": 8980
    },
    {
      "epoch": 4.495,
      "grad_norm": 8.074518203735352,
      "learning_rate": 2.754e-05,
      "loss": 1.3716,
      "step": 8990
    },
    {
      "epoch": 4.5,
      "grad_norm": 10.897294044494629,
      "learning_rate": 2.7515000000000003e-05,
      "loss": 1.6187,
      "step": 9000
    },
    {
      "epoch": 4.505,
      "grad_norm": 11.571277618408203,
      "learning_rate": 2.749e-05,
      "loss": 1.4439,
      "step": 9010
    },
    {
      "epoch": 4.51,
      "grad_norm": 8.061447143554688,
      "learning_rate": 2.7465000000000002e-05,
      "loss": 1.3234,
      "step": 9020
    },
    {
      "epoch": 4.515,
      "grad_norm": 6.168194770812988,
      "learning_rate": 2.7439999999999998e-05,
      "loss": 1.4092,
      "step": 9030
    },
    {
      "epoch": 4.52,
      "grad_norm": 7.263377666473389,
      "learning_rate": 2.7415e-05,
      "loss": 1.2282,
      "step": 9040
    },
    {
      "epoch": 4.525,
      "grad_norm": 15.648825645446777,
      "learning_rate": 2.739e-05,
      "loss": 1.4994,
      "step": 9050
    },
    {
      "epoch": 4.53,
      "grad_norm": 11.300938606262207,
      "learning_rate": 2.7365000000000003e-05,
      "loss": 1.2522,
      "step": 9060
    },
    {
      "epoch": 4.535,
      "grad_norm": 7.076498985290527,
      "learning_rate": 2.734e-05,
      "loss": 1.3616,
      "step": 9070
    },
    {
      "epoch": 4.54,
      "grad_norm": 7.399348735809326,
      "learning_rate": 2.7315000000000002e-05,
      "loss": 1.3026,
      "step": 9080
    },
    {
      "epoch": 4.545,
      "grad_norm": 12.323359489440918,
      "learning_rate": 2.7289999999999998e-05,
      "loss": 1.4293,
      "step": 9090
    },
    {
      "epoch": 4.55,
      "grad_norm": 11.033907890319824,
      "learning_rate": 2.7265e-05,
      "loss": 1.271,
      "step": 9100
    },
    {
      "epoch": 4.555,
      "grad_norm": 13.190176963806152,
      "learning_rate": 2.724e-05,
      "loss": 1.2739,
      "step": 9110
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 8.68847370147705,
      "learning_rate": 2.7215000000000003e-05,
      "loss": 1.5649,
      "step": 9120
    },
    {
      "epoch": 4.5649999999999995,
      "grad_norm": 8.280403137207031,
      "learning_rate": 2.719e-05,
      "loss": 1.4831,
      "step": 9130
    },
    {
      "epoch": 4.57,
      "grad_norm": 7.747569561004639,
      "learning_rate": 2.7165e-05,
      "loss": 1.4365,
      "step": 9140
    },
    {
      "epoch": 4.575,
      "grad_norm": 7.9471659660339355,
      "learning_rate": 2.7139999999999998e-05,
      "loss": 1.6562,
      "step": 9150
    },
    {
      "epoch": 4.58,
      "grad_norm": 9.47234058380127,
      "learning_rate": 2.7115e-05,
      "loss": 1.2818,
      "step": 9160
    },
    {
      "epoch": 4.585,
      "grad_norm": 12.089235305786133,
      "learning_rate": 2.709e-05,
      "loss": 1.7003,
      "step": 9170
    },
    {
      "epoch": 4.59,
      "grad_norm": 12.430665969848633,
      "learning_rate": 2.7065000000000003e-05,
      "loss": 1.4721,
      "step": 9180
    },
    {
      "epoch": 4.595,
      "grad_norm": 15.622217178344727,
      "learning_rate": 2.704e-05,
      "loss": 1.4306,
      "step": 9190
    },
    {
      "epoch": 4.6,
      "grad_norm": 7.141287803649902,
      "learning_rate": 2.7015e-05,
      "loss": 1.4118,
      "step": 9200
    },
    {
      "epoch": 4.605,
      "grad_norm": 9.057461738586426,
      "learning_rate": 2.6989999999999997e-05,
      "loss": 1.5069,
      "step": 9210
    },
    {
      "epoch": 4.61,
      "grad_norm": 9.050868034362793,
      "learning_rate": 2.6965e-05,
      "loss": 1.496,
      "step": 9220
    },
    {
      "epoch": 4.615,
      "grad_norm": 13.456364631652832,
      "learning_rate": 2.694e-05,
      "loss": 1.2116,
      "step": 9230
    },
    {
      "epoch": 4.62,
      "grad_norm": 7.494648456573486,
      "learning_rate": 2.6915000000000002e-05,
      "loss": 1.5799,
      "step": 9240
    },
    {
      "epoch": 4.625,
      "grad_norm": 12.282282829284668,
      "learning_rate": 2.689e-05,
      "loss": 1.3948,
      "step": 9250
    },
    {
      "epoch": 4.63,
      "grad_norm": 11.337965965270996,
      "learning_rate": 2.6865e-05,
      "loss": 1.5538,
      "step": 9260
    },
    {
      "epoch": 4.635,
      "grad_norm": 10.055628776550293,
      "learning_rate": 2.6840000000000004e-05,
      "loss": 1.201,
      "step": 9270
    },
    {
      "epoch": 4.64,
      "grad_norm": 8.931219100952148,
      "learning_rate": 2.6815e-05,
      "loss": 1.276,
      "step": 9280
    },
    {
      "epoch": 4.645,
      "grad_norm": 9.564403533935547,
      "learning_rate": 2.6790000000000003e-05,
      "loss": 1.2865,
      "step": 9290
    },
    {
      "epoch": 4.65,
      "grad_norm": 14.799320220947266,
      "learning_rate": 2.6765000000000002e-05,
      "loss": 1.7254,
      "step": 9300
    },
    {
      "epoch": 4.655,
      "grad_norm": 19.016206741333008,
      "learning_rate": 2.6740000000000005e-05,
      "loss": 1.2909,
      "step": 9310
    },
    {
      "epoch": 4.66,
      "grad_norm": 7.527999401092529,
      "learning_rate": 2.6715e-05,
      "loss": 1.4756,
      "step": 9320
    },
    {
      "epoch": 4.665,
      "grad_norm": 8.675662994384766,
      "learning_rate": 2.6690000000000004e-05,
      "loss": 1.3129,
      "step": 9330
    },
    {
      "epoch": 4.67,
      "grad_norm": 8.939151763916016,
      "learning_rate": 2.6665e-05,
      "loss": 1.2089,
      "step": 9340
    },
    {
      "epoch": 4.675,
      "grad_norm": 12.429323196411133,
      "learning_rate": 2.6640000000000002e-05,
      "loss": 1.4524,
      "step": 9350
    },
    {
      "epoch": 4.68,
      "grad_norm": 7.315086364746094,
      "learning_rate": 2.6615000000000002e-05,
      "loss": 1.4434,
      "step": 9360
    },
    {
      "epoch": 4.6850000000000005,
      "grad_norm": 12.186075210571289,
      "learning_rate": 2.6590000000000005e-05,
      "loss": 1.8251,
      "step": 9370
    },
    {
      "epoch": 4.6899999999999995,
      "grad_norm": 10.512120246887207,
      "learning_rate": 2.6565e-05,
      "loss": 1.2565,
      "step": 9380
    },
    {
      "epoch": 4.695,
      "grad_norm": 9.188180923461914,
      "learning_rate": 2.6540000000000003e-05,
      "loss": 1.4098,
      "step": 9390
    },
    {
      "epoch": 4.7,
      "grad_norm": 14.59998607635498,
      "learning_rate": 2.6515e-05,
      "loss": 1.2626,
      "step": 9400
    },
    {
      "epoch": 4.705,
      "grad_norm": 6.66099739074707,
      "learning_rate": 2.6490000000000002e-05,
      "loss": 1.5876,
      "step": 9410
    },
    {
      "epoch": 4.71,
      "grad_norm": 7.259214878082275,
      "learning_rate": 2.6465e-05,
      "loss": 1.5272,
      "step": 9420
    },
    {
      "epoch": 4.715,
      "grad_norm": 12.832694053649902,
      "learning_rate": 2.6440000000000004e-05,
      "loss": 1.5906,
      "step": 9430
    },
    {
      "epoch": 4.72,
      "grad_norm": 9.991958618164062,
      "learning_rate": 2.6415e-05,
      "loss": 1.4697,
      "step": 9440
    },
    {
      "epoch": 4.725,
      "grad_norm": 9.968073844909668,
      "learning_rate": 2.6390000000000003e-05,
      "loss": 1.5581,
      "step": 9450
    },
    {
      "epoch": 4.73,
      "grad_norm": 10.61303424835205,
      "learning_rate": 2.6365e-05,
      "loss": 1.6064,
      "step": 9460
    },
    {
      "epoch": 4.735,
      "grad_norm": 7.194882392883301,
      "learning_rate": 2.6340000000000002e-05,
      "loss": 1.6221,
      "step": 9470
    },
    {
      "epoch": 4.74,
      "grad_norm": 10.087010383605957,
      "learning_rate": 2.6315e-05,
      "loss": 1.4383,
      "step": 9480
    },
    {
      "epoch": 4.745,
      "grad_norm": 3.4458963871002197,
      "learning_rate": 2.6290000000000004e-05,
      "loss": 1.2392,
      "step": 9490
    },
    {
      "epoch": 4.75,
      "grad_norm": 8.248035430908203,
      "learning_rate": 2.6265e-05,
      "loss": 1.1508,
      "step": 9500
    },
    {
      "epoch": 4.755,
      "grad_norm": 8.664193153381348,
      "learning_rate": 2.6240000000000003e-05,
      "loss": 1.1874,
      "step": 9510
    },
    {
      "epoch": 4.76,
      "grad_norm": 15.395130157470703,
      "learning_rate": 2.6215000000000002e-05,
      "loss": 1.6389,
      "step": 9520
    },
    {
      "epoch": 4.765,
      "grad_norm": 9.718358039855957,
      "learning_rate": 2.6190000000000005e-05,
      "loss": 1.531,
      "step": 9530
    },
    {
      "epoch": 4.77,
      "grad_norm": 8.785362243652344,
      "learning_rate": 2.6165e-05,
      "loss": 1.4036,
      "step": 9540
    },
    {
      "epoch": 4.775,
      "grad_norm": 9.897862434387207,
      "learning_rate": 2.6140000000000004e-05,
      "loss": 1.2483,
      "step": 9550
    },
    {
      "epoch": 4.78,
      "grad_norm": 7.180230617523193,
      "learning_rate": 2.6115e-05,
      "loss": 1.6177,
      "step": 9560
    },
    {
      "epoch": 4.785,
      "grad_norm": 7.38271427154541,
      "learning_rate": 2.6090000000000003e-05,
      "loss": 1.1023,
      "step": 9570
    },
    {
      "epoch": 4.79,
      "grad_norm": 10.992528915405273,
      "learning_rate": 2.6065000000000002e-05,
      "loss": 1.6749,
      "step": 9580
    },
    {
      "epoch": 4.795,
      "grad_norm": 10.508115768432617,
      "learning_rate": 2.6040000000000005e-05,
      "loss": 1.4798,
      "step": 9590
    },
    {
      "epoch": 4.8,
      "grad_norm": 16.6833553314209,
      "learning_rate": 2.6015e-05,
      "loss": 1.482,
      "step": 9600
    },
    {
      "epoch": 4.805,
      "grad_norm": 10.62302017211914,
      "learning_rate": 2.5990000000000004e-05,
      "loss": 1.4593,
      "step": 9610
    },
    {
      "epoch": 4.8100000000000005,
      "grad_norm": 23.2600154876709,
      "learning_rate": 2.5965e-05,
      "loss": 1.4034,
      "step": 9620
    },
    {
      "epoch": 4.8149999999999995,
      "grad_norm": 13.376106262207031,
      "learning_rate": 2.5940000000000002e-05,
      "loss": 1.4483,
      "step": 9630
    },
    {
      "epoch": 4.82,
      "grad_norm": 7.777781009674072,
      "learning_rate": 2.5915000000000002e-05,
      "loss": 1.3328,
      "step": 9640
    },
    {
      "epoch": 4.825,
      "grad_norm": 12.6543607711792,
      "learning_rate": 2.5890000000000005e-05,
      "loss": 1.5983,
      "step": 9650
    },
    {
      "epoch": 4.83,
      "grad_norm": 9.204141616821289,
      "learning_rate": 2.5865e-05,
      "loss": 1.67,
      "step": 9660
    },
    {
      "epoch": 4.835,
      "grad_norm": 10.470245361328125,
      "learning_rate": 2.5840000000000003e-05,
      "loss": 1.4093,
      "step": 9670
    },
    {
      "epoch": 4.84,
      "grad_norm": 7.9232611656188965,
      "learning_rate": 2.5815e-05,
      "loss": 1.2866,
      "step": 9680
    },
    {
      "epoch": 4.845,
      "grad_norm": 7.660031795501709,
      "learning_rate": 2.5790000000000002e-05,
      "loss": 1.4943,
      "step": 9690
    },
    {
      "epoch": 4.85,
      "grad_norm": 17.802928924560547,
      "learning_rate": 2.5765e-05,
      "loss": 1.7504,
      "step": 9700
    },
    {
      "epoch": 4.855,
      "grad_norm": 7.116843223571777,
      "learning_rate": 2.5740000000000004e-05,
      "loss": 1.6206,
      "step": 9710
    },
    {
      "epoch": 4.86,
      "grad_norm": 10.410459518432617,
      "learning_rate": 2.5715e-05,
      "loss": 1.4579,
      "step": 9720
    },
    {
      "epoch": 4.865,
      "grad_norm": 8.516836166381836,
      "learning_rate": 2.5690000000000003e-05,
      "loss": 1.8263,
      "step": 9730
    },
    {
      "epoch": 4.87,
      "grad_norm": 3.276449680328369,
      "learning_rate": 2.5665e-05,
      "loss": 1.3509,
      "step": 9740
    },
    {
      "epoch": 4.875,
      "grad_norm": 14.576449394226074,
      "learning_rate": 2.5640000000000002e-05,
      "loss": 1.4102,
      "step": 9750
    },
    {
      "epoch": 4.88,
      "grad_norm": 11.728813171386719,
      "learning_rate": 2.5615e-05,
      "loss": 1.2045,
      "step": 9760
    },
    {
      "epoch": 4.885,
      "grad_norm": 14.035356521606445,
      "learning_rate": 2.5590000000000004e-05,
      "loss": 1.5279,
      "step": 9770
    },
    {
      "epoch": 4.89,
      "grad_norm": 18.95697021484375,
      "learning_rate": 2.5565e-05,
      "loss": 1.5194,
      "step": 9780
    },
    {
      "epoch": 4.895,
      "grad_norm": 9.397473335266113,
      "learning_rate": 2.5540000000000003e-05,
      "loss": 1.4917,
      "step": 9790
    },
    {
      "epoch": 4.9,
      "grad_norm": 12.720281600952148,
      "learning_rate": 2.5515e-05,
      "loss": 1.4697,
      "step": 9800
    },
    {
      "epoch": 4.905,
      "grad_norm": 10.957321166992188,
      "learning_rate": 2.549e-05,
      "loss": 1.7122,
      "step": 9810
    },
    {
      "epoch": 4.91,
      "grad_norm": 10.154810905456543,
      "learning_rate": 2.5465e-05,
      "loss": 1.6045,
      "step": 9820
    },
    {
      "epoch": 4.915,
      "grad_norm": 9.315003395080566,
      "learning_rate": 2.5440000000000004e-05,
      "loss": 1.455,
      "step": 9830
    },
    {
      "epoch": 4.92,
      "grad_norm": 7.6226325035095215,
      "learning_rate": 2.5415e-05,
      "loss": 1.3917,
      "step": 9840
    },
    {
      "epoch": 4.925,
      "grad_norm": 13.209264755249023,
      "learning_rate": 2.5390000000000003e-05,
      "loss": 1.1816,
      "step": 9850
    },
    {
      "epoch": 4.93,
      "grad_norm": 2.9499435424804688,
      "learning_rate": 2.5365e-05,
      "loss": 1.5319,
      "step": 9860
    },
    {
      "epoch": 4.9350000000000005,
      "grad_norm": 9.72801685333252,
      "learning_rate": 2.534e-05,
      "loss": 0.9504,
      "step": 9870
    },
    {
      "epoch": 4.9399999999999995,
      "grad_norm": 9.510485649108887,
      "learning_rate": 2.5315e-05,
      "loss": 1.5112,
      "step": 9880
    },
    {
      "epoch": 4.945,
      "grad_norm": 12.681191444396973,
      "learning_rate": 2.5290000000000004e-05,
      "loss": 1.342,
      "step": 9890
    },
    {
      "epoch": 4.95,
      "grad_norm": 8.407468795776367,
      "learning_rate": 2.5265e-05,
      "loss": 1.3146,
      "step": 9900
    },
    {
      "epoch": 4.955,
      "grad_norm": 19.18146324157715,
      "learning_rate": 2.5240000000000002e-05,
      "loss": 1.2896,
      "step": 9910
    },
    {
      "epoch": 4.96,
      "grad_norm": 7.7766032218933105,
      "learning_rate": 2.5214999999999998e-05,
      "loss": 1.2614,
      "step": 9920
    },
    {
      "epoch": 4.965,
      "grad_norm": 24.07929801940918,
      "learning_rate": 2.519e-05,
      "loss": 1.6297,
      "step": 9930
    },
    {
      "epoch": 4.97,
      "grad_norm": 7.88754415512085,
      "learning_rate": 2.5165e-05,
      "loss": 1.4802,
      "step": 9940
    },
    {
      "epoch": 4.975,
      "grad_norm": 8.29616928100586,
      "learning_rate": 2.5140000000000003e-05,
      "loss": 1.5029,
      "step": 9950
    },
    {
      "epoch": 4.98,
      "grad_norm": 15.780180931091309,
      "learning_rate": 2.5115e-05,
      "loss": 1.8907,
      "step": 9960
    },
    {
      "epoch": 4.985,
      "grad_norm": 6.9170308113098145,
      "learning_rate": 2.5090000000000002e-05,
      "loss": 1.3523,
      "step": 9970
    },
    {
      "epoch": 4.99,
      "grad_norm": 14.130645751953125,
      "learning_rate": 2.5064999999999998e-05,
      "loss": 1.4123,
      "step": 9980
    },
    {
      "epoch": 4.995,
      "grad_norm": 13.258491516113281,
      "learning_rate": 2.504e-05,
      "loss": 1.6246,
      "step": 9990
    },
    {
      "epoch": 5.0,
      "grad_norm": 8.970356941223145,
      "learning_rate": 2.5015e-05,
      "loss": 1.5377,
      "step": 10000
    },
    {
      "epoch": 5.005,
      "grad_norm": 7.767464637756348,
      "learning_rate": 2.4990000000000003e-05,
      "loss": 1.5425,
      "step": 10010
    },
    {
      "epoch": 5.01,
      "grad_norm": 9.658276557922363,
      "learning_rate": 2.4965000000000002e-05,
      "loss": 1.3343,
      "step": 10020
    },
    {
      "epoch": 5.015,
      "grad_norm": 16.535659790039062,
      "learning_rate": 2.4940000000000002e-05,
      "loss": 1.3149,
      "step": 10030
    },
    {
      "epoch": 5.02,
      "grad_norm": 28.1239070892334,
      "learning_rate": 2.4915e-05,
      "loss": 1.3498,
      "step": 10040
    },
    {
      "epoch": 5.025,
      "grad_norm": 9.509459495544434,
      "learning_rate": 2.489e-05,
      "loss": 1.4821,
      "step": 10050
    },
    {
      "epoch": 5.03,
      "grad_norm": 7.301929473876953,
      "learning_rate": 2.4865000000000003e-05,
      "loss": 1.1705,
      "step": 10060
    },
    {
      "epoch": 5.035,
      "grad_norm": 7.157393455505371,
      "learning_rate": 2.4840000000000003e-05,
      "loss": 1.2095,
      "step": 10070
    },
    {
      "epoch": 5.04,
      "grad_norm": 9.224393844604492,
      "learning_rate": 2.4815000000000002e-05,
      "loss": 1.5338,
      "step": 10080
    },
    {
      "epoch": 5.045,
      "grad_norm": 8.955063819885254,
      "learning_rate": 2.479e-05,
      "loss": 1.3085,
      "step": 10090
    },
    {
      "epoch": 5.05,
      "grad_norm": 9.970220565795898,
      "learning_rate": 2.4765e-05,
      "loss": 1.5192,
      "step": 10100
    },
    {
      "epoch": 5.055,
      "grad_norm": 10.08625316619873,
      "learning_rate": 2.4740000000000004e-05,
      "loss": 1.197,
      "step": 10110
    },
    {
      "epoch": 5.06,
      "grad_norm": 8.62763500213623,
      "learning_rate": 2.4715000000000003e-05,
      "loss": 1.4235,
      "step": 10120
    },
    {
      "epoch": 5.065,
      "grad_norm": 6.807127475738525,
      "learning_rate": 2.4690000000000002e-05,
      "loss": 1.4426,
      "step": 10130
    },
    {
      "epoch": 5.07,
      "grad_norm": 7.26696252822876,
      "learning_rate": 2.4665000000000002e-05,
      "loss": 1.2945,
      "step": 10140
    },
    {
      "epoch": 5.075,
      "grad_norm": 6.89351749420166,
      "learning_rate": 2.464e-05,
      "loss": 1.5873,
      "step": 10150
    },
    {
      "epoch": 5.08,
      "grad_norm": 10.18942928314209,
      "learning_rate": 2.4615e-05,
      "loss": 1.2867,
      "step": 10160
    },
    {
      "epoch": 5.085,
      "grad_norm": 10.658445358276367,
      "learning_rate": 2.4590000000000003e-05,
      "loss": 1.3189,
      "step": 10170
    },
    {
      "epoch": 5.09,
      "grad_norm": 8.209612846374512,
      "learning_rate": 2.4565000000000003e-05,
      "loss": 1.1994,
      "step": 10180
    },
    {
      "epoch": 5.095,
      "grad_norm": 17.103525161743164,
      "learning_rate": 2.4540000000000002e-05,
      "loss": 1.693,
      "step": 10190
    },
    {
      "epoch": 5.1,
      "grad_norm": 10.13664436340332,
      "learning_rate": 2.4515e-05,
      "loss": 1.699,
      "step": 10200
    },
    {
      "epoch": 5.105,
      "grad_norm": 8.990334510803223,
      "learning_rate": 2.449e-05,
      "loss": 1.3945,
      "step": 10210
    },
    {
      "epoch": 5.11,
      "grad_norm": 11.529997825622559,
      "learning_rate": 2.4465e-05,
      "loss": 1.4663,
      "step": 10220
    },
    {
      "epoch": 5.115,
      "grad_norm": 8.343854904174805,
      "learning_rate": 2.4440000000000003e-05,
      "loss": 1.4074,
      "step": 10230
    },
    {
      "epoch": 5.12,
      "grad_norm": 13.235705375671387,
      "learning_rate": 2.4415000000000003e-05,
      "loss": 1.611,
      "step": 10240
    },
    {
      "epoch": 5.125,
      "grad_norm": 9.922150611877441,
      "learning_rate": 2.4390000000000002e-05,
      "loss": 1.3744,
      "step": 10250
    },
    {
      "epoch": 5.13,
      "grad_norm": 7.060261249542236,
      "learning_rate": 2.4365e-05,
      "loss": 1.1826,
      "step": 10260
    },
    {
      "epoch": 5.135,
      "grad_norm": 9.650221824645996,
      "learning_rate": 2.434e-05,
      "loss": 1.0723,
      "step": 10270
    },
    {
      "epoch": 5.14,
      "grad_norm": 9.965155601501465,
      "learning_rate": 2.4315e-05,
      "loss": 1.6678,
      "step": 10280
    },
    {
      "epoch": 5.145,
      "grad_norm": 12.847028732299805,
      "learning_rate": 2.4290000000000003e-05,
      "loss": 1.3955,
      "step": 10290
    },
    {
      "epoch": 5.15,
      "grad_norm": 9.722098350524902,
      "learning_rate": 2.4265000000000002e-05,
      "loss": 1.5119,
      "step": 10300
    },
    {
      "epoch": 5.155,
      "grad_norm": 8.996427536010742,
      "learning_rate": 2.4240000000000002e-05,
      "loss": 1.485,
      "step": 10310
    },
    {
      "epoch": 5.16,
      "grad_norm": 9.326464653015137,
      "learning_rate": 2.4215e-05,
      "loss": 1.3223,
      "step": 10320
    },
    {
      "epoch": 5.165,
      "grad_norm": 7.230937957763672,
      "learning_rate": 2.419e-05,
      "loss": 1.3196,
      "step": 10330
    },
    {
      "epoch": 5.17,
      "grad_norm": 7.980108261108398,
      "learning_rate": 2.4165e-05,
      "loss": 1.386,
      "step": 10340
    },
    {
      "epoch": 5.175,
      "grad_norm": 6.8353424072265625,
      "learning_rate": 2.4140000000000003e-05,
      "loss": 1.5641,
      "step": 10350
    },
    {
      "epoch": 5.18,
      "grad_norm": 9.288260459899902,
      "learning_rate": 2.4115000000000002e-05,
      "loss": 1.1282,
      "step": 10360
    },
    {
      "epoch": 5.185,
      "grad_norm": 9.35451602935791,
      "learning_rate": 2.409e-05,
      "loss": 1.0563,
      "step": 10370
    },
    {
      "epoch": 5.19,
      "grad_norm": 11.743392944335938,
      "learning_rate": 2.4065e-05,
      "loss": 1.6448,
      "step": 10380
    },
    {
      "epoch": 5.195,
      "grad_norm": 8.48523235321045,
      "learning_rate": 2.404e-05,
      "loss": 1.452,
      "step": 10390
    },
    {
      "epoch": 5.2,
      "grad_norm": 8.834818840026855,
      "learning_rate": 2.4015000000000003e-05,
      "loss": 1.4257,
      "step": 10400
    },
    {
      "epoch": 5.205,
      "grad_norm": 8.62919807434082,
      "learning_rate": 2.3990000000000002e-05,
      "loss": 1.3159,
      "step": 10410
    },
    {
      "epoch": 5.21,
      "grad_norm": 12.45914077758789,
      "learning_rate": 2.3965000000000002e-05,
      "loss": 1.468,
      "step": 10420
    },
    {
      "epoch": 5.215,
      "grad_norm": 8.877076148986816,
      "learning_rate": 2.394e-05,
      "loss": 1.6575,
      "step": 10430
    },
    {
      "epoch": 5.22,
      "grad_norm": 8.400531768798828,
      "learning_rate": 2.3915e-05,
      "loss": 1.4487,
      "step": 10440
    },
    {
      "epoch": 5.225,
      "grad_norm": 11.916213035583496,
      "learning_rate": 2.389e-05,
      "loss": 1.5594,
      "step": 10450
    },
    {
      "epoch": 5.23,
      "grad_norm": 13.764117240905762,
      "learning_rate": 2.3865000000000003e-05,
      "loss": 1.5293,
      "step": 10460
    },
    {
      "epoch": 5.235,
      "grad_norm": 16.02562141418457,
      "learning_rate": 2.3840000000000002e-05,
      "loss": 1.7186,
      "step": 10470
    },
    {
      "epoch": 5.24,
      "grad_norm": 12.429101943969727,
      "learning_rate": 2.3815e-05,
      "loss": 1.4605,
      "step": 10480
    },
    {
      "epoch": 5.245,
      "grad_norm": 10.787747383117676,
      "learning_rate": 2.379e-05,
      "loss": 1.2639,
      "step": 10490
    },
    {
      "epoch": 5.25,
      "grad_norm": 15.423383712768555,
      "learning_rate": 2.3765e-05,
      "loss": 1.1218,
      "step": 10500
    },
    {
      "epoch": 5.255,
      "grad_norm": 9.856348037719727,
      "learning_rate": 2.374e-05,
      "loss": 1.7648,
      "step": 10510
    },
    {
      "epoch": 5.26,
      "grad_norm": 22.618785858154297,
      "learning_rate": 2.3715000000000002e-05,
      "loss": 1.6752,
      "step": 10520
    },
    {
      "epoch": 5.265,
      "grad_norm": 8.943086624145508,
      "learning_rate": 2.3690000000000002e-05,
      "loss": 1.1778,
      "step": 10530
    },
    {
      "epoch": 5.27,
      "grad_norm": 9.478671073913574,
      "learning_rate": 2.3665e-05,
      "loss": 1.3083,
      "step": 10540
    },
    {
      "epoch": 5.275,
      "grad_norm": 18.82368278503418,
      "learning_rate": 2.364e-05,
      "loss": 1.7716,
      "step": 10550
    },
    {
      "epoch": 5.28,
      "grad_norm": 11.89556884765625,
      "learning_rate": 2.3615e-05,
      "loss": 1.3666,
      "step": 10560
    },
    {
      "epoch": 5.285,
      "grad_norm": 10.049481391906738,
      "learning_rate": 2.359e-05,
      "loss": 1.6377,
      "step": 10570
    },
    {
      "epoch": 5.29,
      "grad_norm": 12.65097427368164,
      "learning_rate": 2.3565000000000002e-05,
      "loss": 1.2524,
      "step": 10580
    },
    {
      "epoch": 5.295,
      "grad_norm": 14.481379508972168,
      "learning_rate": 2.354e-05,
      "loss": 1.4856,
      "step": 10590
    },
    {
      "epoch": 5.3,
      "grad_norm": 12.700407981872559,
      "learning_rate": 2.3515e-05,
      "loss": 1.5711,
      "step": 10600
    },
    {
      "epoch": 5.305,
      "grad_norm": 11.074708938598633,
      "learning_rate": 2.349e-05,
      "loss": 1.7862,
      "step": 10610
    },
    {
      "epoch": 5.31,
      "grad_norm": 12.432332038879395,
      "learning_rate": 2.3465e-05,
      "loss": 1.3121,
      "step": 10620
    },
    {
      "epoch": 5.315,
      "grad_norm": 7.803463459014893,
      "learning_rate": 2.344e-05,
      "loss": 1.2435,
      "step": 10630
    },
    {
      "epoch": 5.32,
      "grad_norm": 8.965923309326172,
      "learning_rate": 2.3415000000000002e-05,
      "loss": 1.096,
      "step": 10640
    },
    {
      "epoch": 5.325,
      "grad_norm": 12.949094772338867,
      "learning_rate": 2.339e-05,
      "loss": 1.6609,
      "step": 10650
    },
    {
      "epoch": 5.33,
      "grad_norm": 13.507801055908203,
      "learning_rate": 2.3365e-05,
      "loss": 1.4403,
      "step": 10660
    },
    {
      "epoch": 5.335,
      "grad_norm": 10.898324012756348,
      "learning_rate": 2.334e-05,
      "loss": 1.4649,
      "step": 10670
    },
    {
      "epoch": 5.34,
      "grad_norm": 8.554445266723633,
      "learning_rate": 2.3315e-05,
      "loss": 1.3452,
      "step": 10680
    },
    {
      "epoch": 5.345,
      "grad_norm": 5.073429107666016,
      "learning_rate": 2.3290000000000002e-05,
      "loss": 1.1237,
      "step": 10690
    },
    {
      "epoch": 5.35,
      "grad_norm": 9.35539722442627,
      "learning_rate": 2.3265000000000002e-05,
      "loss": 1.4701,
      "step": 10700
    },
    {
      "epoch": 5.355,
      "grad_norm": 9.01118278503418,
      "learning_rate": 2.324e-05,
      "loss": 1.4292,
      "step": 10710
    },
    {
      "epoch": 5.36,
      "grad_norm": 8.825992584228516,
      "learning_rate": 2.3215e-05,
      "loss": 1.3511,
      "step": 10720
    },
    {
      "epoch": 5.365,
      "grad_norm": 8.456379890441895,
      "learning_rate": 2.319e-05,
      "loss": 1.2378,
      "step": 10730
    },
    {
      "epoch": 5.37,
      "grad_norm": 7.314531326293945,
      "learning_rate": 2.3165e-05,
      "loss": 1.4279,
      "step": 10740
    },
    {
      "epoch": 5.375,
      "grad_norm": 11.630128860473633,
      "learning_rate": 2.3140000000000002e-05,
      "loss": 1.5361,
      "step": 10750
    },
    {
      "epoch": 5.38,
      "grad_norm": 14.255874633789062,
      "learning_rate": 2.3115e-05,
      "loss": 1.3415,
      "step": 10760
    },
    {
      "epoch": 5.385,
      "grad_norm": 9.214555740356445,
      "learning_rate": 2.309e-05,
      "loss": 1.2984,
      "step": 10770
    },
    {
      "epoch": 5.39,
      "grad_norm": 9.224347114562988,
      "learning_rate": 2.3065e-05,
      "loss": 1.5782,
      "step": 10780
    },
    {
      "epoch": 5.395,
      "grad_norm": 9.293661117553711,
      "learning_rate": 2.304e-05,
      "loss": 1.5428,
      "step": 10790
    },
    {
      "epoch": 5.4,
      "grad_norm": 7.249505996704102,
      "learning_rate": 2.3015e-05,
      "loss": 1.2452,
      "step": 10800
    },
    {
      "epoch": 5.405,
      "grad_norm": 11.40385627746582,
      "learning_rate": 2.2990000000000002e-05,
      "loss": 1.3314,
      "step": 10810
    },
    {
      "epoch": 5.41,
      "grad_norm": 10.472586631774902,
      "learning_rate": 2.2965e-05,
      "loss": 1.2927,
      "step": 10820
    },
    {
      "epoch": 5.415,
      "grad_norm": 13.086216926574707,
      "learning_rate": 2.294e-05,
      "loss": 1.2036,
      "step": 10830
    },
    {
      "epoch": 5.42,
      "grad_norm": 16.21024513244629,
      "learning_rate": 2.2915e-05,
      "loss": 1.3385,
      "step": 10840
    },
    {
      "epoch": 5.425,
      "grad_norm": 10.564655303955078,
      "learning_rate": 2.289e-05,
      "loss": 1.1102,
      "step": 10850
    },
    {
      "epoch": 5.43,
      "grad_norm": 14.53451156616211,
      "learning_rate": 2.2865e-05,
      "loss": 1.3762,
      "step": 10860
    },
    {
      "epoch": 5.435,
      "grad_norm": 7.97784948348999,
      "learning_rate": 2.284e-05,
      "loss": 1.5368,
      "step": 10870
    },
    {
      "epoch": 5.44,
      "grad_norm": 8.298707962036133,
      "learning_rate": 2.2815e-05,
      "loss": 1.3127,
      "step": 10880
    },
    {
      "epoch": 5.445,
      "grad_norm": 12.17214584350586,
      "learning_rate": 2.279e-05,
      "loss": 1.5023,
      "step": 10890
    },
    {
      "epoch": 5.45,
      "grad_norm": 11.365139961242676,
      "learning_rate": 2.2765e-05,
      "loss": 1.2596,
      "step": 10900
    },
    {
      "epoch": 5.455,
      "grad_norm": 16.096820831298828,
      "learning_rate": 2.274e-05,
      "loss": 1.363,
      "step": 10910
    },
    {
      "epoch": 5.46,
      "grad_norm": 19.547588348388672,
      "learning_rate": 2.2715e-05,
      "loss": 1.2386,
      "step": 10920
    },
    {
      "epoch": 5.465,
      "grad_norm": 7.972989082336426,
      "learning_rate": 2.269e-05,
      "loss": 1.6106,
      "step": 10930
    },
    {
      "epoch": 5.47,
      "grad_norm": 7.115381717681885,
      "learning_rate": 2.2665e-05,
      "loss": 1.2813,
      "step": 10940
    },
    {
      "epoch": 5.475,
      "grad_norm": 10.013738632202148,
      "learning_rate": 2.264e-05,
      "loss": 1.5909,
      "step": 10950
    },
    {
      "epoch": 5.48,
      "grad_norm": 7.780885696411133,
      "learning_rate": 2.2615e-05,
      "loss": 1.2938,
      "step": 10960
    },
    {
      "epoch": 5.485,
      "grad_norm": 14.680370330810547,
      "learning_rate": 2.259e-05,
      "loss": 1.1702,
      "step": 10970
    },
    {
      "epoch": 5.49,
      "grad_norm": 7.676332950592041,
      "learning_rate": 2.2565e-05,
      "loss": 1.4303,
      "step": 10980
    },
    {
      "epoch": 5.495,
      "grad_norm": 9.363633155822754,
      "learning_rate": 2.254e-05,
      "loss": 1.4867,
      "step": 10990
    },
    {
      "epoch": 5.5,
      "grad_norm": 18.94462776184082,
      "learning_rate": 2.2515e-05,
      "loss": 1.4544,
      "step": 11000
    },
    {
      "epoch": 5.505,
      "grad_norm": 8.140963554382324,
      "learning_rate": 2.249e-05,
      "loss": 1.4629,
      "step": 11010
    },
    {
      "epoch": 5.51,
      "grad_norm": 7.861104488372803,
      "learning_rate": 2.2465e-05,
      "loss": 1.4842,
      "step": 11020
    },
    {
      "epoch": 5.515,
      "grad_norm": 14.897332191467285,
      "learning_rate": 2.244e-05,
      "loss": 1.6656,
      "step": 11030
    },
    {
      "epoch": 5.52,
      "grad_norm": 8.068939208984375,
      "learning_rate": 2.2415e-05,
      "loss": 1.287,
      "step": 11040
    },
    {
      "epoch": 5.525,
      "grad_norm": 13.55678939819336,
      "learning_rate": 2.239e-05,
      "loss": 1.1994,
      "step": 11050
    },
    {
      "epoch": 5.53,
      "grad_norm": 7.8165202140808105,
      "learning_rate": 2.2365e-05,
      "loss": 1.3634,
      "step": 11060
    },
    {
      "epoch": 5.535,
      "grad_norm": 13.420527458190918,
      "learning_rate": 2.234e-05,
      "loss": 1.4062,
      "step": 11070
    },
    {
      "epoch": 5.54,
      "grad_norm": 16.77329444885254,
      "learning_rate": 2.2315e-05,
      "loss": 1.3619,
      "step": 11080
    },
    {
      "epoch": 5.545,
      "grad_norm": 8.896756172180176,
      "learning_rate": 2.229e-05,
      "loss": 1.2789,
      "step": 11090
    },
    {
      "epoch": 5.55,
      "grad_norm": 12.351465225219727,
      "learning_rate": 2.2265e-05,
      "loss": 1.5017,
      "step": 11100
    },
    {
      "epoch": 5.555,
      "grad_norm": 7.124480724334717,
      "learning_rate": 2.224e-05,
      "loss": 0.917,
      "step": 11110
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 8.159446716308594,
      "learning_rate": 2.2215e-05,
      "loss": 1.4181,
      "step": 11120
    },
    {
      "epoch": 5.5649999999999995,
      "grad_norm": 16.912338256835938,
      "learning_rate": 2.219e-05,
      "loss": 1.4152,
      "step": 11130
    },
    {
      "epoch": 5.57,
      "grad_norm": 10.63819408416748,
      "learning_rate": 2.2165000000000002e-05,
      "loss": 1.6149,
      "step": 11140
    },
    {
      "epoch": 5.575,
      "grad_norm": 5.74513578414917,
      "learning_rate": 2.214e-05,
      "loss": 1.1762,
      "step": 11150
    },
    {
      "epoch": 5.58,
      "grad_norm": 11.607339859008789,
      "learning_rate": 2.2115e-05,
      "loss": 1.3587,
      "step": 11160
    },
    {
      "epoch": 5.585,
      "grad_norm": 19.23941993713379,
      "learning_rate": 2.2090000000000004e-05,
      "loss": 1.5357,
      "step": 11170
    },
    {
      "epoch": 5.59,
      "grad_norm": 7.372783184051514,
      "learning_rate": 2.2065000000000003e-05,
      "loss": 1.4382,
      "step": 11180
    },
    {
      "epoch": 5.595,
      "grad_norm": 7.364551544189453,
      "learning_rate": 2.2040000000000002e-05,
      "loss": 1.3256,
      "step": 11190
    },
    {
      "epoch": 5.6,
      "grad_norm": 14.807711601257324,
      "learning_rate": 2.2015000000000002e-05,
      "loss": 1.4623,
      "step": 11200
    },
    {
      "epoch": 5.605,
      "grad_norm": 12.886714935302734,
      "learning_rate": 2.199e-05,
      "loss": 1.6766,
      "step": 11210
    },
    {
      "epoch": 5.61,
      "grad_norm": 14.60847282409668,
      "learning_rate": 2.1965e-05,
      "loss": 1.2071,
      "step": 11220
    },
    {
      "epoch": 5.615,
      "grad_norm": 7.246433734893799,
      "learning_rate": 2.1940000000000003e-05,
      "loss": 1.6296,
      "step": 11230
    },
    {
      "epoch": 5.62,
      "grad_norm": 9.06977653503418,
      "learning_rate": 2.1915000000000003e-05,
      "loss": 1.2832,
      "step": 11240
    },
    {
      "epoch": 5.625,
      "grad_norm": 8.098388671875,
      "learning_rate": 2.1890000000000002e-05,
      "loss": 1.4272,
      "step": 11250
    },
    {
      "epoch": 5.63,
      "grad_norm": 8.896381378173828,
      "learning_rate": 2.1865e-05,
      "loss": 1.3405,
      "step": 11260
    },
    {
      "epoch": 5.635,
      "grad_norm": 8.116700172424316,
      "learning_rate": 2.184e-05,
      "loss": 1.3443,
      "step": 11270
    },
    {
      "epoch": 5.64,
      "grad_norm": 28.479839324951172,
      "learning_rate": 2.1815000000000004e-05,
      "loss": 1.2043,
      "step": 11280
    },
    {
      "epoch": 5.645,
      "grad_norm": 9.938375473022461,
      "learning_rate": 2.1790000000000003e-05,
      "loss": 1.5249,
      "step": 11290
    },
    {
      "epoch": 5.65,
      "grad_norm": 4.413005352020264,
      "learning_rate": 2.1765000000000003e-05,
      "loss": 1.4802,
      "step": 11300
    },
    {
      "epoch": 5.655,
      "grad_norm": 10.202543258666992,
      "learning_rate": 2.1740000000000002e-05,
      "loss": 1.7043,
      "step": 11310
    },
    {
      "epoch": 5.66,
      "grad_norm": 9.094538688659668,
      "learning_rate": 2.1715e-05,
      "loss": 1.2383,
      "step": 11320
    },
    {
      "epoch": 5.665,
      "grad_norm": 14.077079772949219,
      "learning_rate": 2.169e-05,
      "loss": 1.3389,
      "step": 11330
    },
    {
      "epoch": 5.67,
      "grad_norm": 9.0540771484375,
      "learning_rate": 2.1665000000000003e-05,
      "loss": 1.0914,
      "step": 11340
    },
    {
      "epoch": 5.675,
      "grad_norm": 9.972881317138672,
      "learning_rate": 2.1640000000000003e-05,
      "loss": 1.1531,
      "step": 11350
    },
    {
      "epoch": 5.68,
      "grad_norm": 12.222355842590332,
      "learning_rate": 2.1615000000000002e-05,
      "loss": 1.336,
      "step": 11360
    },
    {
      "epoch": 5.6850000000000005,
      "grad_norm": 11.521920204162598,
      "learning_rate": 2.159e-05,
      "loss": 1.6668,
      "step": 11370
    },
    {
      "epoch": 5.6899999999999995,
      "grad_norm": 8.652400016784668,
      "learning_rate": 2.1565e-05,
      "loss": 1.5945,
      "step": 11380
    },
    {
      "epoch": 5.695,
      "grad_norm": 7.886066913604736,
      "learning_rate": 2.154e-05,
      "loss": 1.5852,
      "step": 11390
    },
    {
      "epoch": 5.7,
      "grad_norm": 7.997380256652832,
      "learning_rate": 2.1515000000000003e-05,
      "loss": 1.5097,
      "step": 11400
    },
    {
      "epoch": 5.705,
      "grad_norm": 7.4859619140625,
      "learning_rate": 2.1490000000000003e-05,
      "loss": 1.4688,
      "step": 11410
    },
    {
      "epoch": 5.71,
      "grad_norm": 12.669533729553223,
      "learning_rate": 2.1465000000000002e-05,
      "loss": 1.2665,
      "step": 11420
    },
    {
      "epoch": 5.715,
      "grad_norm": 11.974539756774902,
      "learning_rate": 2.144e-05,
      "loss": 1.8056,
      "step": 11430
    },
    {
      "epoch": 5.72,
      "grad_norm": 25.865081787109375,
      "learning_rate": 2.1415e-05,
      "loss": 1.0531,
      "step": 11440
    },
    {
      "epoch": 5.725,
      "grad_norm": 9.198467254638672,
      "learning_rate": 2.139e-05,
      "loss": 1.3283,
      "step": 11450
    },
    {
      "epoch": 5.73,
      "grad_norm": 9.440558433532715,
      "learning_rate": 2.1365000000000003e-05,
      "loss": 1.2845,
      "step": 11460
    },
    {
      "epoch": 5.735,
      "grad_norm": 9.296464920043945,
      "learning_rate": 2.1340000000000002e-05,
      "loss": 1.4388,
      "step": 11470
    },
    {
      "epoch": 5.74,
      "grad_norm": 10.561468124389648,
      "learning_rate": 2.1315000000000002e-05,
      "loss": 1.4019,
      "step": 11480
    },
    {
      "epoch": 5.745,
      "grad_norm": 9.30026912689209,
      "learning_rate": 2.129e-05,
      "loss": 1.5244,
      "step": 11490
    },
    {
      "epoch": 5.75,
      "grad_norm": 10.816069602966309,
      "learning_rate": 2.1265e-05,
      "loss": 1.2521,
      "step": 11500
    },
    {
      "epoch": 5.755,
      "grad_norm": 11.752827644348145,
      "learning_rate": 2.124e-05,
      "loss": 1.1657,
      "step": 11510
    },
    {
      "epoch": 5.76,
      "grad_norm": 18.37864112854004,
      "learning_rate": 2.1215000000000003e-05,
      "loss": 1.5253,
      "step": 11520
    },
    {
      "epoch": 5.765,
      "grad_norm": 9.736493110656738,
      "learning_rate": 2.1190000000000002e-05,
      "loss": 1.4769,
      "step": 11530
    },
    {
      "epoch": 5.77,
      "grad_norm": 10.449606895446777,
      "learning_rate": 2.1165e-05,
      "loss": 1.5312,
      "step": 11540
    },
    {
      "epoch": 5.775,
      "grad_norm": 7.345567226409912,
      "learning_rate": 2.114e-05,
      "loss": 1.4194,
      "step": 11550
    },
    {
      "epoch": 5.78,
      "grad_norm": 12.70362377166748,
      "learning_rate": 2.1115e-05,
      "loss": 1.3946,
      "step": 11560
    },
    {
      "epoch": 5.785,
      "grad_norm": 8.312570571899414,
      "learning_rate": 2.1090000000000003e-05,
      "loss": 1.5889,
      "step": 11570
    },
    {
      "epoch": 5.79,
      "grad_norm": 10.903740882873535,
      "learning_rate": 2.1065000000000002e-05,
      "loss": 1.1482,
      "step": 11580
    },
    {
      "epoch": 5.795,
      "grad_norm": 10.887441635131836,
      "learning_rate": 2.1040000000000002e-05,
      "loss": 1.4358,
      "step": 11590
    },
    {
      "epoch": 5.8,
      "grad_norm": 11.386934280395508,
      "learning_rate": 2.1015e-05,
      "loss": 1.1978,
      "step": 11600
    },
    {
      "epoch": 5.805,
      "grad_norm": 11.252781867980957,
      "learning_rate": 2.099e-05,
      "loss": 1.3163,
      "step": 11610
    },
    {
      "epoch": 5.8100000000000005,
      "grad_norm": 15.249492645263672,
      "learning_rate": 2.0965e-05,
      "loss": 1.6103,
      "step": 11620
    },
    {
      "epoch": 5.8149999999999995,
      "grad_norm": 7.618264198303223,
      "learning_rate": 2.0940000000000003e-05,
      "loss": 1.4115,
      "step": 11630
    },
    {
      "epoch": 5.82,
      "grad_norm": 10.754058837890625,
      "learning_rate": 2.0915000000000002e-05,
      "loss": 1.4154,
      "step": 11640
    },
    {
      "epoch": 5.825,
      "grad_norm": 9.375035285949707,
      "learning_rate": 2.089e-05,
      "loss": 1.6031,
      "step": 11650
    },
    {
      "epoch": 5.83,
      "grad_norm": 10.721931457519531,
      "learning_rate": 2.0865e-05,
      "loss": 1.405,
      "step": 11660
    },
    {
      "epoch": 5.835,
      "grad_norm": 9.807100296020508,
      "learning_rate": 2.084e-05,
      "loss": 1.265,
      "step": 11670
    },
    {
      "epoch": 5.84,
      "grad_norm": 8.851980209350586,
      "learning_rate": 2.0815e-05,
      "loss": 1.4616,
      "step": 11680
    },
    {
      "epoch": 5.845,
      "grad_norm": 7.748307228088379,
      "learning_rate": 2.0790000000000003e-05,
      "loss": 1.1704,
      "step": 11690
    },
    {
      "epoch": 5.85,
      "grad_norm": 10.81777572631836,
      "learning_rate": 2.0765000000000002e-05,
      "loss": 1.3346,
      "step": 11700
    },
    {
      "epoch": 5.855,
      "grad_norm": 7.781022548675537,
      "learning_rate": 2.074e-05,
      "loss": 1.6167,
      "step": 11710
    },
    {
      "epoch": 5.86,
      "grad_norm": 10.501485824584961,
      "learning_rate": 2.0715e-05,
      "loss": 1.378,
      "step": 11720
    },
    {
      "epoch": 5.865,
      "grad_norm": 6.899824142456055,
      "learning_rate": 2.069e-05,
      "loss": 1.6577,
      "step": 11730
    },
    {
      "epoch": 5.87,
      "grad_norm": 17.32853889465332,
      "learning_rate": 2.0665e-05,
      "loss": 1.5057,
      "step": 11740
    },
    {
      "epoch": 5.875,
      "grad_norm": 15.616045951843262,
      "learning_rate": 2.0640000000000002e-05,
      "loss": 1.3485,
      "step": 11750
    },
    {
      "epoch": 5.88,
      "grad_norm": 6.1069135665893555,
      "learning_rate": 2.0615e-05,
      "loss": 1.3839,
      "step": 11760
    },
    {
      "epoch": 5.885,
      "grad_norm": 8.429380416870117,
      "learning_rate": 2.059e-05,
      "loss": 1.5255,
      "step": 11770
    },
    {
      "epoch": 5.89,
      "grad_norm": 10.636749267578125,
      "learning_rate": 2.0565e-05,
      "loss": 1.2992,
      "step": 11780
    },
    {
      "epoch": 5.895,
      "grad_norm": 12.082717895507812,
      "learning_rate": 2.054e-05,
      "loss": 1.658,
      "step": 11790
    },
    {
      "epoch": 5.9,
      "grad_norm": 9.994867324829102,
      "learning_rate": 2.0515e-05,
      "loss": 1.4546,
      "step": 11800
    },
    {
      "epoch": 5.905,
      "grad_norm": 9.09635066986084,
      "learning_rate": 2.0490000000000002e-05,
      "loss": 1.2982,
      "step": 11810
    },
    {
      "epoch": 5.91,
      "grad_norm": 35.88840866088867,
      "learning_rate": 2.0465e-05,
      "loss": 1.6411,
      "step": 11820
    },
    {
      "epoch": 5.915,
      "grad_norm": 13.393132209777832,
      "learning_rate": 2.044e-05,
      "loss": 1.2776,
      "step": 11830
    },
    {
      "epoch": 5.92,
      "grad_norm": 8.138215065002441,
      "learning_rate": 2.0415e-05,
      "loss": 1.5449,
      "step": 11840
    },
    {
      "epoch": 5.925,
      "grad_norm": 9.294061660766602,
      "learning_rate": 2.039e-05,
      "loss": 1.3245,
      "step": 11850
    },
    {
      "epoch": 5.93,
      "grad_norm": 7.753568172454834,
      "learning_rate": 2.0365000000000002e-05,
      "loss": 1.4367,
      "step": 11860
    },
    {
      "epoch": 5.9350000000000005,
      "grad_norm": 7.885664939880371,
      "learning_rate": 2.0340000000000002e-05,
      "loss": 1.274,
      "step": 11870
    },
    {
      "epoch": 5.9399999999999995,
      "grad_norm": 18.00552749633789,
      "learning_rate": 2.0315e-05,
      "loss": 1.1068,
      "step": 11880
    },
    {
      "epoch": 5.945,
      "grad_norm": 10.5753812789917,
      "learning_rate": 2.02925e-05,
      "loss": 1.6031,
      "step": 11890
    },
    {
      "epoch": 5.95,
      "grad_norm": 9.300206184387207,
      "learning_rate": 2.0267500000000002e-05,
      "loss": 1.5684,
      "step": 11900
    },
    {
      "epoch": 5.955,
      "grad_norm": 8.93563461303711,
      "learning_rate": 2.02425e-05,
      "loss": 1.3753,
      "step": 11910
    },
    {
      "epoch": 5.96,
      "grad_norm": 10.174415588378906,
      "learning_rate": 2.02175e-05,
      "loss": 1.6213,
      "step": 11920
    },
    {
      "epoch": 5.965,
      "grad_norm": 10.442447662353516,
      "learning_rate": 2.01925e-05,
      "loss": 1.4613,
      "step": 11930
    },
    {
      "epoch": 5.97,
      "grad_norm": 3.429647207260132,
      "learning_rate": 2.01675e-05,
      "loss": 1.4097,
      "step": 11940
    },
    {
      "epoch": 5.975,
      "grad_norm": 15.32680606842041,
      "learning_rate": 2.01425e-05,
      "loss": 1.4522,
      "step": 11950
    },
    {
      "epoch": 5.98,
      "grad_norm": 9.945024490356445,
      "learning_rate": 2.0117500000000002e-05,
      "loss": 1.587,
      "step": 11960
    },
    {
      "epoch": 5.985,
      "grad_norm": 11.632383346557617,
      "learning_rate": 2.00925e-05,
      "loss": 1.2841,
      "step": 11970
    },
    {
      "epoch": 5.99,
      "grad_norm": 15.90316104888916,
      "learning_rate": 2.00675e-05,
      "loss": 1.5423,
      "step": 11980
    },
    {
      "epoch": 5.995,
      "grad_norm": 13.268305778503418,
      "learning_rate": 2.00425e-05,
      "loss": 1.6058,
      "step": 11990
    },
    {
      "epoch": 6.0,
      "grad_norm": 15.621674537658691,
      "learning_rate": 2.00175e-05,
      "loss": 1.556,
      "step": 12000
    },
    {
      "epoch": 6.005,
      "grad_norm": 10.680451393127441,
      "learning_rate": 1.99925e-05,
      "loss": 1.2024,
      "step": 12010
    },
    {
      "epoch": 6.01,
      "grad_norm": 11.096114158630371,
      "learning_rate": 1.99675e-05,
      "loss": 1.4009,
      "step": 12020
    },
    {
      "epoch": 6.015,
      "grad_norm": 8.925422668457031,
      "learning_rate": 1.99425e-05,
      "loss": 1.4009,
      "step": 12030
    },
    {
      "epoch": 6.02,
      "grad_norm": 9.030593872070312,
      "learning_rate": 1.99175e-05,
      "loss": 1.3343,
      "step": 12040
    },
    {
      "epoch": 6.025,
      "grad_norm": 10.513596534729004,
      "learning_rate": 1.98925e-05,
      "loss": 1.426,
      "step": 12050
    },
    {
      "epoch": 6.03,
      "grad_norm": 9.942825317382812,
      "learning_rate": 1.98675e-05,
      "loss": 1.0871,
      "step": 12060
    },
    {
      "epoch": 6.035,
      "grad_norm": 10.819344520568848,
      "learning_rate": 1.98425e-05,
      "loss": 1.319,
      "step": 12070
    },
    {
      "epoch": 6.04,
      "grad_norm": 14.139849662780762,
      "learning_rate": 1.98175e-05,
      "loss": 1.4223,
      "step": 12080
    },
    {
      "epoch": 6.045,
      "grad_norm": 7.984961986541748,
      "learning_rate": 1.97925e-05,
      "loss": 1.5519,
      "step": 12090
    },
    {
      "epoch": 6.05,
      "grad_norm": 10.536524772644043,
      "learning_rate": 1.97675e-05,
      "loss": 1.4521,
      "step": 12100
    },
    {
      "epoch": 6.055,
      "grad_norm": 10.983304023742676,
      "learning_rate": 1.97425e-05,
      "loss": 1.1307,
      "step": 12110
    },
    {
      "epoch": 6.06,
      "grad_norm": 15.294134140014648,
      "learning_rate": 1.97175e-05,
      "loss": 1.4288,
      "step": 12120
    },
    {
      "epoch": 6.065,
      "grad_norm": 8.37975788116455,
      "learning_rate": 1.9692499999999998e-05,
      "loss": 1.2629,
      "step": 12130
    },
    {
      "epoch": 6.07,
      "grad_norm": 8.719934463500977,
      "learning_rate": 1.96675e-05,
      "loss": 1.6844,
      "step": 12140
    },
    {
      "epoch": 6.075,
      "grad_norm": 6.864327430725098,
      "learning_rate": 1.96425e-05,
      "loss": 0.8362,
      "step": 12150
    },
    {
      "epoch": 6.08,
      "grad_norm": 20.963626861572266,
      "learning_rate": 1.96175e-05,
      "loss": 1.6415,
      "step": 12160
    },
    {
      "epoch": 6.085,
      "grad_norm": 11.826521873474121,
      "learning_rate": 1.95925e-05,
      "loss": 1.071,
      "step": 12170
    },
    {
      "epoch": 6.09,
      "grad_norm": 9.235694885253906,
      "learning_rate": 1.95675e-05,
      "loss": 1.2152,
      "step": 12180
    },
    {
      "epoch": 6.095,
      "grad_norm": 13.630230903625488,
      "learning_rate": 1.95425e-05,
      "loss": 1.556,
      "step": 12190
    },
    {
      "epoch": 6.1,
      "grad_norm": 9.681232452392578,
      "learning_rate": 1.95175e-05,
      "loss": 1.4666,
      "step": 12200
    },
    {
      "epoch": 6.105,
      "grad_norm": 11.031668663024902,
      "learning_rate": 1.94925e-05,
      "loss": 1.3132,
      "step": 12210
    },
    {
      "epoch": 6.11,
      "grad_norm": 9.489617347717285,
      "learning_rate": 1.94675e-05,
      "loss": 1.4106,
      "step": 12220
    },
    {
      "epoch": 6.115,
      "grad_norm": 8.355132102966309,
      "learning_rate": 1.94425e-05,
      "loss": 1.1929,
      "step": 12230
    },
    {
      "epoch": 6.12,
      "grad_norm": 5.799477577209473,
      "learning_rate": 1.94175e-05,
      "loss": 0.8891,
      "step": 12240
    },
    {
      "epoch": 6.125,
      "grad_norm": 9.587138175964355,
      "learning_rate": 1.93925e-05,
      "loss": 1.2371,
      "step": 12250
    },
    {
      "epoch": 6.13,
      "grad_norm": 10.311866760253906,
      "learning_rate": 1.9367500000000004e-05,
      "loss": 1.6431,
      "step": 12260
    },
    {
      "epoch": 6.135,
      "grad_norm": 16.35753631591797,
      "learning_rate": 1.9342500000000003e-05,
      "loss": 1.2234,
      "step": 12270
    },
    {
      "epoch": 6.14,
      "grad_norm": 9.416886329650879,
      "learning_rate": 1.9317500000000003e-05,
      "loss": 1.4099,
      "step": 12280
    },
    {
      "epoch": 6.145,
      "grad_norm": 7.517885684967041,
      "learning_rate": 1.9292500000000002e-05,
      "loss": 1.1751,
      "step": 12290
    },
    {
      "epoch": 6.15,
      "grad_norm": 18.45952606201172,
      "learning_rate": 1.92675e-05,
      "loss": 1.3837,
      "step": 12300
    },
    {
      "epoch": 6.155,
      "grad_norm": 16.305011749267578,
      "learning_rate": 1.92425e-05,
      "loss": 1.6833,
      "step": 12310
    },
    {
      "epoch": 6.16,
      "grad_norm": 15.509243965148926,
      "learning_rate": 1.9217500000000004e-05,
      "loss": 1.5551,
      "step": 12320
    },
    {
      "epoch": 6.165,
      "grad_norm": 10.856868743896484,
      "learning_rate": 1.9192500000000003e-05,
      "loss": 1.827,
      "step": 12330
    },
    {
      "epoch": 6.17,
      "grad_norm": 10.657371520996094,
      "learning_rate": 1.9167500000000002e-05,
      "loss": 1.3,
      "step": 12340
    },
    {
      "epoch": 6.175,
      "grad_norm": 11.571272850036621,
      "learning_rate": 1.9142500000000002e-05,
      "loss": 1.4214,
      "step": 12350
    },
    {
      "epoch": 6.18,
      "grad_norm": 12.290665626525879,
      "learning_rate": 1.91175e-05,
      "loss": 1.2268,
      "step": 12360
    },
    {
      "epoch": 6.185,
      "grad_norm": 13.526755332946777,
      "learning_rate": 1.90925e-05,
      "loss": 1.3625,
      "step": 12370
    },
    {
      "epoch": 6.19,
      "grad_norm": 12.43107795715332,
      "learning_rate": 1.9067500000000003e-05,
      "loss": 1.1787,
      "step": 12380
    },
    {
      "epoch": 6.195,
      "grad_norm": 8.385309219360352,
      "learning_rate": 1.9042500000000003e-05,
      "loss": 1.1454,
      "step": 12390
    },
    {
      "epoch": 6.2,
      "grad_norm": 19.447237014770508,
      "learning_rate": 1.9017500000000002e-05,
      "loss": 1.333,
      "step": 12400
    },
    {
      "epoch": 6.205,
      "grad_norm": 21.902111053466797,
      "learning_rate": 1.89925e-05,
      "loss": 1.677,
      "step": 12410
    },
    {
      "epoch": 6.21,
      "grad_norm": 8.551217079162598,
      "learning_rate": 1.89675e-05,
      "loss": 1.4428,
      "step": 12420
    },
    {
      "epoch": 6.215,
      "grad_norm": 12.32699966430664,
      "learning_rate": 1.89425e-05,
      "loss": 1.2433,
      "step": 12430
    },
    {
      "epoch": 6.22,
      "grad_norm": 10.418787956237793,
      "learning_rate": 1.8917500000000003e-05,
      "loss": 1.3015,
      "step": 12440
    },
    {
      "epoch": 6.225,
      "grad_norm": 11.8844633102417,
      "learning_rate": 1.8892500000000002e-05,
      "loss": 1.0611,
      "step": 12450
    },
    {
      "epoch": 6.23,
      "grad_norm": 9.137578010559082,
      "learning_rate": 1.8867500000000002e-05,
      "loss": 1.517,
      "step": 12460
    },
    {
      "epoch": 6.235,
      "grad_norm": 12.870170593261719,
      "learning_rate": 1.88425e-05,
      "loss": 1.5221,
      "step": 12470
    },
    {
      "epoch": 6.24,
      "grad_norm": 15.208890914916992,
      "learning_rate": 1.88175e-05,
      "loss": 1.3445,
      "step": 12480
    },
    {
      "epoch": 6.245,
      "grad_norm": 11.759698867797852,
      "learning_rate": 1.8792500000000003e-05,
      "loss": 1.4406,
      "step": 12490
    },
    {
      "epoch": 6.25,
      "grad_norm": 9.213372230529785,
      "learning_rate": 1.8767500000000003e-05,
      "loss": 1.3813,
      "step": 12500
    },
    {
      "epoch": 6.255,
      "grad_norm": 8.348336219787598,
      "learning_rate": 1.8742500000000002e-05,
      "loss": 1.0579,
      "step": 12510
    },
    {
      "epoch": 6.26,
      "grad_norm": 14.78292465209961,
      "learning_rate": 1.87175e-05,
      "loss": 1.2594,
      "step": 12520
    },
    {
      "epoch": 6.265,
      "grad_norm": 13.16629409790039,
      "learning_rate": 1.86925e-05,
      "loss": 1.3717,
      "step": 12530
    },
    {
      "epoch": 6.27,
      "grad_norm": 3.4848062992095947,
      "learning_rate": 1.86675e-05,
      "loss": 1.4227,
      "step": 12540
    },
    {
      "epoch": 6.275,
      "grad_norm": 8.336298942565918,
      "learning_rate": 1.8642500000000003e-05,
      "loss": 1.156,
      "step": 12550
    },
    {
      "epoch": 6.28,
      "grad_norm": 10.928388595581055,
      "learning_rate": 1.8617500000000003e-05,
      "loss": 1.3166,
      "step": 12560
    },
    {
      "epoch": 6.285,
      "grad_norm": 9.507966995239258,
      "learning_rate": 1.8592500000000002e-05,
      "loss": 1.5186,
      "step": 12570
    },
    {
      "epoch": 6.29,
      "grad_norm": 14.508803367614746,
      "learning_rate": 1.85675e-05,
      "loss": 1.2602,
      "step": 12580
    },
    {
      "epoch": 6.295,
      "grad_norm": 9.673163414001465,
      "learning_rate": 1.85425e-05,
      "loss": 1.3479,
      "step": 12590
    },
    {
      "epoch": 6.3,
      "grad_norm": 8.44310188293457,
      "learning_rate": 1.85175e-05,
      "loss": 1.2952,
      "step": 12600
    },
    {
      "epoch": 6.305,
      "grad_norm": 16.866230010986328,
      "learning_rate": 1.8492500000000003e-05,
      "loss": 1.4951,
      "step": 12610
    },
    {
      "epoch": 6.31,
      "grad_norm": 25.202030181884766,
      "learning_rate": 1.8467500000000002e-05,
      "loss": 1.5117,
      "step": 12620
    },
    {
      "epoch": 6.315,
      "grad_norm": 7.9560651779174805,
      "learning_rate": 1.8442500000000002e-05,
      "loss": 1.5045,
      "step": 12630
    },
    {
      "epoch": 6.32,
      "grad_norm": 7.374784469604492,
      "learning_rate": 1.84175e-05,
      "loss": 1.273,
      "step": 12640
    },
    {
      "epoch": 6.325,
      "grad_norm": 14.465551376342773,
      "learning_rate": 1.83925e-05,
      "loss": 1.5244,
      "step": 12650
    },
    {
      "epoch": 6.33,
      "grad_norm": 8.084637641906738,
      "learning_rate": 1.83675e-05,
      "loss": 1.5111,
      "step": 12660
    },
    {
      "epoch": 6.335,
      "grad_norm": 15.67161750793457,
      "learning_rate": 1.8342500000000003e-05,
      "loss": 1.7495,
      "step": 12670
    },
    {
      "epoch": 6.34,
      "grad_norm": 16.000194549560547,
      "learning_rate": 1.8317500000000002e-05,
      "loss": 1.1248,
      "step": 12680
    },
    {
      "epoch": 6.345,
      "grad_norm": 6.856542110443115,
      "learning_rate": 1.82925e-05,
      "loss": 1.6,
      "step": 12690
    },
    {
      "epoch": 6.35,
      "grad_norm": 14.658571243286133,
      "learning_rate": 1.82675e-05,
      "loss": 1.5697,
      "step": 12700
    },
    {
      "epoch": 6.355,
      "grad_norm": 8.897823333740234,
      "learning_rate": 1.82425e-05,
      "loss": 1.5289,
      "step": 12710
    },
    {
      "epoch": 6.36,
      "grad_norm": 13.145492553710938,
      "learning_rate": 1.82175e-05,
      "loss": 1.1077,
      "step": 12720
    },
    {
      "epoch": 6.365,
      "grad_norm": 6.321283340454102,
      "learning_rate": 1.8192500000000002e-05,
      "loss": 1.2892,
      "step": 12730
    },
    {
      "epoch": 6.37,
      "grad_norm": 10.111543655395508,
      "learning_rate": 1.8167500000000002e-05,
      "loss": 1.3663,
      "step": 12740
    },
    {
      "epoch": 6.375,
      "grad_norm": 16.860090255737305,
      "learning_rate": 1.81425e-05,
      "loss": 1.5224,
      "step": 12750
    },
    {
      "epoch": 6.38,
      "grad_norm": 12.491769790649414,
      "learning_rate": 1.81175e-05,
      "loss": 1.655,
      "step": 12760
    },
    {
      "epoch": 6.385,
      "grad_norm": 11.010844230651855,
      "learning_rate": 1.80925e-05,
      "loss": 1.3997,
      "step": 12770
    },
    {
      "epoch": 6.39,
      "grad_norm": 19.938297271728516,
      "learning_rate": 1.8067500000000003e-05,
      "loss": 1.2562,
      "step": 12780
    },
    {
      "epoch": 6.395,
      "grad_norm": 7.393562316894531,
      "learning_rate": 1.8042500000000002e-05,
      "loss": 1.3681,
      "step": 12790
    },
    {
      "epoch": 6.4,
      "grad_norm": 8.531933784484863,
      "learning_rate": 1.80175e-05,
      "loss": 1.5548,
      "step": 12800
    },
    {
      "epoch": 6.405,
      "grad_norm": 37.863529205322266,
      "learning_rate": 1.79925e-05,
      "loss": 1.6963,
      "step": 12810
    },
    {
      "epoch": 6.41,
      "grad_norm": 11.956579208374023,
      "learning_rate": 1.79675e-05,
      "loss": 1.2917,
      "step": 12820
    },
    {
      "epoch": 6.415,
      "grad_norm": 10.723190307617188,
      "learning_rate": 1.79425e-05,
      "loss": 1.2016,
      "step": 12830
    },
    {
      "epoch": 6.42,
      "grad_norm": 8.76948070526123,
      "learning_rate": 1.7917500000000003e-05,
      "loss": 1.5042,
      "step": 12840
    },
    {
      "epoch": 6.425,
      "grad_norm": 14.84986686706543,
      "learning_rate": 1.7892500000000002e-05,
      "loss": 1.2417,
      "step": 12850
    },
    {
      "epoch": 6.43,
      "grad_norm": 13.178223609924316,
      "learning_rate": 1.78675e-05,
      "loss": 1.4556,
      "step": 12860
    },
    {
      "epoch": 6.435,
      "grad_norm": 10.8202543258667,
      "learning_rate": 1.78425e-05,
      "loss": 1.435,
      "step": 12870
    },
    {
      "epoch": 6.44,
      "grad_norm": 10.287103652954102,
      "learning_rate": 1.78175e-05,
      "loss": 1.3897,
      "step": 12880
    },
    {
      "epoch": 6.445,
      "grad_norm": 9.316651344299316,
      "learning_rate": 1.77925e-05,
      "loss": 1.3878,
      "step": 12890
    },
    {
      "epoch": 6.45,
      "grad_norm": 13.767394065856934,
      "learning_rate": 1.7767500000000002e-05,
      "loss": 1.1355,
      "step": 12900
    },
    {
      "epoch": 6.455,
      "grad_norm": 7.140660285949707,
      "learning_rate": 1.77425e-05,
      "loss": 1.3932,
      "step": 12910
    },
    {
      "epoch": 6.46,
      "grad_norm": 19.620534896850586,
      "learning_rate": 1.77175e-05,
      "loss": 1.5124,
      "step": 12920
    },
    {
      "epoch": 6.465,
      "grad_norm": 7.964268207550049,
      "learning_rate": 1.76925e-05,
      "loss": 1.1974,
      "step": 12930
    },
    {
      "epoch": 6.47,
      "grad_norm": 11.86419677734375,
      "learning_rate": 1.76675e-05,
      "loss": 1.1676,
      "step": 12940
    },
    {
      "epoch": 6.475,
      "grad_norm": 8.952374458312988,
      "learning_rate": 1.76425e-05,
      "loss": 1.3147,
      "step": 12950
    },
    {
      "epoch": 6.48,
      "grad_norm": 11.094085693359375,
      "learning_rate": 1.7617500000000002e-05,
      "loss": 1.4252,
      "step": 12960
    },
    {
      "epoch": 6.485,
      "grad_norm": 9.018543243408203,
      "learning_rate": 1.75925e-05,
      "loss": 1.2555,
      "step": 12970
    },
    {
      "epoch": 6.49,
      "grad_norm": 16.700435638427734,
      "learning_rate": 1.75675e-05,
      "loss": 1.4796,
      "step": 12980
    },
    {
      "epoch": 6.495,
      "grad_norm": 10.346571922302246,
      "learning_rate": 1.75425e-05,
      "loss": 1.6031,
      "step": 12990
    },
    {
      "epoch": 6.5,
      "grad_norm": 14.495729446411133,
      "learning_rate": 1.75175e-05,
      "loss": 1.5699,
      "step": 13000
    },
    {
      "epoch": 6.505,
      "grad_norm": 19.372608184814453,
      "learning_rate": 1.74925e-05,
      "loss": 1.5985,
      "step": 13010
    },
    {
      "epoch": 6.51,
      "grad_norm": 12.635724067687988,
      "learning_rate": 1.7467500000000002e-05,
      "loss": 1.5742,
      "step": 13020
    },
    {
      "epoch": 6.515,
      "grad_norm": 8.46345043182373,
      "learning_rate": 1.74425e-05,
      "loss": 1.5646,
      "step": 13030
    },
    {
      "epoch": 6.52,
      "grad_norm": 7.239118576049805,
      "learning_rate": 1.74175e-05,
      "loss": 1.3658,
      "step": 13040
    },
    {
      "epoch": 6.525,
      "grad_norm": 9.45977783203125,
      "learning_rate": 1.73925e-05,
      "loss": 1.4535,
      "step": 13050
    },
    {
      "epoch": 6.53,
      "grad_norm": 8.249375343322754,
      "learning_rate": 1.73675e-05,
      "loss": 1.2606,
      "step": 13060
    },
    {
      "epoch": 6.535,
      "grad_norm": 7.876945495605469,
      "learning_rate": 1.7342500000000002e-05,
      "loss": 1.2431,
      "step": 13070
    },
    {
      "epoch": 6.54,
      "grad_norm": 8.290129661560059,
      "learning_rate": 1.73175e-05,
      "loss": 1.3763,
      "step": 13080
    },
    {
      "epoch": 6.545,
      "grad_norm": 6.639563083648682,
      "learning_rate": 1.72925e-05,
      "loss": 1.339,
      "step": 13090
    },
    {
      "epoch": 6.55,
      "grad_norm": 8.321603775024414,
      "learning_rate": 1.72675e-05,
      "loss": 1.262,
      "step": 13100
    },
    {
      "epoch": 6.555,
      "grad_norm": 9.785722732543945,
      "learning_rate": 1.72425e-05,
      "loss": 1.2172,
      "step": 13110
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 9.006270408630371,
      "learning_rate": 1.72175e-05,
      "loss": 1.3148,
      "step": 13120
    },
    {
      "epoch": 6.5649999999999995,
      "grad_norm": 18.295677185058594,
      "learning_rate": 1.7192500000000002e-05,
      "loss": 1.3424,
      "step": 13130
    },
    {
      "epoch": 6.57,
      "grad_norm": 13.333073616027832,
      "learning_rate": 1.71675e-05,
      "loss": 1.6101,
      "step": 13140
    },
    {
      "epoch": 6.575,
      "grad_norm": 12.4327974319458,
      "learning_rate": 1.71425e-05,
      "loss": 1.2676,
      "step": 13150
    },
    {
      "epoch": 6.58,
      "grad_norm": 15.226380348205566,
      "learning_rate": 1.71175e-05,
      "loss": 1.3688,
      "step": 13160
    },
    {
      "epoch": 6.585,
      "grad_norm": 11.17329216003418,
      "learning_rate": 1.70925e-05,
      "loss": 1.0225,
      "step": 13170
    },
    {
      "epoch": 6.59,
      "grad_norm": 16.547197341918945,
      "learning_rate": 1.70675e-05,
      "loss": 1.7075,
      "step": 13180
    },
    {
      "epoch": 6.595,
      "grad_norm": 9.850931167602539,
      "learning_rate": 1.70425e-05,
      "loss": 1.6945,
      "step": 13190
    },
    {
      "epoch": 6.6,
      "grad_norm": 5.283811569213867,
      "learning_rate": 1.70175e-05,
      "loss": 1.2816,
      "step": 13200
    },
    {
      "epoch": 6.605,
      "grad_norm": 11.05231761932373,
      "learning_rate": 1.69925e-05,
      "loss": 1.537,
      "step": 13210
    },
    {
      "epoch": 6.61,
      "grad_norm": 19.2031307220459,
      "learning_rate": 1.69675e-05,
      "loss": 1.4725,
      "step": 13220
    },
    {
      "epoch": 6.615,
      "grad_norm": 10.985647201538086,
      "learning_rate": 1.69425e-05,
      "loss": 1.414,
      "step": 13230
    },
    {
      "epoch": 6.62,
      "grad_norm": 11.993797302246094,
      "learning_rate": 1.69175e-05,
      "loss": 1.2903,
      "step": 13240
    },
    {
      "epoch": 6.625,
      "grad_norm": 26.856910705566406,
      "learning_rate": 1.68925e-05,
      "loss": 1.3329,
      "step": 13250
    },
    {
      "epoch": 6.63,
      "grad_norm": 6.341915130615234,
      "learning_rate": 1.68675e-05,
      "loss": 1.2024,
      "step": 13260
    },
    {
      "epoch": 6.635,
      "grad_norm": 14.622100830078125,
      "learning_rate": 1.68425e-05,
      "loss": 1.3197,
      "step": 13270
    },
    {
      "epoch": 6.64,
      "grad_norm": 10.251432418823242,
      "learning_rate": 1.68175e-05,
      "loss": 1.4997,
      "step": 13280
    },
    {
      "epoch": 6.645,
      "grad_norm": 8.846360206604004,
      "learning_rate": 1.67925e-05,
      "loss": 1.4187,
      "step": 13290
    },
    {
      "epoch": 6.65,
      "grad_norm": 8.505555152893066,
      "learning_rate": 1.6767499999999998e-05,
      "loss": 1.6882,
      "step": 13300
    },
    {
      "epoch": 6.655,
      "grad_norm": 9.598162651062012,
      "learning_rate": 1.67425e-05,
      "loss": 1.3557,
      "step": 13310
    },
    {
      "epoch": 6.66,
      "grad_norm": 14.496675491333008,
      "learning_rate": 1.67175e-05,
      "loss": 1.5182,
      "step": 13320
    },
    {
      "epoch": 6.665,
      "grad_norm": 9.006491661071777,
      "learning_rate": 1.66925e-05,
      "loss": 1.3557,
      "step": 13330
    },
    {
      "epoch": 6.67,
      "grad_norm": 11.893783569335938,
      "learning_rate": 1.66675e-05,
      "loss": 1.5459,
      "step": 13340
    },
    {
      "epoch": 6.675,
      "grad_norm": 14.46450424194336,
      "learning_rate": 1.66425e-05,
      "loss": 1.2511,
      "step": 13350
    },
    {
      "epoch": 6.68,
      "grad_norm": 11.271031379699707,
      "learning_rate": 1.66175e-05,
      "loss": 1.4957,
      "step": 13360
    },
    {
      "epoch": 6.6850000000000005,
      "grad_norm": 16.619421005249023,
      "learning_rate": 1.65925e-05,
      "loss": 1.2739,
      "step": 13370
    },
    {
      "epoch": 6.6899999999999995,
      "grad_norm": 12.068692207336426,
      "learning_rate": 1.65675e-05,
      "loss": 1.4801,
      "step": 13380
    },
    {
      "epoch": 6.695,
      "grad_norm": 9.719405174255371,
      "learning_rate": 1.65425e-05,
      "loss": 1.2381,
      "step": 13390
    },
    {
      "epoch": 6.7,
      "grad_norm": 16.29535675048828,
      "learning_rate": 1.65175e-05,
      "loss": 1.3287,
      "step": 13400
    },
    {
      "epoch": 6.705,
      "grad_norm": 10.64688491821289,
      "learning_rate": 1.64925e-05,
      "loss": 0.8417,
      "step": 13410
    },
    {
      "epoch": 6.71,
      "grad_norm": 11.592047691345215,
      "learning_rate": 1.64675e-05,
      "loss": 1.4324,
      "step": 13420
    },
    {
      "epoch": 6.715,
      "grad_norm": 9.632554054260254,
      "learning_rate": 1.64425e-05,
      "loss": 1.5513,
      "step": 13430
    },
    {
      "epoch": 6.72,
      "grad_norm": 13.69654369354248,
      "learning_rate": 1.64175e-05,
      "loss": 1.1137,
      "step": 13440
    },
    {
      "epoch": 6.725,
      "grad_norm": 6.073660373687744,
      "learning_rate": 1.63925e-05,
      "loss": 1.5658,
      "step": 13450
    },
    {
      "epoch": 6.73,
      "grad_norm": 13.953903198242188,
      "learning_rate": 1.63675e-05,
      "loss": 1.3822,
      "step": 13460
    },
    {
      "epoch": 6.735,
      "grad_norm": 8.000772476196289,
      "learning_rate": 1.6342499999999998e-05,
      "loss": 1.3229,
      "step": 13470
    },
    {
      "epoch": 6.74,
      "grad_norm": 11.377580642700195,
      "learning_rate": 1.63175e-05,
      "loss": 1.0882,
      "step": 13480
    },
    {
      "epoch": 6.745,
      "grad_norm": 8.544958114624023,
      "learning_rate": 1.62925e-05,
      "loss": 1.5674,
      "step": 13490
    },
    {
      "epoch": 6.75,
      "grad_norm": 9.52164077758789,
      "learning_rate": 1.62675e-05,
      "loss": 1.2131,
      "step": 13500
    },
    {
      "epoch": 6.755,
      "grad_norm": 7.586373329162598,
      "learning_rate": 1.6242500000000002e-05,
      "loss": 1.5031,
      "step": 13510
    },
    {
      "epoch": 6.76,
      "grad_norm": 7.341939449310303,
      "learning_rate": 1.6217500000000002e-05,
      "loss": 1.3086,
      "step": 13520
    },
    {
      "epoch": 6.765,
      "grad_norm": 18.424663543701172,
      "learning_rate": 1.61925e-05,
      "loss": 1.4575,
      "step": 13530
    },
    {
      "epoch": 6.77,
      "grad_norm": 10.294825553894043,
      "learning_rate": 1.61675e-05,
      "loss": 1.139,
      "step": 13540
    },
    {
      "epoch": 6.775,
      "grad_norm": 10.356466293334961,
      "learning_rate": 1.6142500000000003e-05,
      "loss": 1.2906,
      "step": 13550
    },
    {
      "epoch": 6.78,
      "grad_norm": 3.1952250003814697,
      "learning_rate": 1.6117500000000003e-05,
      "loss": 1.613,
      "step": 13560
    },
    {
      "epoch": 6.785,
      "grad_norm": 14.396075248718262,
      "learning_rate": 1.6092500000000002e-05,
      "loss": 1.0053,
      "step": 13570
    },
    {
      "epoch": 6.79,
      "grad_norm": 15.812240600585938,
      "learning_rate": 1.60675e-05,
      "loss": 1.6089,
      "step": 13580
    },
    {
      "epoch": 6.795,
      "grad_norm": 10.815146446228027,
      "learning_rate": 1.60425e-05,
      "loss": 1.5892,
      "step": 13590
    },
    {
      "epoch": 6.8,
      "grad_norm": 12.193781852722168,
      "learning_rate": 1.60175e-05,
      "loss": 1.4094,
      "step": 13600
    },
    {
      "epoch": 6.805,
      "grad_norm": 10.475726127624512,
      "learning_rate": 1.5992500000000003e-05,
      "loss": 1.4499,
      "step": 13610
    },
    {
      "epoch": 6.8100000000000005,
      "grad_norm": 8.839152336120605,
      "learning_rate": 1.597e-05,
      "loss": 1.4332,
      "step": 13620
    },
    {
      "epoch": 6.8149999999999995,
      "grad_norm": 10.848367691040039,
      "learning_rate": 1.5945e-05,
      "loss": 1.3869,
      "step": 13630
    },
    {
      "epoch": 6.82,
      "grad_norm": 10.966277122497559,
      "learning_rate": 1.592e-05,
      "loss": 1.3403,
      "step": 13640
    },
    {
      "epoch": 6.825,
      "grad_norm": 21.044282913208008,
      "learning_rate": 1.5895000000000003e-05,
      "loss": 1.3797,
      "step": 13650
    },
    {
      "epoch": 6.83,
      "grad_norm": 9.201872825622559,
      "learning_rate": 1.5870000000000002e-05,
      "loss": 1.6097,
      "step": 13660
    },
    {
      "epoch": 6.835,
      "grad_norm": 8.568415641784668,
      "learning_rate": 1.5845e-05,
      "loss": 1.463,
      "step": 13670
    },
    {
      "epoch": 6.84,
      "grad_norm": 9.173192977905273,
      "learning_rate": 1.582e-05,
      "loss": 1.3836,
      "step": 13680
    },
    {
      "epoch": 6.845,
      "grad_norm": 9.971323013305664,
      "learning_rate": 1.5795e-05,
      "loss": 1.1512,
      "step": 13690
    },
    {
      "epoch": 6.85,
      "grad_norm": 12.243620872497559,
      "learning_rate": 1.577e-05,
      "loss": 1.3103,
      "step": 13700
    },
    {
      "epoch": 6.855,
      "grad_norm": 13.875969886779785,
      "learning_rate": 1.5745000000000003e-05,
      "loss": 1.4716,
      "step": 13710
    },
    {
      "epoch": 6.86,
      "grad_norm": 9.82236099243164,
      "learning_rate": 1.5720000000000002e-05,
      "loss": 1.6434,
      "step": 13720
    },
    {
      "epoch": 6.865,
      "grad_norm": 7.271821022033691,
      "learning_rate": 1.5695e-05,
      "loss": 1.6269,
      "step": 13730
    },
    {
      "epoch": 6.87,
      "grad_norm": 9.623581886291504,
      "learning_rate": 1.567e-05,
      "loss": 1.3261,
      "step": 13740
    },
    {
      "epoch": 6.875,
      "grad_norm": 12.90614128112793,
      "learning_rate": 1.5645e-05,
      "loss": 1.3991,
      "step": 13750
    },
    {
      "epoch": 6.88,
      "grad_norm": 10.009925842285156,
      "learning_rate": 1.5620000000000003e-05,
      "loss": 1.2838,
      "step": 13760
    },
    {
      "epoch": 6.885,
      "grad_norm": 14.279468536376953,
      "learning_rate": 1.5595000000000002e-05,
      "loss": 1.5848,
      "step": 13770
    },
    {
      "epoch": 6.89,
      "grad_norm": 11.479314804077148,
      "learning_rate": 1.5570000000000002e-05,
      "loss": 1.4062,
      "step": 13780
    },
    {
      "epoch": 6.895,
      "grad_norm": 8.989198684692383,
      "learning_rate": 1.5545e-05,
      "loss": 1.386,
      "step": 13790
    },
    {
      "epoch": 6.9,
      "grad_norm": 7.031529426574707,
      "learning_rate": 1.552e-05,
      "loss": 1.6951,
      "step": 13800
    },
    {
      "epoch": 6.905,
      "grad_norm": 13.24977970123291,
      "learning_rate": 1.5495e-05,
      "loss": 1.2073,
      "step": 13810
    },
    {
      "epoch": 6.91,
      "grad_norm": 9.259722709655762,
      "learning_rate": 1.5470000000000003e-05,
      "loss": 1.1012,
      "step": 13820
    },
    {
      "epoch": 6.915,
      "grad_norm": 12.523540496826172,
      "learning_rate": 1.5445000000000002e-05,
      "loss": 1.5334,
      "step": 13830
    },
    {
      "epoch": 6.92,
      "grad_norm": 13.103689193725586,
      "learning_rate": 1.542e-05,
      "loss": 1.0144,
      "step": 13840
    },
    {
      "epoch": 6.925,
      "grad_norm": 10.266468048095703,
      "learning_rate": 1.5395e-05,
      "loss": 1.5092,
      "step": 13850
    },
    {
      "epoch": 6.93,
      "grad_norm": 9.303352355957031,
      "learning_rate": 1.537e-05,
      "loss": 1.3105,
      "step": 13860
    },
    {
      "epoch": 6.9350000000000005,
      "grad_norm": 10.103008270263672,
      "learning_rate": 1.5345e-05,
      "loss": 1.2205,
      "step": 13870
    },
    {
      "epoch": 6.9399999999999995,
      "grad_norm": 10.566364288330078,
      "learning_rate": 1.5320000000000002e-05,
      "loss": 1.4859,
      "step": 13880
    },
    {
      "epoch": 6.945,
      "grad_norm": 13.522005081176758,
      "learning_rate": 1.5295000000000002e-05,
      "loss": 1.7145,
      "step": 13890
    },
    {
      "epoch": 6.95,
      "grad_norm": 11.001720428466797,
      "learning_rate": 1.527e-05,
      "loss": 1.5214,
      "step": 13900
    },
    {
      "epoch": 6.955,
      "grad_norm": 9.457428932189941,
      "learning_rate": 1.5245e-05,
      "loss": 1.5304,
      "step": 13910
    },
    {
      "epoch": 6.96,
      "grad_norm": 9.590585708618164,
      "learning_rate": 1.5220000000000002e-05,
      "loss": 1.4206,
      "step": 13920
    },
    {
      "epoch": 6.965,
      "grad_norm": 9.68995475769043,
      "learning_rate": 1.5195000000000001e-05,
      "loss": 1.6139,
      "step": 13930
    },
    {
      "epoch": 6.97,
      "grad_norm": 13.00925064086914,
      "learning_rate": 1.517e-05,
      "loss": 1.1523,
      "step": 13940
    },
    {
      "epoch": 6.975,
      "grad_norm": 12.606402397155762,
      "learning_rate": 1.5145000000000002e-05,
      "loss": 1.3643,
      "step": 13950
    },
    {
      "epoch": 6.98,
      "grad_norm": 8.260712623596191,
      "learning_rate": 1.5120000000000001e-05,
      "loss": 1.6363,
      "step": 13960
    },
    {
      "epoch": 6.985,
      "grad_norm": 8.472787857055664,
      "learning_rate": 1.5095e-05,
      "loss": 1.3921,
      "step": 13970
    },
    {
      "epoch": 6.99,
      "grad_norm": 7.572866439819336,
      "learning_rate": 1.5070000000000001e-05,
      "loss": 1.5461,
      "step": 13980
    },
    {
      "epoch": 6.995,
      "grad_norm": 12.95375919342041,
      "learning_rate": 1.5045e-05,
      "loss": 1.3215,
      "step": 13990
    },
    {
      "epoch": 7.0,
      "grad_norm": 7.497564792633057,
      "learning_rate": 1.502e-05,
      "loss": 1.42,
      "step": 14000
    },
    {
      "epoch": 7.005,
      "grad_norm": 10.70007610321045,
      "learning_rate": 1.4995000000000001e-05,
      "loss": 1.5704,
      "step": 14010
    },
    {
      "epoch": 7.01,
      "grad_norm": 9.334879875183105,
      "learning_rate": 1.497e-05,
      "loss": 1.5313,
      "step": 14020
    },
    {
      "epoch": 7.015,
      "grad_norm": 11.417760848999023,
      "learning_rate": 1.4945e-05,
      "loss": 1.1711,
      "step": 14030
    },
    {
      "epoch": 7.02,
      "grad_norm": 8.144631385803223,
      "learning_rate": 1.4920000000000001e-05,
      "loss": 1.6887,
      "step": 14040
    },
    {
      "epoch": 7.025,
      "grad_norm": 7.2280402183532715,
      "learning_rate": 1.4895e-05,
      "loss": 1.2115,
      "step": 14050
    },
    {
      "epoch": 7.03,
      "grad_norm": 9.773512840270996,
      "learning_rate": 1.487e-05,
      "loss": 1.5042,
      "step": 14060
    },
    {
      "epoch": 7.035,
      "grad_norm": 8.717400550842285,
      "learning_rate": 1.4845000000000001e-05,
      "loss": 1.0839,
      "step": 14070
    },
    {
      "epoch": 7.04,
      "grad_norm": 8.606925964355469,
      "learning_rate": 1.482e-05,
      "loss": 1.5071,
      "step": 14080
    },
    {
      "epoch": 7.045,
      "grad_norm": 11.881999015808105,
      "learning_rate": 1.4795e-05,
      "loss": 0.9707,
      "step": 14090
    },
    {
      "epoch": 7.05,
      "grad_norm": 14.924909591674805,
      "learning_rate": 1.4770000000000001e-05,
      "loss": 1.4016,
      "step": 14100
    },
    {
      "epoch": 7.055,
      "grad_norm": 4.701065540313721,
      "learning_rate": 1.4745e-05,
      "loss": 1.3997,
      "step": 14110
    },
    {
      "epoch": 7.06,
      "grad_norm": 21.524925231933594,
      "learning_rate": 1.472e-05,
      "loss": 1.4585,
      "step": 14120
    },
    {
      "epoch": 7.065,
      "grad_norm": 9.719365119934082,
      "learning_rate": 1.4695e-05,
      "loss": 1.4585,
      "step": 14130
    },
    {
      "epoch": 7.07,
      "grad_norm": 15.25279712677002,
      "learning_rate": 1.467e-05,
      "loss": 1.521,
      "step": 14140
    },
    {
      "epoch": 7.075,
      "grad_norm": 8.267512321472168,
      "learning_rate": 1.4645e-05,
      "loss": 1.3394,
      "step": 14150
    },
    {
      "epoch": 7.08,
      "grad_norm": 15.819769859313965,
      "learning_rate": 1.462e-05,
      "loss": 1.6216,
      "step": 14160
    },
    {
      "epoch": 7.085,
      "grad_norm": 10.713112831115723,
      "learning_rate": 1.4595e-05,
      "loss": 1.2429,
      "step": 14170
    },
    {
      "epoch": 7.09,
      "grad_norm": 11.914334297180176,
      "learning_rate": 1.4570000000000001e-05,
      "loss": 1.0236,
      "step": 14180
    },
    {
      "epoch": 7.095,
      "grad_norm": 9.884017944335938,
      "learning_rate": 1.4545e-05,
      "loss": 1.7589,
      "step": 14190
    },
    {
      "epoch": 7.1,
      "grad_norm": 13.697054862976074,
      "learning_rate": 1.452e-05,
      "loss": 1.3975,
      "step": 14200
    },
    {
      "epoch": 7.105,
      "grad_norm": 12.137537956237793,
      "learning_rate": 1.4495000000000001e-05,
      "loss": 1.4088,
      "step": 14210
    },
    {
      "epoch": 7.11,
      "grad_norm": 10.879427909851074,
      "learning_rate": 1.447e-05,
      "loss": 1.5138,
      "step": 14220
    },
    {
      "epoch": 7.115,
      "grad_norm": 19.53947639465332,
      "learning_rate": 1.4445e-05,
      "loss": 1.5629,
      "step": 14230
    },
    {
      "epoch": 7.12,
      "grad_norm": 17.260412216186523,
      "learning_rate": 1.4420000000000001e-05,
      "loss": 1.0345,
      "step": 14240
    },
    {
      "epoch": 7.125,
      "grad_norm": 9.732755661010742,
      "learning_rate": 1.4395e-05,
      "loss": 1.6346,
      "step": 14250
    },
    {
      "epoch": 7.13,
      "grad_norm": 13.60247802734375,
      "learning_rate": 1.437e-05,
      "loss": 1.3922,
      "step": 14260
    },
    {
      "epoch": 7.135,
      "grad_norm": 7.342385292053223,
      "learning_rate": 1.4345e-05,
      "loss": 1.1753,
      "step": 14270
    },
    {
      "epoch": 7.14,
      "grad_norm": 8.891366004943848,
      "learning_rate": 1.432e-05,
      "loss": 1.0553,
      "step": 14280
    },
    {
      "epoch": 7.145,
      "grad_norm": 12.879212379455566,
      "learning_rate": 1.4295e-05,
      "loss": 1.2423,
      "step": 14290
    },
    {
      "epoch": 7.15,
      "grad_norm": 12.40346908569336,
      "learning_rate": 1.427e-05,
      "loss": 1.6182,
      "step": 14300
    },
    {
      "epoch": 7.155,
      "grad_norm": 18.93574333190918,
      "learning_rate": 1.4245e-05,
      "loss": 1.1563,
      "step": 14310
    },
    {
      "epoch": 7.16,
      "grad_norm": 11.769803047180176,
      "learning_rate": 1.422e-05,
      "loss": 1.6413,
      "step": 14320
    },
    {
      "epoch": 7.165,
      "grad_norm": 10.069984436035156,
      "learning_rate": 1.4195e-05,
      "loss": 1.2666,
      "step": 14330
    },
    {
      "epoch": 7.17,
      "grad_norm": 10.134649276733398,
      "learning_rate": 1.417e-05,
      "loss": 1.3613,
      "step": 14340
    },
    {
      "epoch": 7.175,
      "grad_norm": 8.565126419067383,
      "learning_rate": 1.4145e-05,
      "loss": 1.4707,
      "step": 14350
    },
    {
      "epoch": 7.18,
      "grad_norm": 10.895659446716309,
      "learning_rate": 1.412e-05,
      "loss": 1.6351,
      "step": 14360
    },
    {
      "epoch": 7.185,
      "grad_norm": 7.435609340667725,
      "learning_rate": 1.4095e-05,
      "loss": 1.4442,
      "step": 14370
    },
    {
      "epoch": 7.19,
      "grad_norm": 16.88941764831543,
      "learning_rate": 1.4069999999999999e-05,
      "loss": 1.3917,
      "step": 14380
    },
    {
      "epoch": 7.195,
      "grad_norm": 8.632956504821777,
      "learning_rate": 1.4045e-05,
      "loss": 1.2788,
      "step": 14390
    },
    {
      "epoch": 7.2,
      "grad_norm": 11.902286529541016,
      "learning_rate": 1.402e-05,
      "loss": 1.5224,
      "step": 14400
    },
    {
      "epoch": 7.205,
      "grad_norm": 26.931856155395508,
      "learning_rate": 1.3994999999999999e-05,
      "loss": 1.191,
      "step": 14410
    },
    {
      "epoch": 7.21,
      "grad_norm": 12.448493957519531,
      "learning_rate": 1.397e-05,
      "loss": 1.3012,
      "step": 14420
    },
    {
      "epoch": 7.215,
      "grad_norm": 8.465118408203125,
      "learning_rate": 1.3945e-05,
      "loss": 1.1762,
      "step": 14430
    },
    {
      "epoch": 7.22,
      "grad_norm": 9.086732864379883,
      "learning_rate": 1.3919999999999999e-05,
      "loss": 1.2532,
      "step": 14440
    },
    {
      "epoch": 7.225,
      "grad_norm": 15.896384239196777,
      "learning_rate": 1.3895e-05,
      "loss": 1.0456,
      "step": 14450
    },
    {
      "epoch": 7.23,
      "grad_norm": 4.4498491287231445,
      "learning_rate": 1.387e-05,
      "loss": 1.2613,
      "step": 14460
    },
    {
      "epoch": 7.235,
      "grad_norm": 7.123124599456787,
      "learning_rate": 1.3845e-05,
      "loss": 1.438,
      "step": 14470
    },
    {
      "epoch": 7.24,
      "grad_norm": 15.794777870178223,
      "learning_rate": 1.382e-05,
      "loss": 1.3825,
      "step": 14480
    },
    {
      "epoch": 7.245,
      "grad_norm": 15.897557258605957,
      "learning_rate": 1.3795e-05,
      "loss": 1.3608,
      "step": 14490
    },
    {
      "epoch": 7.25,
      "grad_norm": 10.995614051818848,
      "learning_rate": 1.377e-05,
      "loss": 1.1607,
      "step": 14500
    },
    {
      "epoch": 7.255,
      "grad_norm": 7.789018154144287,
      "learning_rate": 1.3745e-05,
      "loss": 1.5024,
      "step": 14510
    },
    {
      "epoch": 7.26,
      "grad_norm": 9.136334419250488,
      "learning_rate": 1.3719999999999999e-05,
      "loss": 1.6433,
      "step": 14520
    },
    {
      "epoch": 7.265,
      "grad_norm": 10.302796363830566,
      "learning_rate": 1.3695e-05,
      "loss": 1.5772,
      "step": 14530
    },
    {
      "epoch": 7.27,
      "grad_norm": 9.707849502563477,
      "learning_rate": 1.367e-05,
      "loss": 1.5396,
      "step": 14540
    },
    {
      "epoch": 7.275,
      "grad_norm": 11.664998054504395,
      "learning_rate": 1.3644999999999999e-05,
      "loss": 1.436,
      "step": 14550
    },
    {
      "epoch": 7.28,
      "grad_norm": 10.750310897827148,
      "learning_rate": 1.362e-05,
      "loss": 1.1917,
      "step": 14560
    },
    {
      "epoch": 7.285,
      "grad_norm": 7.128277778625488,
      "learning_rate": 1.3595e-05,
      "loss": 1.3188,
      "step": 14570
    },
    {
      "epoch": 7.29,
      "grad_norm": 20.372047424316406,
      "learning_rate": 1.3569999999999999e-05,
      "loss": 1.2582,
      "step": 14580
    },
    {
      "epoch": 7.295,
      "grad_norm": 14.625643730163574,
      "learning_rate": 1.3545e-05,
      "loss": 1.0524,
      "step": 14590
    },
    {
      "epoch": 7.3,
      "grad_norm": 10.940330505371094,
      "learning_rate": 1.352e-05,
      "loss": 0.8029,
      "step": 14600
    },
    {
      "epoch": 7.305,
      "grad_norm": 12.052569389343262,
      "learning_rate": 1.3494999999999999e-05,
      "loss": 1.477,
      "step": 14610
    },
    {
      "epoch": 7.31,
      "grad_norm": 11.816007614135742,
      "learning_rate": 1.347e-05,
      "loss": 1.5342,
      "step": 14620
    },
    {
      "epoch": 7.315,
      "grad_norm": 2.299139976501465,
      "learning_rate": 1.3445e-05,
      "loss": 1.2561,
      "step": 14630
    },
    {
      "epoch": 7.32,
      "grad_norm": 23.569578170776367,
      "learning_rate": 1.3420000000000002e-05,
      "loss": 1.391,
      "step": 14640
    },
    {
      "epoch": 7.325,
      "grad_norm": 8.214024543762207,
      "learning_rate": 1.3395000000000001e-05,
      "loss": 1.4636,
      "step": 14650
    },
    {
      "epoch": 7.33,
      "grad_norm": 10.464752197265625,
      "learning_rate": 1.3370000000000002e-05,
      "loss": 1.6909,
      "step": 14660
    },
    {
      "epoch": 7.335,
      "grad_norm": 8.793702125549316,
      "learning_rate": 1.3345000000000002e-05,
      "loss": 1.348,
      "step": 14670
    },
    {
      "epoch": 7.34,
      "grad_norm": 9.95125961303711,
      "learning_rate": 1.3320000000000001e-05,
      "loss": 1.0138,
      "step": 14680
    },
    {
      "epoch": 7.345,
      "grad_norm": 9.58221435546875,
      "learning_rate": 1.3295000000000002e-05,
      "loss": 1.4955,
      "step": 14690
    },
    {
      "epoch": 7.35,
      "grad_norm": 14.327465057373047,
      "learning_rate": 1.3270000000000002e-05,
      "loss": 1.6542,
      "step": 14700
    },
    {
      "epoch": 7.355,
      "grad_norm": 17.304561614990234,
      "learning_rate": 1.3245000000000001e-05,
      "loss": 1.0763,
      "step": 14710
    },
    {
      "epoch": 7.36,
      "grad_norm": 9.224785804748535,
      "learning_rate": 1.3220000000000002e-05,
      "loss": 1.4313,
      "step": 14720
    },
    {
      "epoch": 7.365,
      "grad_norm": 12.949785232543945,
      "learning_rate": 1.3195000000000002e-05,
      "loss": 1.6685,
      "step": 14730
    },
    {
      "epoch": 7.37,
      "grad_norm": 12.111857414245605,
      "learning_rate": 1.3170000000000001e-05,
      "loss": 1.5282,
      "step": 14740
    },
    {
      "epoch": 7.375,
      "grad_norm": 8.023566246032715,
      "learning_rate": 1.3145000000000002e-05,
      "loss": 1.2511,
      "step": 14750
    },
    {
      "epoch": 7.38,
      "grad_norm": 8.671634674072266,
      "learning_rate": 1.3120000000000001e-05,
      "loss": 1.4159,
      "step": 14760
    },
    {
      "epoch": 7.385,
      "grad_norm": 8.550018310546875,
      "learning_rate": 1.3095000000000003e-05,
      "loss": 1.3523,
      "step": 14770
    },
    {
      "epoch": 7.39,
      "grad_norm": 4.149327278137207,
      "learning_rate": 1.3070000000000002e-05,
      "loss": 1.2398,
      "step": 14780
    },
    {
      "epoch": 7.395,
      "grad_norm": 15.096786499023438,
      "learning_rate": 1.3045000000000001e-05,
      "loss": 1.3897,
      "step": 14790
    },
    {
      "epoch": 7.4,
      "grad_norm": 8.337715148925781,
      "learning_rate": 1.3020000000000002e-05,
      "loss": 1.4105,
      "step": 14800
    },
    {
      "epoch": 7.405,
      "grad_norm": 18.48263931274414,
      "learning_rate": 1.2995000000000002e-05,
      "loss": 1.3941,
      "step": 14810
    },
    {
      "epoch": 7.41,
      "grad_norm": 11.397296905517578,
      "learning_rate": 1.2970000000000001e-05,
      "loss": 1.4821,
      "step": 14820
    },
    {
      "epoch": 7.415,
      "grad_norm": 19.864185333251953,
      "learning_rate": 1.2945000000000002e-05,
      "loss": 1.1238,
      "step": 14830
    },
    {
      "epoch": 7.42,
      "grad_norm": 7.721499443054199,
      "learning_rate": 1.2920000000000002e-05,
      "loss": 1.3037,
      "step": 14840
    },
    {
      "epoch": 7.425,
      "grad_norm": 11.109038352966309,
      "learning_rate": 1.2895000000000001e-05,
      "loss": 1.3779,
      "step": 14850
    },
    {
      "epoch": 7.43,
      "grad_norm": 10.3799467086792,
      "learning_rate": 1.2870000000000002e-05,
      "loss": 1.8833,
      "step": 14860
    },
    {
      "epoch": 7.435,
      "grad_norm": 7.049321174621582,
      "learning_rate": 1.2845000000000002e-05,
      "loss": 1.3471,
      "step": 14870
    },
    {
      "epoch": 7.44,
      "grad_norm": 10.11280345916748,
      "learning_rate": 1.2820000000000001e-05,
      "loss": 1.3497,
      "step": 14880
    },
    {
      "epoch": 7.445,
      "grad_norm": 16.918947219848633,
      "learning_rate": 1.2795000000000002e-05,
      "loss": 1.4566,
      "step": 14890
    },
    {
      "epoch": 7.45,
      "grad_norm": 9.754278182983398,
      "learning_rate": 1.2770000000000001e-05,
      "loss": 1.2492,
      "step": 14900
    },
    {
      "epoch": 7.455,
      "grad_norm": 10.068087577819824,
      "learning_rate": 1.2745e-05,
      "loss": 1.2554,
      "step": 14910
    },
    {
      "epoch": 7.46,
      "grad_norm": 10.563584327697754,
      "learning_rate": 1.2720000000000002e-05,
      "loss": 1.5539,
      "step": 14920
    },
    {
      "epoch": 7.465,
      "grad_norm": 14.33267879486084,
      "learning_rate": 1.2695000000000001e-05,
      "loss": 1.2446,
      "step": 14930
    },
    {
      "epoch": 7.47,
      "grad_norm": 15.317033767700195,
      "learning_rate": 1.267e-05,
      "loss": 1.0768,
      "step": 14940
    },
    {
      "epoch": 7.475,
      "grad_norm": 11.399691581726074,
      "learning_rate": 1.2645000000000002e-05,
      "loss": 1.3367,
      "step": 14950
    },
    {
      "epoch": 7.48,
      "grad_norm": 9.48040771484375,
      "learning_rate": 1.2620000000000001e-05,
      "loss": 1.2204,
      "step": 14960
    },
    {
      "epoch": 7.485,
      "grad_norm": 8.707855224609375,
      "learning_rate": 1.2595e-05,
      "loss": 1.3721,
      "step": 14970
    },
    {
      "epoch": 7.49,
      "grad_norm": 11.658935546875,
      "learning_rate": 1.2570000000000002e-05,
      "loss": 1.4486,
      "step": 14980
    },
    {
      "epoch": 7.495,
      "grad_norm": 9.546170234680176,
      "learning_rate": 1.2545000000000001e-05,
      "loss": 1.4415,
      "step": 14990
    },
    {
      "epoch": 7.5,
      "grad_norm": 8.880865097045898,
      "learning_rate": 1.252e-05,
      "loss": 0.9668,
      "step": 15000
    },
    {
      "epoch": 7.505,
      "grad_norm": 15.774835586547852,
      "learning_rate": 1.2495000000000001e-05,
      "loss": 1.555,
      "step": 15010
    },
    {
      "epoch": 7.51,
      "grad_norm": 8.209112167358398,
      "learning_rate": 1.2470000000000001e-05,
      "loss": 1.2206,
      "step": 15020
    },
    {
      "epoch": 7.515,
      "grad_norm": 10.865178108215332,
      "learning_rate": 1.2445e-05,
      "loss": 1.6345,
      "step": 15030
    },
    {
      "epoch": 7.52,
      "grad_norm": 8.312976837158203,
      "learning_rate": 1.2420000000000001e-05,
      "loss": 1.4618,
      "step": 15040
    },
    {
      "epoch": 7.525,
      "grad_norm": 18.31566619873047,
      "learning_rate": 1.2395e-05,
      "loss": 1.4203,
      "step": 15050
    },
    {
      "epoch": 7.53,
      "grad_norm": 7.687749862670898,
      "learning_rate": 1.2370000000000002e-05,
      "loss": 1.4919,
      "step": 15060
    },
    {
      "epoch": 7.535,
      "grad_norm": 12.020703315734863,
      "learning_rate": 1.2345000000000001e-05,
      "loss": 1.442,
      "step": 15070
    },
    {
      "epoch": 7.54,
      "grad_norm": 7.329344272613525,
      "learning_rate": 1.232e-05,
      "loss": 1.2341,
      "step": 15080
    },
    {
      "epoch": 7.545,
      "grad_norm": 11.340383529663086,
      "learning_rate": 1.2295000000000002e-05,
      "loss": 1.4194,
      "step": 15090
    },
    {
      "epoch": 7.55,
      "grad_norm": 8.573111534118652,
      "learning_rate": 1.2270000000000001e-05,
      "loss": 1.1177,
      "step": 15100
    },
    {
      "epoch": 7.555,
      "grad_norm": 10.893514633178711,
      "learning_rate": 1.2245e-05,
      "loss": 1.2065,
      "step": 15110
    },
    {
      "epoch": 7.5600000000000005,
      "grad_norm": 8.43956184387207,
      "learning_rate": 1.2220000000000002e-05,
      "loss": 1.6744,
      "step": 15120
    },
    {
      "epoch": 7.5649999999999995,
      "grad_norm": 3.9078941345214844,
      "learning_rate": 1.2195000000000001e-05,
      "loss": 1.4222,
      "step": 15130
    },
    {
      "epoch": 7.57,
      "grad_norm": 3.074484348297119,
      "learning_rate": 1.217e-05,
      "loss": 1.2111,
      "step": 15140
    },
    {
      "epoch": 7.575,
      "grad_norm": 11.506542205810547,
      "learning_rate": 1.2145000000000001e-05,
      "loss": 1.3175,
      "step": 15150
    },
    {
      "epoch": 7.58,
      "grad_norm": 13.615071296691895,
      "learning_rate": 1.2120000000000001e-05,
      "loss": 1.5213,
      "step": 15160
    },
    {
      "epoch": 7.585,
      "grad_norm": 15.329399108886719,
      "learning_rate": 1.2095e-05,
      "loss": 1.7147,
      "step": 15170
    },
    {
      "epoch": 7.59,
      "grad_norm": 24.05440330505371,
      "learning_rate": 1.2070000000000001e-05,
      "loss": 1.4415,
      "step": 15180
    },
    {
      "epoch": 7.595,
      "grad_norm": 11.921486854553223,
      "learning_rate": 1.2045e-05,
      "loss": 1.1759,
      "step": 15190
    },
    {
      "epoch": 7.6,
      "grad_norm": 7.0197954177856445,
      "learning_rate": 1.202e-05,
      "loss": 1.6536,
      "step": 15200
    },
    {
      "epoch": 7.605,
      "grad_norm": 16.81943702697754,
      "learning_rate": 1.1995000000000001e-05,
      "loss": 1.3136,
      "step": 15210
    },
    {
      "epoch": 7.61,
      "grad_norm": 10.229065895080566,
      "learning_rate": 1.197e-05,
      "loss": 1.4045,
      "step": 15220
    },
    {
      "epoch": 7.615,
      "grad_norm": 9.66737174987793,
      "learning_rate": 1.1945e-05,
      "loss": 1.6182,
      "step": 15230
    },
    {
      "epoch": 7.62,
      "grad_norm": 16.724775314331055,
      "learning_rate": 1.1920000000000001e-05,
      "loss": 1.2914,
      "step": 15240
    },
    {
      "epoch": 7.625,
      "grad_norm": 12.358342170715332,
      "learning_rate": 1.1895e-05,
      "loss": 1.3559,
      "step": 15250
    },
    {
      "epoch": 7.63,
      "grad_norm": 9.479061126708984,
      "learning_rate": 1.187e-05,
      "loss": 1.1916,
      "step": 15260
    },
    {
      "epoch": 7.635,
      "grad_norm": 8.57143783569336,
      "learning_rate": 1.1845000000000001e-05,
      "loss": 1.419,
      "step": 15270
    },
    {
      "epoch": 7.64,
      "grad_norm": 6.472116470336914,
      "learning_rate": 1.182e-05,
      "loss": 1.3355,
      "step": 15280
    },
    {
      "epoch": 7.645,
      "grad_norm": 12.802021026611328,
      "learning_rate": 1.1795e-05,
      "loss": 1.4757,
      "step": 15290
    },
    {
      "epoch": 7.65,
      "grad_norm": 8.435407638549805,
      "learning_rate": 1.177e-05,
      "loss": 1.3698,
      "step": 15300
    },
    {
      "epoch": 7.655,
      "grad_norm": 16.194549560546875,
      "learning_rate": 1.1745e-05,
      "loss": 1.3009,
      "step": 15310
    },
    {
      "epoch": 7.66,
      "grad_norm": 13.824015617370605,
      "learning_rate": 1.172e-05,
      "loss": 1.2096,
      "step": 15320
    },
    {
      "epoch": 7.665,
      "grad_norm": 11.262365341186523,
      "learning_rate": 1.1695e-05,
      "loss": 1.3248,
      "step": 15330
    },
    {
      "epoch": 7.67,
      "grad_norm": 10.083255767822266,
      "learning_rate": 1.167e-05,
      "loss": 1.4209,
      "step": 15340
    },
    {
      "epoch": 7.675,
      "grad_norm": 10.125337600708008,
      "learning_rate": 1.1645000000000001e-05,
      "loss": 1.6934,
      "step": 15350
    },
    {
      "epoch": 7.68,
      "grad_norm": 8.21068000793457,
      "learning_rate": 1.162e-05,
      "loss": 1.1255,
      "step": 15360
    },
    {
      "epoch": 7.6850000000000005,
      "grad_norm": 9.500419616699219,
      "learning_rate": 1.1595e-05,
      "loss": 1.3894,
      "step": 15370
    },
    {
      "epoch": 7.6899999999999995,
      "grad_norm": 8.483969688415527,
      "learning_rate": 1.1570000000000001e-05,
      "loss": 1.5082,
      "step": 15380
    },
    {
      "epoch": 7.695,
      "grad_norm": 13.8231782913208,
      "learning_rate": 1.1545e-05,
      "loss": 1.5284,
      "step": 15390
    },
    {
      "epoch": 7.7,
      "grad_norm": 10.956887245178223,
      "learning_rate": 1.152e-05,
      "loss": 1.1686,
      "step": 15400
    },
    {
      "epoch": 7.705,
      "grad_norm": 13.190049171447754,
      "learning_rate": 1.1495000000000001e-05,
      "loss": 1.0894,
      "step": 15410
    },
    {
      "epoch": 7.71,
      "grad_norm": 11.257421493530273,
      "learning_rate": 1.147e-05,
      "loss": 1.3578,
      "step": 15420
    },
    {
      "epoch": 7.715,
      "grad_norm": 8.766258239746094,
      "learning_rate": 1.1445e-05,
      "loss": 1.4572,
      "step": 15430
    },
    {
      "epoch": 7.72,
      "grad_norm": 7.664327621459961,
      "learning_rate": 1.142e-05,
      "loss": 1.5646,
      "step": 15440
    },
    {
      "epoch": 7.725,
      "grad_norm": 12.029424667358398,
      "learning_rate": 1.1395e-05,
      "loss": 1.4079,
      "step": 15450
    },
    {
      "epoch": 7.73,
      "grad_norm": 8.802350044250488,
      "learning_rate": 1.137e-05,
      "loss": 1.3315,
      "step": 15460
    },
    {
      "epoch": 7.735,
      "grad_norm": 20.837570190429688,
      "learning_rate": 1.1345e-05,
      "loss": 1.3251,
      "step": 15470
    },
    {
      "epoch": 7.74,
      "grad_norm": 7.8651123046875,
      "learning_rate": 1.132e-05,
      "loss": 1.3157,
      "step": 15480
    },
    {
      "epoch": 7.745,
      "grad_norm": 10.372162818908691,
      "learning_rate": 1.1295e-05,
      "loss": 1.2685,
      "step": 15490
    },
    {
      "epoch": 7.75,
      "grad_norm": 12.166505813598633,
      "learning_rate": 1.127e-05,
      "loss": 1.4019,
      "step": 15500
    },
    {
      "epoch": 7.755,
      "grad_norm": 9.21354866027832,
      "learning_rate": 1.1245e-05,
      "loss": 1.0959,
      "step": 15510
    },
    {
      "epoch": 7.76,
      "grad_norm": 12.625642776489258,
      "learning_rate": 1.122e-05,
      "loss": 1.538,
      "step": 15520
    },
    {
      "epoch": 7.765,
      "grad_norm": 11.1820707321167,
      "learning_rate": 1.1195e-05,
      "loss": 1.3842,
      "step": 15530
    },
    {
      "epoch": 7.77,
      "grad_norm": 8.65934944152832,
      "learning_rate": 1.117e-05,
      "loss": 1.1203,
      "step": 15540
    },
    {
      "epoch": 7.775,
      "grad_norm": 16.448564529418945,
      "learning_rate": 1.1145e-05,
      "loss": 1.5923,
      "step": 15550
    },
    {
      "epoch": 7.78,
      "grad_norm": 8.477446556091309,
      "learning_rate": 1.112e-05,
      "loss": 1.0735,
      "step": 15560
    },
    {
      "epoch": 7.785,
      "grad_norm": 7.276724338531494,
      "learning_rate": 1.1095e-05,
      "loss": 1.3682,
      "step": 15570
    },
    {
      "epoch": 7.79,
      "grad_norm": 14.925875663757324,
      "learning_rate": 1.107e-05,
      "loss": 1.2833,
      "step": 15580
    },
    {
      "epoch": 7.795,
      "grad_norm": 8.512124061584473,
      "learning_rate": 1.1045000000000002e-05,
      "loss": 1.3633,
      "step": 15590
    },
    {
      "epoch": 7.8,
      "grad_norm": 14.004772186279297,
      "learning_rate": 1.1020000000000001e-05,
      "loss": 1.0796,
      "step": 15600
    },
    {
      "epoch": 7.805,
      "grad_norm": 7.570554256439209,
      "learning_rate": 1.0995e-05,
      "loss": 1.4195,
      "step": 15610
    },
    {
      "epoch": 7.8100000000000005,
      "grad_norm": 13.133840560913086,
      "learning_rate": 1.0970000000000002e-05,
      "loss": 1.2603,
      "step": 15620
    },
    {
      "epoch": 7.8149999999999995,
      "grad_norm": 10.22547435760498,
      "learning_rate": 1.0945000000000001e-05,
      "loss": 1.4371,
      "step": 15630
    },
    {
      "epoch": 7.82,
      "grad_norm": 11.059224128723145,
      "learning_rate": 1.092e-05,
      "loss": 1.2288,
      "step": 15640
    },
    {
      "epoch": 7.825,
      "grad_norm": 31.34429931640625,
      "learning_rate": 1.0895000000000002e-05,
      "loss": 1.3266,
      "step": 15650
    },
    {
      "epoch": 7.83,
      "grad_norm": 11.50826358795166,
      "learning_rate": 1.0870000000000001e-05,
      "loss": 1.3837,
      "step": 15660
    },
    {
      "epoch": 7.835,
      "grad_norm": 10.277238845825195,
      "learning_rate": 1.0845e-05,
      "loss": 1.1577,
      "step": 15670
    },
    {
      "epoch": 7.84,
      "grad_norm": 7.195296287536621,
      "learning_rate": 1.0820000000000001e-05,
      "loss": 1.4453,
      "step": 15680
    },
    {
      "epoch": 7.845,
      "grad_norm": 17.8033447265625,
      "learning_rate": 1.0795e-05,
      "loss": 1.3709,
      "step": 15690
    },
    {
      "epoch": 7.85,
      "grad_norm": 8.268622398376465,
      "learning_rate": 1.077e-05,
      "loss": 1.5237,
      "step": 15700
    },
    {
      "epoch": 7.855,
      "grad_norm": 8.377190589904785,
      "learning_rate": 1.0745000000000001e-05,
      "loss": 1.4961,
      "step": 15710
    },
    {
      "epoch": 7.86,
      "grad_norm": 8.897144317626953,
      "learning_rate": 1.072e-05,
      "loss": 1.3009,
      "step": 15720
    },
    {
      "epoch": 7.865,
      "grad_norm": 7.722391128540039,
      "learning_rate": 1.0695e-05,
      "loss": 1.3675,
      "step": 15730
    },
    {
      "epoch": 7.87,
      "grad_norm": 20.76051139831543,
      "learning_rate": 1.0670000000000001e-05,
      "loss": 1.6065,
      "step": 15740
    },
    {
      "epoch": 7.875,
      "grad_norm": 15.344175338745117,
      "learning_rate": 1.0645e-05,
      "loss": 1.2553,
      "step": 15750
    },
    {
      "epoch": 7.88,
      "grad_norm": 15.760662078857422,
      "learning_rate": 1.062e-05,
      "loss": 1.2113,
      "step": 15760
    },
    {
      "epoch": 7.885,
      "grad_norm": 9.674193382263184,
      "learning_rate": 1.0595000000000001e-05,
      "loss": 1.5855,
      "step": 15770
    },
    {
      "epoch": 7.89,
      "grad_norm": 8.009461402893066,
      "learning_rate": 1.057e-05,
      "loss": 1.3226,
      "step": 15780
    },
    {
      "epoch": 7.895,
      "grad_norm": 13.241443634033203,
      "learning_rate": 1.0545000000000002e-05,
      "loss": 1.2399,
      "step": 15790
    },
    {
      "epoch": 7.9,
      "grad_norm": 15.613551139831543,
      "learning_rate": 1.0520000000000001e-05,
      "loss": 1.4799,
      "step": 15800
    },
    {
      "epoch": 7.905,
      "grad_norm": 10.913623809814453,
      "learning_rate": 1.0495e-05,
      "loss": 1.4394,
      "step": 15810
    },
    {
      "epoch": 7.91,
      "grad_norm": 12.14555549621582,
      "learning_rate": 1.0470000000000001e-05,
      "loss": 1.3536,
      "step": 15820
    },
    {
      "epoch": 7.915,
      "grad_norm": 7.795254230499268,
      "learning_rate": 1.0445e-05,
      "loss": 1.4827,
      "step": 15830
    },
    {
      "epoch": 7.92,
      "grad_norm": 13.767722129821777,
      "learning_rate": 1.042e-05,
      "loss": 1.5199,
      "step": 15840
    },
    {
      "epoch": 7.925,
      "grad_norm": 8.03269100189209,
      "learning_rate": 1.0395000000000001e-05,
      "loss": 1.4634,
      "step": 15850
    },
    {
      "epoch": 7.93,
      "grad_norm": 7.9145307540893555,
      "learning_rate": 1.037e-05,
      "loss": 1.4149,
      "step": 15860
    },
    {
      "epoch": 7.9350000000000005,
      "grad_norm": 14.351274490356445,
      "learning_rate": 1.0345e-05,
      "loss": 1.243,
      "step": 15870
    },
    {
      "epoch": 7.9399999999999995,
      "grad_norm": 6.988156318664551,
      "learning_rate": 1.0320000000000001e-05,
      "loss": 1.414,
      "step": 15880
    },
    {
      "epoch": 7.945,
      "grad_norm": 7.250924587249756,
      "learning_rate": 1.0295e-05,
      "loss": 1.5586,
      "step": 15890
    },
    {
      "epoch": 7.95,
      "grad_norm": 9.951979637145996,
      "learning_rate": 1.027e-05,
      "loss": 1.067,
      "step": 15900
    },
    {
      "epoch": 7.955,
      "grad_norm": 19.026390075683594,
      "learning_rate": 1.0245000000000001e-05,
      "loss": 1.3239,
      "step": 15910
    },
    {
      "epoch": 7.96,
      "grad_norm": 17.843915939331055,
      "learning_rate": 1.022e-05,
      "loss": 1.0502,
      "step": 15920
    },
    {
      "epoch": 7.965,
      "grad_norm": 17.519563674926758,
      "learning_rate": 1.0195e-05,
      "loss": 1.2391,
      "step": 15930
    },
    {
      "epoch": 7.97,
      "grad_norm": 18.12908172607422,
      "learning_rate": 1.0170000000000001e-05,
      "loss": 1.5711,
      "step": 15940
    },
    {
      "epoch": 7.975,
      "grad_norm": 10.264111518859863,
      "learning_rate": 1.0145e-05,
      "loss": 1.5663,
      "step": 15950
    },
    {
      "epoch": 7.98,
      "grad_norm": 6.590771675109863,
      "learning_rate": 1.012e-05,
      "loss": 1.2558,
      "step": 15960
    },
    {
      "epoch": 7.985,
      "grad_norm": 7.397881984710693,
      "learning_rate": 1.0095e-05,
      "loss": 0.9225,
      "step": 15970
    },
    {
      "epoch": 7.99,
      "grad_norm": 7.882055282592773,
      "learning_rate": 1.007e-05,
      "loss": 1.2454,
      "step": 15980
    },
    {
      "epoch": 7.995,
      "grad_norm": 9.637895584106445,
      "learning_rate": 1.0045e-05,
      "loss": 1.3584,
      "step": 15990
    },
    {
      "epoch": 8.0,
      "grad_norm": 7.369724750518799,
      "learning_rate": 1.002e-05,
      "loss": 1.2682,
      "step": 16000
    },
    {
      "epoch": 8.005,
      "grad_norm": 8.883990287780762,
      "learning_rate": 9.995e-06,
      "loss": 1.2871,
      "step": 16010
    },
    {
      "epoch": 8.01,
      "grad_norm": 13.335676193237305,
      "learning_rate": 9.97e-06,
      "loss": 1.3454,
      "step": 16020
    },
    {
      "epoch": 8.015,
      "grad_norm": 10.231727600097656,
      "learning_rate": 9.945e-06,
      "loss": 1.3003,
      "step": 16030
    },
    {
      "epoch": 8.02,
      "grad_norm": 18.965028762817383,
      "learning_rate": 9.92e-06,
      "loss": 1.4625,
      "step": 16040
    },
    {
      "epoch": 8.025,
      "grad_norm": 10.776744842529297,
      "learning_rate": 9.895e-06,
      "loss": 1.3295,
      "step": 16050
    },
    {
      "epoch": 8.03,
      "grad_norm": 7.848062992095947,
      "learning_rate": 9.87e-06,
      "loss": 1.3858,
      "step": 16060
    },
    {
      "epoch": 8.035,
      "grad_norm": 13.435563087463379,
      "learning_rate": 9.845e-06,
      "loss": 1.4421,
      "step": 16070
    },
    {
      "epoch": 8.04,
      "grad_norm": 19.414657592773438,
      "learning_rate": 9.820000000000001e-06,
      "loss": 1.5624,
      "step": 16080
    },
    {
      "epoch": 8.045,
      "grad_norm": 10.548855781555176,
      "learning_rate": 9.795e-06,
      "loss": 1.4659,
      "step": 16090
    },
    {
      "epoch": 8.05,
      "grad_norm": 15.164057731628418,
      "learning_rate": 9.77e-06,
      "loss": 1.2645,
      "step": 16100
    },
    {
      "epoch": 8.055,
      "grad_norm": 8.871747016906738,
      "learning_rate": 9.745e-06,
      "loss": 1.5663,
      "step": 16110
    },
    {
      "epoch": 8.06,
      "grad_norm": 10.496175765991211,
      "learning_rate": 9.72e-06,
      "loss": 1.0642,
      "step": 16120
    },
    {
      "epoch": 8.065,
      "grad_norm": 16.57656478881836,
      "learning_rate": 9.695e-06,
      "loss": 1.4442,
      "step": 16130
    },
    {
      "epoch": 8.07,
      "grad_norm": 9.711004257202148,
      "learning_rate": 9.67e-06,
      "loss": 1.2708,
      "step": 16140
    },
    {
      "epoch": 8.075,
      "grad_norm": 8.242280006408691,
      "learning_rate": 9.645e-06,
      "loss": 1.2987,
      "step": 16150
    },
    {
      "epoch": 8.08,
      "grad_norm": 11.177806854248047,
      "learning_rate": 9.62e-06,
      "loss": 1.2819,
      "step": 16160
    },
    {
      "epoch": 8.085,
      "grad_norm": 9.64599609375,
      "learning_rate": 9.595e-06,
      "loss": 1.3526,
      "step": 16170
    },
    {
      "epoch": 8.09,
      "grad_norm": 11.30612564086914,
      "learning_rate": 9.57e-06,
      "loss": 1.3453,
      "step": 16180
    },
    {
      "epoch": 8.095,
      "grad_norm": 16.436349868774414,
      "learning_rate": 9.545e-06,
      "loss": 1.521,
      "step": 16190
    },
    {
      "epoch": 8.1,
      "grad_norm": 19.967308044433594,
      "learning_rate": 9.52e-06,
      "loss": 1.5563,
      "step": 16200
    },
    {
      "epoch": 8.105,
      "grad_norm": 12.72319507598877,
      "learning_rate": 9.495000000000001e-06,
      "loss": 1.6863,
      "step": 16210
    },
    {
      "epoch": 8.11,
      "grad_norm": 13.937365531921387,
      "learning_rate": 9.47e-06,
      "loss": 1.4789,
      "step": 16220
    },
    {
      "epoch": 8.115,
      "grad_norm": 7.7721662521362305,
      "learning_rate": 9.445000000000002e-06,
      "loss": 1.0933,
      "step": 16230
    },
    {
      "epoch": 8.12,
      "grad_norm": 7.147563934326172,
      "learning_rate": 9.420000000000001e-06,
      "loss": 1.2761,
      "step": 16240
    },
    {
      "epoch": 8.125,
      "grad_norm": 4.898922443389893,
      "learning_rate": 9.395e-06,
      "loss": 1.3856,
      "step": 16250
    },
    {
      "epoch": 8.13,
      "grad_norm": 7.758277893066406,
      "learning_rate": 9.370000000000002e-06,
      "loss": 1.4857,
      "step": 16260
    },
    {
      "epoch": 8.135,
      "grad_norm": 8.452637672424316,
      "learning_rate": 9.345000000000001e-06,
      "loss": 1.0157,
      "step": 16270
    },
    {
      "epoch": 8.14,
      "grad_norm": 9.518208503723145,
      "learning_rate": 9.32e-06,
      "loss": 1.0053,
      "step": 16280
    },
    {
      "epoch": 8.145,
      "grad_norm": 8.961414337158203,
      "learning_rate": 9.295000000000002e-06,
      "loss": 1.2135,
      "step": 16290
    },
    {
      "epoch": 8.15,
      "grad_norm": 17.29681396484375,
      "learning_rate": 9.270000000000001e-06,
      "loss": 1.2547,
      "step": 16300
    },
    {
      "epoch": 8.155,
      "grad_norm": 8.873661041259766,
      "learning_rate": 9.245e-06,
      "loss": 1.517,
      "step": 16310
    },
    {
      "epoch": 8.16,
      "grad_norm": 10.60014820098877,
      "learning_rate": 9.220000000000002e-06,
      "loss": 1.4908,
      "step": 16320
    },
    {
      "epoch": 8.165,
      "grad_norm": 8.916640281677246,
      "learning_rate": 9.195000000000001e-06,
      "loss": 1.2584,
      "step": 16330
    },
    {
      "epoch": 8.17,
      "grad_norm": 9.333613395690918,
      "learning_rate": 9.17e-06,
      "loss": 1.2722,
      "step": 16340
    },
    {
      "epoch": 8.175,
      "grad_norm": 8.504388809204102,
      "learning_rate": 9.145000000000001e-06,
      "loss": 1.2124,
      "step": 16350
    },
    {
      "epoch": 8.18,
      "grad_norm": 15.59859561920166,
      "learning_rate": 9.12e-06,
      "loss": 1.537,
      "step": 16360
    },
    {
      "epoch": 8.185,
      "grad_norm": 14.4231538772583,
      "learning_rate": 9.095e-06,
      "loss": 1.3428,
      "step": 16370
    },
    {
      "epoch": 8.19,
      "grad_norm": 11.464383125305176,
      "learning_rate": 9.070000000000001e-06,
      "loss": 1.1089,
      "step": 16380
    },
    {
      "epoch": 8.195,
      "grad_norm": 7.714766025543213,
      "learning_rate": 9.045e-06,
      "loss": 1.5013,
      "step": 16390
    },
    {
      "epoch": 8.2,
      "grad_norm": 11.943524360656738,
      "learning_rate": 9.02e-06,
      "loss": 1.4317,
      "step": 16400
    },
    {
      "epoch": 8.205,
      "grad_norm": 9.552927017211914,
      "learning_rate": 8.995000000000001e-06,
      "loss": 1.2223,
      "step": 16410
    },
    {
      "epoch": 8.21,
      "grad_norm": 6.298645973205566,
      "learning_rate": 8.97e-06,
      "loss": 1.3754,
      "step": 16420
    },
    {
      "epoch": 8.215,
      "grad_norm": 31.134780883789062,
      "learning_rate": 8.945e-06,
      "loss": 1.5873,
      "step": 16430
    },
    {
      "epoch": 8.22,
      "grad_norm": 9.473581314086914,
      "learning_rate": 8.920000000000001e-06,
      "loss": 1.5978,
      "step": 16440
    },
    {
      "epoch": 8.225,
      "grad_norm": 12.13872241973877,
      "learning_rate": 8.895e-06,
      "loss": 1.528,
      "step": 16450
    },
    {
      "epoch": 8.23,
      "grad_norm": 15.506418228149414,
      "learning_rate": 8.87e-06,
      "loss": 1.2821,
      "step": 16460
    },
    {
      "epoch": 8.235,
      "grad_norm": 8.514320373535156,
      "learning_rate": 8.845000000000001e-06,
      "loss": 1.284,
      "step": 16470
    },
    {
      "epoch": 8.24,
      "grad_norm": 13.012846946716309,
      "learning_rate": 8.82e-06,
      "loss": 1.3079,
      "step": 16480
    },
    {
      "epoch": 8.245,
      "grad_norm": 13.629781723022461,
      "learning_rate": 8.795e-06,
      "loss": 1.4482,
      "step": 16490
    },
    {
      "epoch": 8.25,
      "grad_norm": 10.903276443481445,
      "learning_rate": 8.77e-06,
      "loss": 1.2797,
      "step": 16500
    },
    {
      "epoch": 8.255,
      "grad_norm": 9.505387306213379,
      "learning_rate": 8.745e-06,
      "loss": 1.3298,
      "step": 16510
    },
    {
      "epoch": 8.26,
      "grad_norm": 8.853967666625977,
      "learning_rate": 8.720000000000001e-06,
      "loss": 1.0303,
      "step": 16520
    },
    {
      "epoch": 8.265,
      "grad_norm": 12.084053993225098,
      "learning_rate": 8.695e-06,
      "loss": 1.0419,
      "step": 16530
    },
    {
      "epoch": 8.27,
      "grad_norm": 11.077902793884277,
      "learning_rate": 8.67e-06,
      "loss": 1.3161,
      "step": 16540
    },
    {
      "epoch": 8.275,
      "grad_norm": 15.116937637329102,
      "learning_rate": 8.645000000000001e-06,
      "loss": 1.2904,
      "step": 16550
    },
    {
      "epoch": 8.28,
      "grad_norm": 15.259364128112793,
      "learning_rate": 8.62e-06,
      "loss": 1.2007,
      "step": 16560
    },
    {
      "epoch": 8.285,
      "grad_norm": 11.045249938964844,
      "learning_rate": 8.595e-06,
      "loss": 1.3686,
      "step": 16570
    },
    {
      "epoch": 8.29,
      "grad_norm": 8.537599563598633,
      "learning_rate": 8.570000000000001e-06,
      "loss": 1.2687,
      "step": 16580
    },
    {
      "epoch": 8.295,
      "grad_norm": 11.251130104064941,
      "learning_rate": 8.545e-06,
      "loss": 1.2881,
      "step": 16590
    },
    {
      "epoch": 8.3,
      "grad_norm": 22.117218017578125,
      "learning_rate": 8.52e-06,
      "loss": 1.2717,
      "step": 16600
    },
    {
      "epoch": 8.305,
      "grad_norm": 10.401545524597168,
      "learning_rate": 8.495e-06,
      "loss": 1.2712,
      "step": 16610
    },
    {
      "epoch": 8.31,
      "grad_norm": 21.68651008605957,
      "learning_rate": 8.47e-06,
      "loss": 1.3876,
      "step": 16620
    },
    {
      "epoch": 8.315,
      "grad_norm": 11.110724449157715,
      "learning_rate": 8.445e-06,
      "loss": 1.3228,
      "step": 16630
    },
    {
      "epoch": 8.32,
      "grad_norm": 13.820343017578125,
      "learning_rate": 8.42e-06,
      "loss": 1.3081,
      "step": 16640
    },
    {
      "epoch": 8.325,
      "grad_norm": 11.620357513427734,
      "learning_rate": 8.395e-06,
      "loss": 1.5606,
      "step": 16650
    },
    {
      "epoch": 8.33,
      "grad_norm": 10.854544639587402,
      "learning_rate": 8.37e-06,
      "loss": 1.2712,
      "step": 16660
    },
    {
      "epoch": 8.335,
      "grad_norm": 7.225449562072754,
      "learning_rate": 8.345e-06,
      "loss": 1.6083,
      "step": 16670
    },
    {
      "epoch": 8.34,
      "grad_norm": 19.190458297729492,
      "learning_rate": 8.32e-06,
      "loss": 1.2323,
      "step": 16680
    },
    {
      "epoch": 8.345,
      "grad_norm": 9.746294021606445,
      "learning_rate": 8.295e-06,
      "loss": 1.4279,
      "step": 16690
    },
    {
      "epoch": 8.35,
      "grad_norm": 9.687347412109375,
      "learning_rate": 8.27e-06,
      "loss": 1.2535,
      "step": 16700
    },
    {
      "epoch": 8.355,
      "grad_norm": 10.160788536071777,
      "learning_rate": 8.245e-06,
      "loss": 1.5352,
      "step": 16710
    },
    {
      "epoch": 8.36,
      "grad_norm": 14.408690452575684,
      "learning_rate": 8.22e-06,
      "loss": 1.3525,
      "step": 16720
    },
    {
      "epoch": 8.365,
      "grad_norm": 10.56328010559082,
      "learning_rate": 8.195e-06,
      "loss": 1.5351,
      "step": 16730
    },
    {
      "epoch": 8.37,
      "grad_norm": 9.078725814819336,
      "learning_rate": 8.17e-06,
      "loss": 0.8562,
      "step": 16740
    },
    {
      "epoch": 8.375,
      "grad_norm": 10.214818954467773,
      "learning_rate": 8.144999999999999e-06,
      "loss": 1.4842,
      "step": 16750
    },
    {
      "epoch": 8.38,
      "grad_norm": 9.330488204956055,
      "learning_rate": 8.12e-06,
      "loss": 1.2302,
      "step": 16760
    },
    {
      "epoch": 8.385,
      "grad_norm": 12.681753158569336,
      "learning_rate": 8.095e-06,
      "loss": 1.0337,
      "step": 16770
    },
    {
      "epoch": 8.39,
      "grad_norm": 8.375150680541992,
      "learning_rate": 8.069999999999999e-06,
      "loss": 1.3366,
      "step": 16780
    },
    {
      "epoch": 8.395,
      "grad_norm": 2.717820882797241,
      "learning_rate": 8.045e-06,
      "loss": 1.2381,
      "step": 16790
    },
    {
      "epoch": 8.4,
      "grad_norm": 9.639850616455078,
      "learning_rate": 8.02e-06,
      "loss": 1.5096,
      "step": 16800
    },
    {
      "epoch": 8.405,
      "grad_norm": 24.219173431396484,
      "learning_rate": 7.995e-06,
      "loss": 1.2833,
      "step": 16810
    },
    {
      "epoch": 8.41,
      "grad_norm": 14.702183723449707,
      "learning_rate": 7.97e-06,
      "loss": 1.0929,
      "step": 16820
    },
    {
      "epoch": 8.415,
      "grad_norm": 15.762906074523926,
      "learning_rate": 7.945000000000001e-06,
      "loss": 1.2986,
      "step": 16830
    },
    {
      "epoch": 8.42,
      "grad_norm": 11.452576637268066,
      "learning_rate": 7.92e-06,
      "loss": 1.675,
      "step": 16840
    },
    {
      "epoch": 8.425,
      "grad_norm": 12.601475715637207,
      "learning_rate": 7.895000000000001e-06,
      "loss": 1.4006,
      "step": 16850
    },
    {
      "epoch": 8.43,
      "grad_norm": 17.595195770263672,
      "learning_rate": 7.870000000000001e-06,
      "loss": 1.4671,
      "step": 16860
    },
    {
      "epoch": 8.435,
      "grad_norm": 18.623640060424805,
      "learning_rate": 7.845e-06,
      "loss": 1.3736,
      "step": 16870
    },
    {
      "epoch": 8.44,
      "grad_norm": 9.157865524291992,
      "learning_rate": 7.820000000000001e-06,
      "loss": 1.2757,
      "step": 16880
    },
    {
      "epoch": 8.445,
      "grad_norm": 9.5056734085083,
      "learning_rate": 7.795e-06,
      "loss": 1.4162,
      "step": 16890
    },
    {
      "epoch": 8.45,
      "grad_norm": 10.658425331115723,
      "learning_rate": 7.77e-06,
      "loss": 1.5474,
      "step": 16900
    },
    {
      "epoch": 8.455,
      "grad_norm": 12.226984024047852,
      "learning_rate": 7.745000000000001e-06,
      "loss": 1.2092,
      "step": 16910
    },
    {
      "epoch": 8.46,
      "grad_norm": 9.68529987335205,
      "learning_rate": 7.72e-06,
      "loss": 0.941,
      "step": 16920
    },
    {
      "epoch": 8.465,
      "grad_norm": 10.631078720092773,
      "learning_rate": 7.695e-06,
      "loss": 1.263,
      "step": 16930
    },
    {
      "epoch": 8.47,
      "grad_norm": 9.271919250488281,
      "learning_rate": 7.670000000000001e-06,
      "loss": 1.4784,
      "step": 16940
    },
    {
      "epoch": 8.475,
      "grad_norm": 17.109750747680664,
      "learning_rate": 7.645e-06,
      "loss": 1.5764,
      "step": 16950
    },
    {
      "epoch": 8.48,
      "grad_norm": 13.898713111877441,
      "learning_rate": 7.620000000000001e-06,
      "loss": 1.343,
      "step": 16960
    },
    {
      "epoch": 8.485,
      "grad_norm": 8.430032730102539,
      "learning_rate": 7.595000000000001e-06,
      "loss": 1.448,
      "step": 16970
    },
    {
      "epoch": 8.49,
      "grad_norm": 6.935168743133545,
      "learning_rate": 7.57e-06,
      "loss": 1.0975,
      "step": 16980
    },
    {
      "epoch": 8.495,
      "grad_norm": 10.362820625305176,
      "learning_rate": 7.545000000000001e-06,
      "loss": 1.3157,
      "step": 16990
    },
    {
      "epoch": 8.5,
      "grad_norm": 10.645501136779785,
      "learning_rate": 7.520000000000001e-06,
      "loss": 1.4367,
      "step": 17000
    },
    {
      "epoch": 8.505,
      "grad_norm": 11.49375057220459,
      "learning_rate": 7.495e-06,
      "loss": 1.256,
      "step": 17010
    },
    {
      "epoch": 8.51,
      "grad_norm": 18.19539451599121,
      "learning_rate": 7.4700000000000005e-06,
      "loss": 1.7058,
      "step": 17020
    },
    {
      "epoch": 8.515,
      "grad_norm": 9.895668029785156,
      "learning_rate": 7.445000000000001e-06,
      "loss": 0.9986,
      "step": 17030
    },
    {
      "epoch": 8.52,
      "grad_norm": 15.231727600097656,
      "learning_rate": 7.420000000000001e-06,
      "loss": 1.3156,
      "step": 17040
    },
    {
      "epoch": 8.525,
      "grad_norm": 8.230504035949707,
      "learning_rate": 7.395e-06,
      "loss": 1.5804,
      "step": 17050
    },
    {
      "epoch": 8.53,
      "grad_norm": 10.09670352935791,
      "learning_rate": 7.370000000000001e-06,
      "loss": 1.4656,
      "step": 17060
    },
    {
      "epoch": 8.535,
      "grad_norm": 12.941852569580078,
      "learning_rate": 7.345000000000001e-06,
      "loss": 1.3464,
      "step": 17070
    },
    {
      "epoch": 8.54,
      "grad_norm": 12.701456069946289,
      "learning_rate": 7.32e-06,
      "loss": 1.7625,
      "step": 17080
    },
    {
      "epoch": 8.545,
      "grad_norm": 12.9408597946167,
      "learning_rate": 7.2950000000000005e-06,
      "loss": 0.9861,
      "step": 17090
    },
    {
      "epoch": 8.55,
      "grad_norm": 3.3485138416290283,
      "learning_rate": 7.270000000000001e-06,
      "loss": 1.4153,
      "step": 17100
    },
    {
      "epoch": 8.555,
      "grad_norm": 14.963432312011719,
      "learning_rate": 7.245e-06,
      "loss": 1.3518,
      "step": 17110
    },
    {
      "epoch": 8.56,
      "grad_norm": 9.291206359863281,
      "learning_rate": 7.22e-06,
      "loss": 1.3019,
      "step": 17120
    },
    {
      "epoch": 8.565,
      "grad_norm": 9.788750648498535,
      "learning_rate": 7.1950000000000006e-06,
      "loss": 1.163,
      "step": 17130
    },
    {
      "epoch": 8.57,
      "grad_norm": 12.421220779418945,
      "learning_rate": 7.17e-06,
      "loss": 1.2672,
      "step": 17140
    },
    {
      "epoch": 8.575,
      "grad_norm": 18.117530822753906,
      "learning_rate": 7.145e-06,
      "loss": 1.3113,
      "step": 17150
    },
    {
      "epoch": 8.58,
      "grad_norm": 10.35178279876709,
      "learning_rate": 7.1200000000000004e-06,
      "loss": 1.1654,
      "step": 17160
    },
    {
      "epoch": 8.585,
      "grad_norm": 20.76406478881836,
      "learning_rate": 7.095000000000001e-06,
      "loss": 1.4138,
      "step": 17170
    },
    {
      "epoch": 8.59,
      "grad_norm": 6.972121238708496,
      "learning_rate": 7.07e-06,
      "loss": 1.2623,
      "step": 17180
    },
    {
      "epoch": 8.595,
      "grad_norm": 16.87966537475586,
      "learning_rate": 7.045e-06,
      "loss": 1.5694,
      "step": 17190
    },
    {
      "epoch": 8.6,
      "grad_norm": 9.0916748046875,
      "learning_rate": 7.0200000000000006e-06,
      "loss": 1.2739,
      "step": 17200
    },
    {
      "epoch": 8.605,
      "grad_norm": 11.792902946472168,
      "learning_rate": 6.995e-06,
      "loss": 1.5436,
      "step": 17210
    },
    {
      "epoch": 8.61,
      "grad_norm": 18.710689544677734,
      "learning_rate": 6.97e-06,
      "loss": 1.5373,
      "step": 17220
    },
    {
      "epoch": 8.615,
      "grad_norm": 12.220490455627441,
      "learning_rate": 6.945e-06,
      "loss": 1.3206,
      "step": 17230
    },
    {
      "epoch": 8.62,
      "grad_norm": 18.009916305541992,
      "learning_rate": 6.92e-06,
      "loss": 1.3508,
      "step": 17240
    },
    {
      "epoch": 8.625,
      "grad_norm": 9.448031425476074,
      "learning_rate": 6.895e-06,
      "loss": 1.3037,
      "step": 17250
    },
    {
      "epoch": 8.63,
      "grad_norm": 17.198848724365234,
      "learning_rate": 6.87e-06,
      "loss": 1.5506,
      "step": 17260
    },
    {
      "epoch": 8.635,
      "grad_norm": 16.24267578125,
      "learning_rate": 6.845e-06,
      "loss": 1.2879,
      "step": 17270
    },
    {
      "epoch": 8.64,
      "grad_norm": 14.111296653747559,
      "learning_rate": 6.82e-06,
      "loss": 1.5433,
      "step": 17280
    },
    {
      "epoch": 8.645,
      "grad_norm": 12.783778190612793,
      "learning_rate": 6.795e-06,
      "loss": 1.4869,
      "step": 17290
    },
    {
      "epoch": 8.65,
      "grad_norm": 10.745162963867188,
      "learning_rate": 6.7699999999999996e-06,
      "loss": 1.4441,
      "step": 17300
    },
    {
      "epoch": 8.655,
      "grad_norm": 12.14281940460205,
      "learning_rate": 6.745e-06,
      "loss": 1.2719,
      "step": 17310
    },
    {
      "epoch": 8.66,
      "grad_norm": 11.513909339904785,
      "learning_rate": 6.72e-06,
      "loss": 1.2354,
      "step": 17320
    },
    {
      "epoch": 8.665,
      "grad_norm": 8.981812477111816,
      "learning_rate": 6.695e-06,
      "loss": 1.1533,
      "step": 17330
    },
    {
      "epoch": 8.67,
      "grad_norm": 10.743597984313965,
      "learning_rate": 6.67e-06,
      "loss": 1.3374,
      "step": 17340
    },
    {
      "epoch": 8.675,
      "grad_norm": 11.948805809020996,
      "learning_rate": 6.645e-06,
      "loss": 1.0971,
      "step": 17350
    },
    {
      "epoch": 8.68,
      "grad_norm": 15.618749618530273,
      "learning_rate": 6.62e-06,
      "loss": 1.2622,
      "step": 17360
    },
    {
      "epoch": 8.685,
      "grad_norm": 9.30190372467041,
      "learning_rate": 6.5949999999999995e-06,
      "loss": 1.4933,
      "step": 17370
    },
    {
      "epoch": 8.69,
      "grad_norm": 8.580385208129883,
      "learning_rate": 6.57e-06,
      "loss": 1.4147,
      "step": 17380
    },
    {
      "epoch": 8.695,
      "grad_norm": 9.129956245422363,
      "learning_rate": 6.545e-06,
      "loss": 1.2607,
      "step": 17390
    },
    {
      "epoch": 8.7,
      "grad_norm": 10.204818725585938,
      "learning_rate": 6.519999999999999e-06,
      "loss": 1.5163,
      "step": 17400
    },
    {
      "epoch": 8.705,
      "grad_norm": 8.225704193115234,
      "learning_rate": 6.495e-06,
      "loss": 1.4953,
      "step": 17410
    },
    {
      "epoch": 8.71,
      "grad_norm": 7.742677211761475,
      "learning_rate": 6.47e-06,
      "loss": 1.3787,
      "step": 17420
    },
    {
      "epoch": 8.715,
      "grad_norm": 11.52987003326416,
      "learning_rate": 6.444999999999999e-06,
      "loss": 1.1015,
      "step": 17430
    },
    {
      "epoch": 8.72,
      "grad_norm": 8.55860424041748,
      "learning_rate": 6.4199999999999995e-06,
      "loss": 1.3862,
      "step": 17440
    },
    {
      "epoch": 8.725,
      "grad_norm": 8.60275936126709,
      "learning_rate": 6.395000000000001e-06,
      "loss": 0.9904,
      "step": 17450
    },
    {
      "epoch": 8.73,
      "grad_norm": 6.116757392883301,
      "learning_rate": 6.370000000000001e-06,
      "loss": 1.0778,
      "step": 17460
    },
    {
      "epoch": 8.735,
      "grad_norm": 10.108495712280273,
      "learning_rate": 6.345000000000001e-06,
      "loss": 1.5749,
      "step": 17470
    },
    {
      "epoch": 8.74,
      "grad_norm": 12.191802024841309,
      "learning_rate": 6.320000000000001e-06,
      "loss": 1.2508,
      "step": 17480
    },
    {
      "epoch": 8.745,
      "grad_norm": 12.644144058227539,
      "learning_rate": 6.295000000000001e-06,
      "loss": 1.2592,
      "step": 17490
    },
    {
      "epoch": 8.75,
      "grad_norm": 10.400503158569336,
      "learning_rate": 6.270000000000001e-06,
      "loss": 1.3478,
      "step": 17500
    },
    {
      "epoch": 8.755,
      "grad_norm": 8.363035202026367,
      "learning_rate": 6.245e-06,
      "loss": 1.3424,
      "step": 17510
    },
    {
      "epoch": 8.76,
      "grad_norm": 8.895122528076172,
      "learning_rate": 6.22e-06,
      "loss": 1.3561,
      "step": 17520
    },
    {
      "epoch": 8.765,
      "grad_norm": 13.417889595031738,
      "learning_rate": 6.195e-06,
      "loss": 1.4368,
      "step": 17530
    },
    {
      "epoch": 8.77,
      "grad_norm": 11.187039375305176,
      "learning_rate": 6.17e-06,
      "loss": 1.2771,
      "step": 17540
    },
    {
      "epoch": 8.775,
      "grad_norm": 11.801631927490234,
      "learning_rate": 6.1450000000000005e-06,
      "loss": 1.3251,
      "step": 17550
    },
    {
      "epoch": 8.78,
      "grad_norm": 12.24074935913086,
      "learning_rate": 6.12e-06,
      "loss": 1.3135,
      "step": 17560
    },
    {
      "epoch": 8.785,
      "grad_norm": 12.182608604431152,
      "learning_rate": 6.095e-06,
      "loss": 1.3019,
      "step": 17570
    },
    {
      "epoch": 8.79,
      "grad_norm": 10.299640655517578,
      "learning_rate": 6.07e-06,
      "loss": 1.3132,
      "step": 17580
    },
    {
      "epoch": 8.795,
      "grad_norm": 10.428474426269531,
      "learning_rate": 6.045e-06,
      "loss": 1.4026,
      "step": 17590
    },
    {
      "epoch": 8.8,
      "grad_norm": 10.160880088806152,
      "learning_rate": 6.02e-06,
      "loss": 1.5912,
      "step": 17600
    },
    {
      "epoch": 8.805,
      "grad_norm": 12.839667320251465,
      "learning_rate": 5.995e-06,
      "loss": 1.195,
      "step": 17610
    },
    {
      "epoch": 8.81,
      "grad_norm": 11.540071487426758,
      "learning_rate": 5.9700000000000004e-06,
      "loss": 1.2465,
      "step": 17620
    },
    {
      "epoch": 8.815,
      "grad_norm": 9.232870101928711,
      "learning_rate": 5.945000000000001e-06,
      "loss": 1.3891,
      "step": 17630
    },
    {
      "epoch": 8.82,
      "grad_norm": 8.542784690856934,
      "learning_rate": 5.920000000000001e-06,
      "loss": 1.496,
      "step": 17640
    },
    {
      "epoch": 8.825,
      "grad_norm": 10.309331893920898,
      "learning_rate": 5.895e-06,
      "loss": 1.1923,
      "step": 17650
    },
    {
      "epoch": 8.83,
      "grad_norm": 12.318666458129883,
      "learning_rate": 5.8725e-06,
      "loss": 1.6372,
      "step": 17660
    },
    {
      "epoch": 8.835,
      "grad_norm": 12.092958450317383,
      "learning_rate": 5.8475e-06,
      "loss": 1.4311,
      "step": 17670
    },
    {
      "epoch": 8.84,
      "grad_norm": 8.510946273803711,
      "learning_rate": 5.822500000000001e-06,
      "loss": 1.2601,
      "step": 17680
    },
    {
      "epoch": 8.845,
      "grad_norm": 9.97537612915039,
      "learning_rate": 5.7975e-06,
      "loss": 1.7229,
      "step": 17690
    },
    {
      "epoch": 8.85,
      "grad_norm": 9.700827598571777,
      "learning_rate": 5.7725e-06,
      "loss": 1.5308,
      "step": 17700
    },
    {
      "epoch": 8.855,
      "grad_norm": 11.556681632995605,
      "learning_rate": 5.7475000000000005e-06,
      "loss": 1.3143,
      "step": 17710
    },
    {
      "epoch": 8.86,
      "grad_norm": 9.228693962097168,
      "learning_rate": 5.7225e-06,
      "loss": 1.3791,
      "step": 17720
    },
    {
      "epoch": 8.865,
      "grad_norm": 10.160850524902344,
      "learning_rate": 5.6975e-06,
      "loss": 1.2019,
      "step": 17730
    },
    {
      "epoch": 8.87,
      "grad_norm": 19.332420349121094,
      "learning_rate": 5.6725e-06,
      "loss": 1.4337,
      "step": 17740
    },
    {
      "epoch": 8.875,
      "grad_norm": 7.758626461029053,
      "learning_rate": 5.6475e-06,
      "loss": 1.4445,
      "step": 17750
    },
    {
      "epoch": 8.88,
      "grad_norm": 11.448282241821289,
      "learning_rate": 5.6225e-06,
      "loss": 1.549,
      "step": 17760
    },
    {
      "epoch": 8.885,
      "grad_norm": 10.790922164916992,
      "learning_rate": 5.5975e-06,
      "loss": 1.1152,
      "step": 17770
    },
    {
      "epoch": 8.89,
      "grad_norm": 13.6246919631958,
      "learning_rate": 5.5725e-06,
      "loss": 1.3776,
      "step": 17780
    },
    {
      "epoch": 8.895,
      "grad_norm": 23.387008666992188,
      "learning_rate": 5.5475e-06,
      "loss": 1.2618,
      "step": 17790
    },
    {
      "epoch": 8.9,
      "grad_norm": 10.890978813171387,
      "learning_rate": 5.522500000000001e-06,
      "loss": 1.4374,
      "step": 17800
    },
    {
      "epoch": 8.905,
      "grad_norm": 15.529552459716797,
      "learning_rate": 5.4975e-06,
      "loss": 1.2981,
      "step": 17810
    },
    {
      "epoch": 8.91,
      "grad_norm": 16.32754135131836,
      "learning_rate": 5.4725000000000005e-06,
      "loss": 0.9575,
      "step": 17820
    },
    {
      "epoch": 8.915,
      "grad_norm": 11.748148918151855,
      "learning_rate": 5.447500000000001e-06,
      "loss": 1.5628,
      "step": 17830
    },
    {
      "epoch": 8.92,
      "grad_norm": 14.310422897338867,
      "learning_rate": 5.4225e-06,
      "loss": 1.6383,
      "step": 17840
    },
    {
      "epoch": 8.925,
      "grad_norm": 11.658122062683105,
      "learning_rate": 5.3975e-06,
      "loss": 1.2372,
      "step": 17850
    },
    {
      "epoch": 8.93,
      "grad_norm": 11.69891357421875,
      "learning_rate": 5.372500000000001e-06,
      "loss": 1.2712,
      "step": 17860
    },
    {
      "epoch": 8.935,
      "grad_norm": 16.955738067626953,
      "learning_rate": 5.3475e-06,
      "loss": 1.7073,
      "step": 17870
    },
    {
      "epoch": 8.94,
      "grad_norm": 12.464007377624512,
      "learning_rate": 5.3225e-06,
      "loss": 1.5474,
      "step": 17880
    },
    {
      "epoch": 8.945,
      "grad_norm": 8.280304908752441,
      "learning_rate": 5.2975000000000005e-06,
      "loss": 1.2715,
      "step": 17890
    },
    {
      "epoch": 8.95,
      "grad_norm": 12.128174781799316,
      "learning_rate": 5.272500000000001e-06,
      "loss": 1.4735,
      "step": 17900
    },
    {
      "epoch": 8.955,
      "grad_norm": 10.291014671325684,
      "learning_rate": 5.2475e-06,
      "loss": 1.2878,
      "step": 17910
    },
    {
      "epoch": 8.96,
      "grad_norm": 7.915862083435059,
      "learning_rate": 5.2225e-06,
      "loss": 1.4164,
      "step": 17920
    },
    {
      "epoch": 8.965,
      "grad_norm": 13.760589599609375,
      "learning_rate": 5.197500000000001e-06,
      "loss": 1.1695,
      "step": 17930
    },
    {
      "epoch": 8.97,
      "grad_norm": 13.50061321258545,
      "learning_rate": 5.1725e-06,
      "loss": 1.3438,
      "step": 17940
    },
    {
      "epoch": 8.975,
      "grad_norm": 13.84926986694336,
      "learning_rate": 5.1475e-06,
      "loss": 1.1079,
      "step": 17950
    },
    {
      "epoch": 8.98,
      "grad_norm": 16.21271514892578,
      "learning_rate": 5.1225000000000005e-06,
      "loss": 1.3512,
      "step": 17960
    },
    {
      "epoch": 8.985,
      "grad_norm": 9.702455520629883,
      "learning_rate": 5.0975e-06,
      "loss": 1.4729,
      "step": 17970
    },
    {
      "epoch": 8.99,
      "grad_norm": 9.00101375579834,
      "learning_rate": 5.0725e-06,
      "loss": 1.4828,
      "step": 17980
    },
    {
      "epoch": 8.995,
      "grad_norm": 11.527958869934082,
      "learning_rate": 5.0475e-06,
      "loss": 1.4268,
      "step": 17990
    },
    {
      "epoch": 9.0,
      "grad_norm": 10.174894332885742,
      "learning_rate": 5.0225e-06,
      "loss": 1.1914,
      "step": 18000
    },
    {
      "epoch": 9.005,
      "grad_norm": 15.600160598754883,
      "learning_rate": 4.9975e-06,
      "loss": 1.3253,
      "step": 18010
    },
    {
      "epoch": 9.01,
      "grad_norm": 8.752520561218262,
      "learning_rate": 4.9725e-06,
      "loss": 1.5221,
      "step": 18020
    },
    {
      "epoch": 9.015,
      "grad_norm": 8.033248901367188,
      "learning_rate": 4.9475e-06,
      "loss": 1.2269,
      "step": 18030
    },
    {
      "epoch": 9.02,
      "grad_norm": 13.569689750671387,
      "learning_rate": 4.9225e-06,
      "loss": 1.1654,
      "step": 18040
    },
    {
      "epoch": 9.025,
      "grad_norm": 15.58932113647461,
      "learning_rate": 4.8975e-06,
      "loss": 1.5627,
      "step": 18050
    },
    {
      "epoch": 9.03,
      "grad_norm": 8.084019660949707,
      "learning_rate": 4.8725e-06,
      "loss": 1.4977,
      "step": 18060
    },
    {
      "epoch": 9.035,
      "grad_norm": 8.494126319885254,
      "learning_rate": 4.8475e-06,
      "loss": 1.0537,
      "step": 18070
    },
    {
      "epoch": 9.04,
      "grad_norm": 9.684860229492188,
      "learning_rate": 4.8225e-06,
      "loss": 1.3821,
      "step": 18080
    },
    {
      "epoch": 9.045,
      "grad_norm": 11.935186386108398,
      "learning_rate": 4.7975e-06,
      "loss": 1.5572,
      "step": 18090
    },
    {
      "epoch": 9.05,
      "grad_norm": 10.730171203613281,
      "learning_rate": 4.7725e-06,
      "loss": 1.7004,
      "step": 18100
    },
    {
      "epoch": 9.055,
      "grad_norm": 7.121708393096924,
      "learning_rate": 4.747500000000001e-06,
      "loss": 1.0502,
      "step": 18110
    },
    {
      "epoch": 9.06,
      "grad_norm": 33.30253601074219,
      "learning_rate": 4.722500000000001e-06,
      "loss": 1.1853,
      "step": 18120
    },
    {
      "epoch": 9.065,
      "grad_norm": 9.178088188171387,
      "learning_rate": 4.6975e-06,
      "loss": 1.3477,
      "step": 18130
    },
    {
      "epoch": 9.07,
      "grad_norm": 8.388293266296387,
      "learning_rate": 4.672500000000001e-06,
      "loss": 1.4824,
      "step": 18140
    },
    {
      "epoch": 9.075,
      "grad_norm": 9.920723915100098,
      "learning_rate": 4.65e-06,
      "loss": 1.3192,
      "step": 18150
    },
    {
      "epoch": 9.08,
      "grad_norm": 8.621161460876465,
      "learning_rate": 4.625e-06,
      "loss": 1.183,
      "step": 18160
    },
    {
      "epoch": 9.085,
      "grad_norm": 13.836723327636719,
      "learning_rate": 4.6e-06,
      "loss": 1.4029,
      "step": 18170
    },
    {
      "epoch": 9.09,
      "grad_norm": 12.0051908493042,
      "learning_rate": 4.575e-06,
      "loss": 1.3814,
      "step": 18180
    },
    {
      "epoch": 9.095,
      "grad_norm": 12.128058433532715,
      "learning_rate": 4.5500000000000005e-06,
      "loss": 1.2288,
      "step": 18190
    },
    {
      "epoch": 9.1,
      "grad_norm": 18.42339324951172,
      "learning_rate": 4.525e-06,
      "loss": 1.5411,
      "step": 18200
    },
    {
      "epoch": 9.105,
      "grad_norm": 15.491430282592773,
      "learning_rate": 4.5e-06,
      "loss": 1.3903,
      "step": 18210
    },
    {
      "epoch": 9.11,
      "grad_norm": 13.355429649353027,
      "learning_rate": 4.475e-06,
      "loss": 1.0679,
      "step": 18220
    },
    {
      "epoch": 9.115,
      "grad_norm": 9.97132682800293,
      "learning_rate": 4.45e-06,
      "loss": 1.5485,
      "step": 18230
    },
    {
      "epoch": 9.12,
      "grad_norm": 9.138700485229492,
      "learning_rate": 4.425e-06,
      "loss": 1.3824,
      "step": 18240
    },
    {
      "epoch": 9.125,
      "grad_norm": 7.77468729019165,
      "learning_rate": 4.4e-06,
      "loss": 1.5561,
      "step": 18250
    },
    {
      "epoch": 9.13,
      "grad_norm": 11.503454208374023,
      "learning_rate": 4.375e-06,
      "loss": 1.2931,
      "step": 18260
    },
    {
      "epoch": 9.135,
      "grad_norm": 16.05057716369629,
      "learning_rate": 4.35e-06,
      "loss": 0.8722,
      "step": 18270
    },
    {
      "epoch": 9.14,
      "grad_norm": 7.8831024169921875,
      "learning_rate": 4.325e-06,
      "loss": 1.3962,
      "step": 18280
    },
    {
      "epoch": 9.145,
      "grad_norm": 10.53811264038086,
      "learning_rate": 4.2999999999999995e-06,
      "loss": 1.3633,
      "step": 18290
    },
    {
      "epoch": 9.15,
      "grad_norm": 7.6166157722473145,
      "learning_rate": 4.2750000000000006e-06,
      "loss": 1.5806,
      "step": 18300
    },
    {
      "epoch": 9.155,
      "grad_norm": 7.173612594604492,
      "learning_rate": 4.250000000000001e-06,
      "loss": 1.3162,
      "step": 18310
    },
    {
      "epoch": 9.16,
      "grad_norm": 10.66296672821045,
      "learning_rate": 4.225e-06,
      "loss": 1.2956,
      "step": 18320
    },
    {
      "epoch": 9.165,
      "grad_norm": 8.890759468078613,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 1.3522,
      "step": 18330
    },
    {
      "epoch": 9.17,
      "grad_norm": 15.429217338562012,
      "learning_rate": 4.175000000000001e-06,
      "loss": 1.2713,
      "step": 18340
    },
    {
      "epoch": 9.175,
      "grad_norm": 26.292699813842773,
      "learning_rate": 4.15e-06,
      "loss": 1.2358,
      "step": 18350
    },
    {
      "epoch": 9.18,
      "grad_norm": 12.13711929321289,
      "learning_rate": 4.125e-06,
      "loss": 1.5073,
      "step": 18360
    },
    {
      "epoch": 9.185,
      "grad_norm": 10.258399963378906,
      "learning_rate": 4.1000000000000006e-06,
      "loss": 1.1277,
      "step": 18370
    },
    {
      "epoch": 9.19,
      "grad_norm": 13.359496116638184,
      "learning_rate": 4.075e-06,
      "loss": 1.1567,
      "step": 18380
    },
    {
      "epoch": 9.195,
      "grad_norm": 11.632502555847168,
      "learning_rate": 4.05e-06,
      "loss": 1.3733,
      "step": 18390
    },
    {
      "epoch": 9.2,
      "grad_norm": 11.364365577697754,
      "learning_rate": 4.0250000000000004e-06,
      "loss": 1.3139,
      "step": 18400
    },
    {
      "epoch": 9.205,
      "grad_norm": 16.172279357910156,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.3714,
      "step": 18410
    },
    {
      "epoch": 9.21,
      "grad_norm": 12.192036628723145,
      "learning_rate": 3.975e-06,
      "loss": 1.66,
      "step": 18420
    },
    {
      "epoch": 9.215,
      "grad_norm": 8.131698608398438,
      "learning_rate": 3.95e-06,
      "loss": 1.102,
      "step": 18430
    },
    {
      "epoch": 9.22,
      "grad_norm": 9.027885437011719,
      "learning_rate": 3.9250000000000005e-06,
      "loss": 1.3645,
      "step": 18440
    },
    {
      "epoch": 9.225,
      "grad_norm": 11.83421516418457,
      "learning_rate": 3.9e-06,
      "loss": 1.4761,
      "step": 18450
    },
    {
      "epoch": 9.23,
      "grad_norm": 10.245942115783691,
      "learning_rate": 3.875e-06,
      "loss": 1.2586,
      "step": 18460
    },
    {
      "epoch": 9.235,
      "grad_norm": 11.357202529907227,
      "learning_rate": 3.85e-06,
      "loss": 1.5537,
      "step": 18470
    },
    {
      "epoch": 9.24,
      "grad_norm": 18.45097541809082,
      "learning_rate": 3.825e-06,
      "loss": 1.2489,
      "step": 18480
    },
    {
      "epoch": 9.245,
      "grad_norm": 2.9228861331939697,
      "learning_rate": 3.8e-06,
      "loss": 1.4193,
      "step": 18490
    },
    {
      "epoch": 9.25,
      "grad_norm": 11.202465057373047,
      "learning_rate": 3.775e-06,
      "loss": 1.4648,
      "step": 18500
    },
    {
      "epoch": 9.255,
      "grad_norm": 30.633380889892578,
      "learning_rate": 3.75e-06,
      "loss": 1.4641,
      "step": 18510
    },
    {
      "epoch": 9.26,
      "grad_norm": 8.565043449401855,
      "learning_rate": 3.725e-06,
      "loss": 1.3953,
      "step": 18520
    },
    {
      "epoch": 9.265,
      "grad_norm": 12.688318252563477,
      "learning_rate": 3.7e-06,
      "loss": 1.273,
      "step": 18530
    },
    {
      "epoch": 9.27,
      "grad_norm": 20.20746421813965,
      "learning_rate": 3.675e-06,
      "loss": 1.5204,
      "step": 18540
    },
    {
      "epoch": 9.275,
      "grad_norm": 10.78516960144043,
      "learning_rate": 3.6499999999999998e-06,
      "loss": 1.2075,
      "step": 18550
    },
    {
      "epoch": 9.28,
      "grad_norm": 18.75747299194336,
      "learning_rate": 3.625e-06,
      "loss": 1.3926,
      "step": 18560
    },
    {
      "epoch": 9.285,
      "grad_norm": 16.006040573120117,
      "learning_rate": 3.6e-06,
      "loss": 1.1728,
      "step": 18570
    },
    {
      "epoch": 9.29,
      "grad_norm": 10.373071670532227,
      "learning_rate": 3.575e-06,
      "loss": 1.2896,
      "step": 18580
    },
    {
      "epoch": 9.295,
      "grad_norm": 15.838116645812988,
      "learning_rate": 3.55e-06,
      "loss": 1.2322,
      "step": 18590
    },
    {
      "epoch": 9.3,
      "grad_norm": 9.646230697631836,
      "learning_rate": 3.5249999999999997e-06,
      "loss": 1.3173,
      "step": 18600
    },
    {
      "epoch": 9.305,
      "grad_norm": 9.048189163208008,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 1.6127,
      "step": 18610
    },
    {
      "epoch": 9.31,
      "grad_norm": 21.35651206970215,
      "learning_rate": 3.4750000000000006e-06,
      "loss": 1.5083,
      "step": 18620
    },
    {
      "epoch": 9.315,
      "grad_norm": 10.47373104095459,
      "learning_rate": 3.4500000000000004e-06,
      "loss": 1.4749,
      "step": 18630
    },
    {
      "epoch": 9.32,
      "grad_norm": 9.913459777832031,
      "learning_rate": 3.4250000000000002e-06,
      "loss": 1.1283,
      "step": 18640
    },
    {
      "epoch": 9.325,
      "grad_norm": 9.061835289001465,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 1.5789,
      "step": 18650
    },
    {
      "epoch": 9.33,
      "grad_norm": 9.939757347106934,
      "learning_rate": 3.3750000000000003e-06,
      "loss": 1.4252,
      "step": 18660
    },
    {
      "epoch": 9.335,
      "grad_norm": 19.03021812438965,
      "learning_rate": 3.3500000000000005e-06,
      "loss": 1.1717,
      "step": 18670
    },
    {
      "epoch": 9.34,
      "grad_norm": 18.78498649597168,
      "learning_rate": 3.3250000000000004e-06,
      "loss": 1.4547,
      "step": 18680
    },
    {
      "epoch": 9.345,
      "grad_norm": 10.82094955444336,
      "learning_rate": 3.3e-06,
      "loss": 1.669,
      "step": 18690
    },
    {
      "epoch": 9.35,
      "grad_norm": 18.86426544189453,
      "learning_rate": 3.2750000000000004e-06,
      "loss": 1.2172,
      "step": 18700
    },
    {
      "epoch": 9.355,
      "grad_norm": 9.29246711730957,
      "learning_rate": 3.2500000000000002e-06,
      "loss": 1.1356,
      "step": 18710
    },
    {
      "epoch": 9.36,
      "grad_norm": 11.091920852661133,
      "learning_rate": 3.225e-06,
      "loss": 1.5101,
      "step": 18720
    },
    {
      "epoch": 9.365,
      "grad_norm": 10.106118202209473,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 1.0398,
      "step": 18730
    },
    {
      "epoch": 9.37,
      "grad_norm": 7.902194023132324,
      "learning_rate": 3.175e-06,
      "loss": 0.8063,
      "step": 18740
    },
    {
      "epoch": 9.375,
      "grad_norm": 11.564774513244629,
      "learning_rate": 3.1500000000000003e-06,
      "loss": 1.6629,
      "step": 18750
    },
    {
      "epoch": 9.38,
      "grad_norm": 10.179364204406738,
      "learning_rate": 3.125e-06,
      "loss": 1.2669,
      "step": 18760
    },
    {
      "epoch": 9.385,
      "grad_norm": 14.052530288696289,
      "learning_rate": 3.1e-06,
      "loss": 1.4774,
      "step": 18770
    },
    {
      "epoch": 9.39,
      "grad_norm": 8.133795738220215,
      "learning_rate": 3.075e-06,
      "loss": 1.1694,
      "step": 18780
    },
    {
      "epoch": 9.395,
      "grad_norm": 9.044683456420898,
      "learning_rate": 3.05e-06,
      "loss": 1.2617,
      "step": 18790
    },
    {
      "epoch": 9.4,
      "grad_norm": 16.699756622314453,
      "learning_rate": 3.0250000000000003e-06,
      "loss": 1.1325,
      "step": 18800
    },
    {
      "epoch": 9.405,
      "grad_norm": 13.320663452148438,
      "learning_rate": 3e-06,
      "loss": 1.4786,
      "step": 18810
    },
    {
      "epoch": 9.41,
      "grad_norm": 11.094718933105469,
      "learning_rate": 2.975e-06,
      "loss": 1.4673,
      "step": 18820
    },
    {
      "epoch": 9.415,
      "grad_norm": 8.483503341674805,
      "learning_rate": 2.95e-06,
      "loss": 1.5736,
      "step": 18830
    },
    {
      "epoch": 9.42,
      "grad_norm": 13.625447273254395,
      "learning_rate": 2.9250000000000004e-06,
      "loss": 1.7415,
      "step": 18840
    },
    {
      "epoch": 9.425,
      "grad_norm": 11.035293579101562,
      "learning_rate": 2.9e-06,
      "loss": 1.5295,
      "step": 18850
    },
    {
      "epoch": 9.43,
      "grad_norm": 9.78320026397705,
      "learning_rate": 2.8750000000000004e-06,
      "loss": 1.5529,
      "step": 18860
    },
    {
      "epoch": 9.435,
      "grad_norm": 19.181062698364258,
      "learning_rate": 2.8500000000000002e-06,
      "loss": 1.5074,
      "step": 18870
    },
    {
      "epoch": 9.44,
      "grad_norm": 18.68915367126465,
      "learning_rate": 2.825e-06,
      "loss": 1.3852,
      "step": 18880
    },
    {
      "epoch": 9.445,
      "grad_norm": 19.128406524658203,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 1.4535,
      "step": 18890
    },
    {
      "epoch": 9.45,
      "grad_norm": 12.01793098449707,
      "learning_rate": 2.775e-06,
      "loss": 0.9381,
      "step": 18900
    },
    {
      "epoch": 9.455,
      "grad_norm": 18.77243995666504,
      "learning_rate": 2.7500000000000004e-06,
      "loss": 1.4275,
      "step": 18910
    },
    {
      "epoch": 9.46,
      "grad_norm": 11.974771499633789,
      "learning_rate": 2.725e-06,
      "loss": 1.2268,
      "step": 18920
    },
    {
      "epoch": 9.465,
      "grad_norm": 10.943181991577148,
      "learning_rate": 2.7e-06,
      "loss": 1.3755,
      "step": 18930
    },
    {
      "epoch": 9.47,
      "grad_norm": 12.038331985473633,
      "learning_rate": 2.6750000000000002e-06,
      "loss": 1.2114,
      "step": 18940
    },
    {
      "epoch": 9.475,
      "grad_norm": 11.800651550292969,
      "learning_rate": 2.65e-06,
      "loss": 1.3811,
      "step": 18950
    },
    {
      "epoch": 9.48,
      "grad_norm": 12.56313419342041,
      "learning_rate": 2.625e-06,
      "loss": 1.3681,
      "step": 18960
    },
    {
      "epoch": 9.485,
      "grad_norm": 9.647197723388672,
      "learning_rate": 2.6e-06,
      "loss": 1.3717,
      "step": 18970
    },
    {
      "epoch": 9.49,
      "grad_norm": 9.890409469604492,
      "learning_rate": 2.575e-06,
      "loss": 1.3145,
      "step": 18980
    },
    {
      "epoch": 9.495,
      "grad_norm": 10.480463981628418,
      "learning_rate": 2.55e-06,
      "loss": 1.0905,
      "step": 18990
    },
    {
      "epoch": 9.5,
      "grad_norm": 12.061518669128418,
      "learning_rate": 2.5250000000000004e-06,
      "loss": 1.4277,
      "step": 19000
    }
  ],
  "logging_steps": 10,
  "max_steps": 20000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.7852762783744e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
